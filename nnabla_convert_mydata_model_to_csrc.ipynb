{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMd9qVONusgP5iBbxGWynRp",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ronron-gh/ESP32_EdgeAI_PlatformIO/blob/main/nnabla_convert_mydata_model_to_csrc.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "nnablaをインストールする。"
      ],
      "metadata": {
        "id": "rXJs-ZgSD3lK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install nnabla-ext-cuda114"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "ef69qbZjGiG5",
        "outputId": "642a6989-c310-4f58-988f-f2fec76b02ed"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting nnabla-ext-cuda114\n",
            "  Downloading nnabla_ext_cuda114-1.33.1-cp310-cp310-manylinux_2_17_x86_64.whl (121.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m121.1/121.1 MB\u001b[0m \u001b[31m6.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from nnabla-ext-cuda114) (67.7.2)\n",
            "Collecting nnabla==1.33.1 (from nnabla-ext-cuda114)\n",
            "  Downloading nnabla-1.33.1-cp310-cp310-manylinux_2_17_x86_64.whl (166.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m166.6/166.6 MB\u001b[0m \u001b[31m8.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: Cython in /usr/local/lib/python3.10/dist-packages (from nnabla==1.33.1->nnabla-ext-cuda114) (0.29.34)\n",
            "Collecting numpy~=1.23.0 (from nnabla==1.33.1->nnabla-ext-cuda114)\n",
            "  Downloading numpy-1.23.5-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (17.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m17.1/17.1 MB\u001b[0m \u001b[31m84.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting boto3 (from nnabla==1.33.1->nnabla-ext-cuda114)\n",
            "  Downloading boto3-1.26.146-py3-none-any.whl (135 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m135.6/135.6 kB\u001b[0m \u001b[31m18.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting configparser (from nnabla==1.33.1->nnabla-ext-cuda114)\n",
            "  Downloading configparser-5.3.0-py3-none-any.whl (19 kB)\n",
            "Requirement already satisfied: contextlib2 in /usr/local/lib/python3.10/dist-packages (from nnabla==1.33.1->nnabla-ext-cuda114) (0.6.0.post1)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.10/dist-packages (from nnabla==1.33.1->nnabla-ext-cuda114) (3.8.0)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from nnabla==1.33.1->nnabla-ext-cuda114) (6.0)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from nnabla==1.33.1->nnabla-ext-cuda114) (1.10.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from nnabla==1.33.1->nnabla-ext-cuda114) (1.16.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nnabla==1.33.1->nnabla-ext-cuda114) (4.65.0)\n",
            "Requirement already satisfied: imageio in /usr/local/lib/python3.10/dist-packages (from nnabla==1.33.1->nnabla-ext-cuda114) (2.25.1)\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.10/dist-packages (from nnabla==1.33.1->nnabla-ext-cuda114) (8.4.0)\n",
            "Collecting ply (from nnabla==1.33.1->nnabla-ext-cuda114)\n",
            "  Downloading ply-3.11-py2.py3-none-any.whl (49 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.6/49.6 kB\u001b[0m \u001b[31m7.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting protobuf<=3.19.4 (from nnabla==1.33.1->nnabla-ext-cuda114)\n",
            "  Downloading protobuf-3.19.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m77.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting botocore<1.30.0,>=1.29.146 (from boto3->nnabla==1.33.1->nnabla-ext-cuda114)\n",
            "  Downloading botocore-1.29.146-py3-none-any.whl (10.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.8/10.8 MB\u001b[0m \u001b[31m112.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting jmespath<2.0.0,>=0.7.1 (from boto3->nnabla==1.33.1->nnabla-ext-cuda114)\n",
            "  Downloading jmespath-1.0.1-py3-none-any.whl (20 kB)\n",
            "Collecting s3transfer<0.7.0,>=0.6.0 (from boto3->nnabla==1.33.1->nnabla-ext-cuda114)\n",
            "  Downloading s3transfer-0.6.1-py3-none-any.whl (79 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m79.8/79.8 kB\u001b[0m \u001b[31m11.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: python-dateutil<3.0.0,>=2.1 in /usr/local/lib/python3.10/dist-packages (from botocore<1.30.0,>=1.29.146->boto3->nnabla==1.33.1->nnabla-ext-cuda114) (2.8.2)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.25.4 in /usr/local/lib/python3.10/dist-packages (from botocore<1.30.0,>=1.29.146->boto3->nnabla==1.33.1->nnabla-ext-cuda114) (1.26.15)\n",
            "Installing collected packages: ply, protobuf, numpy, jmespath, configparser, botocore, s3transfer, boto3, nnabla, nnabla-ext-cuda114\n",
            "  Attempting uninstall: protobuf\n",
            "    Found existing installation: protobuf 3.20.3\n",
            "    Uninstalling protobuf-3.20.3:\n",
            "      Successfully uninstalled protobuf-3.20.3\n",
            "  Attempting uninstall: numpy\n",
            "    Found existing installation: numpy 1.22.4\n",
            "    Uninstalling numpy-1.22.4:\n",
            "      Successfully uninstalled numpy-1.22.4\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "google-api-core 2.11.0 requires protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5, but you have protobuf 3.19.4 which is incompatible.\n",
            "google-cloud-bigquery 3.9.0 requires protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5, but you have protobuf 3.19.4 which is incompatible.\n",
            "google-cloud-bigquery-storage 2.19.1 requires protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5, but you have protobuf 3.19.4 which is incompatible.\n",
            "google-cloud-datastore 2.15.1 requires protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5, but you have protobuf 3.19.4 which is incompatible.\n",
            "google-cloud-firestore 2.11.0 requires protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5, but you have protobuf 3.19.4 which is incompatible.\n",
            "google-cloud-language 2.9.1 requires protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5, but you have protobuf 3.19.4 which is incompatible.\n",
            "google-cloud-translate 3.11.1 requires protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5, but you have protobuf 3.19.4 which is incompatible.\n",
            "googleapis-common-protos 1.59.0 requires protobuf!=3.20.0,!=3.20.1,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5, but you have protobuf 3.19.4 which is incompatible.\n",
            "tensorboard 2.12.2 requires protobuf>=3.19.6, but you have protobuf 3.19.4 which is incompatible.\n",
            "tensorflow 2.12.0 requires protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3, but you have protobuf 3.19.4 which is incompatible.\n",
            "tensorflow-datasets 4.9.2 requires protobuf>=3.20, but you have protobuf 3.19.4 which is incompatible.\n",
            "tensorflow-hub 0.13.0 requires protobuf>=3.19.6, but you have protobuf 3.19.4 which is incompatible.\n",
            "tensorflow-metadata 1.13.1 requires protobuf<5,>=3.20.3, but you have protobuf 3.19.4 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed boto3-1.26.146 botocore-1.29.146 configparser-5.3.0 jmespath-1.0.1 nnabla-1.33.1 nnabla-ext-cuda114-1.33.1 numpy-1.23.5 ply-3.11 protobuf-3.19.4 s3transfer-0.6.1\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "configparser",
                  "google",
                  "numpy"
                ]
              }
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "numpyを1.23.0に下げて、pillowを9.1.0に上げる（そうしないと、この後のclassification.pyの実行でエラーになった。数カ月前まではnumpyの変更だけで動いていたので、今後もこのようなバージョンの不整合は発生するかもしれない）。"
      ],
      "metadata": {
        "id": "E45JSZ8pECXk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install numpy==1.23.0"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 394
        },
        "id": "GFw9wEI5G1-8",
        "outputId": "72a9b5f4-2b6f-4afe-b4d5-3eb9dc98e67a"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting numpy==1.23.0\n",
            "  Downloading numpy-1.23.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (17.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m17.0/17.0 MB\u001b[0m \u001b[31m59.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: numpy\n",
            "  Attempting uninstall: numpy\n",
            "    Found existing installation: numpy 1.23.5\n",
            "    Uninstalling numpy-1.23.5:\n",
            "      Successfully uninstalled numpy-1.23.5\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "tensorboard 2.12.2 requires protobuf>=3.19.6, but you have protobuf 3.19.4 which is incompatible.\n",
            "tensorflow 2.12.0 requires protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3, but you have protobuf 3.19.4 which is incompatible.\n",
            "tensorflow-datasets 4.9.2 requires protobuf>=3.20, but you have protobuf 3.19.4 which is incompatible.\n",
            "tensorflow-hub 0.13.0 requires protobuf>=3.19.6, but you have protobuf 3.19.4 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed numpy-1.23.0\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "numpy"
                ]
              }
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pillow==9.1.0"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 285
        },
        "id": "cstzfbIsEGOO",
        "outputId": "e6314c1f-5e7b-4b96-eb16-10e589056609"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting pillow==9.1.0\n",
            "  Downloading Pillow-9.1.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.3/4.3 MB\u001b[0m \u001b[31m36.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: pillow\n",
            "  Attempting uninstall: pillow\n",
            "    Found existing installation: Pillow 8.4.0\n",
            "    Uninstalling Pillow-8.4.0:\n",
            "      Successfully uninstalled Pillow-8.4.0\n",
            "Successfully installed pillow-9.1.0\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "PIL"
                ]
              }
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "nnablaのサンプル群をGitHubから取得。"
      ],
      "metadata": {
        "id": "nCYTRdT5D6Aj"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yBnv_Ye6GTYm",
        "outputId": "ccf1f2a2-88d8-4138-e8c9-e1767ef89446"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'nnabla-examples'...\n",
            "remote: Enumerating objects: 9598, done.\u001b[K\n",
            "remote: Counting objects: 100% (2156/2156), done.\u001b[K\n",
            "remote: Compressing objects: 100% (851/851), done.\u001b[K\n",
            "remote: Total 9598 (delta 1263), reused 2093 (delta 1247), pack-reused 7442\u001b[K\n",
            "Receiving objects: 100% (9598/9598), 299.11 MiB | 34.28 MiB/s, done.\n",
            "Resolving deltas: 100% (5165/5165), done.\n",
            "Updating files: 100% (1711/1711), done.\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/sony/nnabla-examples.git"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "本ブログで公開しているデータセットやプログラムをGitHubから取得。"
      ],
      "metadata": {
        "id": "UgWFw-HwT2o0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/ronron-gh/ESP32_EdgeAI_PlatformIO.git"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ecF5PgfsUl0b",
        "outputId": "869c6340-61a8-449f-c50e-b27d66a34a00"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'ESP32_EdgeAI_PlatformIO'...\n",
            "remote: Enumerating objects: 745, done.\u001b[K\n",
            "remote: Counting objects: 100% (745/745), done.\u001b[K\n",
            "remote: Compressing objects: 100% (670/670), done.\u001b[K\n",
            "remote: Total 745 (delta 68), reused 742 (delta 68), pack-reused 0\u001b[K\n",
            "Receiving objects: 100% (745/745), 2.42 MiB | 26.40 MiB/s, done.\n",
            "Resolving deltas: 100% (68/68), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd ESP32_EdgeAI_PlatformIO"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L-iBemakaxJw",
        "outputId": "2e1f001c-9cc3-49c6-f73a-b083faaf9a1d"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/ESP32_EdgeAI_PlatformIO\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "データセットをnnablaに読み込ませるためのリストファイル(train.csv, test.csv)を作成する。画像は28x28のモノクロに変換する。（CSVの作成と画像の変換を行うPythonスクリプトを用意しました。）"
      ],
      "metadata": {
        "id": "fq9sNf-7a3qC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!python make_dataset_csv.py my_dataset/finger_direction"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8we8BQekb6gN",
        "outputId": "fae188ea-2c8a-4724-c9d7-5e71523297f1"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The current folder is my_dataset/finger_direction\n",
            "The current folder is my_dataset/finger_direction/1\n",
            "listed file : my_dataset/finger_direction/1/0031.jpg\n",
            "listed file : my_dataset/finger_direction/1/0022.jpg\n",
            "listed file : my_dataset/finger_direction/1/0005.jpg\n",
            "listed file : my_dataset/finger_direction/1/0048.jpg\n",
            "listed file : my_dataset/finger_direction/1/0015.jpg\n",
            "listed file : my_dataset/finger_direction/1/0044.jpg\n",
            "listed file : my_dataset/finger_direction/1/0008.jpg\n",
            "listed file : my_dataset/finger_direction/1/0045.jpg\n",
            "listed file : my_dataset/finger_direction/1/0035.jpg\n",
            "listed file : my_dataset/finger_direction/1/0032.jpg\n",
            "listed file : my_dataset/finger_direction/1/0007.jpg\n",
            "listed file : my_dataset/finger_direction/1/0006.jpg\n",
            "listed file : my_dataset/finger_direction/1/0036.jpg\n",
            "listed file : my_dataset/finger_direction/1/0020.jpg\n",
            "listed file : my_dataset/finger_direction/1/0037.jpg\n",
            "listed file : my_dataset/finger_direction/1/0029.jpg\n",
            "listed file : my_dataset/finger_direction/1/0014.jpg\n",
            "listed file : my_dataset/finger_direction/1/0017.jpg\n",
            "listed file : my_dataset/finger_direction/1/0021.jpg\n",
            "listed file : my_dataset/finger_direction/1/0043.jpg\n",
            "listed file : my_dataset/finger_direction/1/0030.jpg\n",
            "listed file : my_dataset/finger_direction/1/0052.jpg\n",
            "listed file : my_dataset/finger_direction/1/0053.jpg\n",
            "listed file : my_dataset/finger_direction/1/0025.jpg\n",
            "listed file : my_dataset/finger_direction/1/0039.jpg\n",
            "listed file : my_dataset/finger_direction/1/0018.jpg\n",
            "listed file : my_dataset/finger_direction/1/0049.jpg\n",
            "listed file : my_dataset/finger_direction/1/0046.jpg\n",
            "listed file : my_dataset/finger_direction/1/0028.jpg\n",
            "listed file : my_dataset/finger_direction/1/0055.jpg\n",
            "listed file : my_dataset/finger_direction/1/0023.jpg\n",
            "listed file : my_dataset/finger_direction/1/0009.jpg\n",
            "listed file : my_dataset/finger_direction/1/0041.jpg\n",
            "listed file : my_dataset/finger_direction/1/0038.jpg\n",
            "listed file : my_dataset/finger_direction/1/0040.jpg\n",
            "listed file : my_dataset/finger_direction/1/0016.jpg\n",
            "listed file : my_dataset/finger_direction/1/0042.jpg\n",
            "listed file : my_dataset/finger_direction/1/0026.jpg\n",
            "listed file : my_dataset/finger_direction/1/0024.jpg\n",
            "listed file : my_dataset/finger_direction/1/0034.jpg\n",
            "listed file : my_dataset/finger_direction/1/0013.jpg\n",
            "listed file : my_dataset/finger_direction/1/0012.jpg\n",
            "listed file : my_dataset/finger_direction/1/0047.jpg\n",
            "listed file : my_dataset/finger_direction/1/0054.jpg\n",
            "listed file : my_dataset/finger_direction/1/0050.jpg\n",
            "listed file : my_dataset/finger_direction/1/0011.jpg\n",
            "listed file : my_dataset/finger_direction/1/0033.jpg\n",
            "listed file : my_dataset/finger_direction/1/0010.jpg\n",
            "listed file : my_dataset/finger_direction/1/0019.jpg\n",
            "listed file : my_dataset/finger_direction/1/0051.jpg\n",
            "listed file : my_dataset/finger_direction/1/0056.jpg\n",
            "listed file : my_dataset/finger_direction/1/0027.jpg\n",
            "The current folder is my_dataset/finger_direction/4\n",
            "listed file : my_dataset/finger_direction/4/0086.jpg\n",
            "listed file : my_dataset/finger_direction/4/0022.jpg\n",
            "listed file : my_dataset/finger_direction/4/0077.jpg\n",
            "listed file : my_dataset/finger_direction/4/0048.jpg\n",
            "listed file : my_dataset/finger_direction/4/0015.jpg\n",
            "listed file : my_dataset/finger_direction/4/0035.jpg\n",
            "listed file : my_dataset/finger_direction/4/0032.jpg\n",
            "listed file : my_dataset/finger_direction/4/0069.jpg\n",
            "listed file : my_dataset/finger_direction/4/0036.jpg\n",
            "listed file : my_dataset/finger_direction/4/0085.jpg\n",
            "listed file : my_dataset/finger_direction/4/0020.jpg\n",
            "listed file : my_dataset/finger_direction/4/0037.jpg\n",
            "listed file : my_dataset/finger_direction/4/0029.jpg\n",
            "listed file : my_dataset/finger_direction/4/0014.jpg\n",
            "listed file : my_dataset/finger_direction/4/0017.jpg\n",
            "listed file : my_dataset/finger_direction/4/0021.jpg\n",
            "listed file : my_dataset/finger_direction/4/0064.jpg\n",
            "listed file : my_dataset/finger_direction/4/0052.jpg\n",
            "listed file : my_dataset/finger_direction/4/0053.jpg\n",
            "listed file : my_dataset/finger_direction/4/0025.jpg\n",
            "listed file : my_dataset/finger_direction/4/0039.jpg\n",
            "listed file : my_dataset/finger_direction/4/0018.jpg\n",
            "listed file : my_dataset/finger_direction/4/0049.jpg\n",
            "listed file : my_dataset/finger_direction/4/0063.jpg\n",
            "listed file : my_dataset/finger_direction/4/0081.jpg\n",
            "listed file : my_dataset/finger_direction/4/0046.jpg\n",
            "listed file : my_dataset/finger_direction/4/0070.jpg\n",
            "listed file : my_dataset/finger_direction/4/0061.jpg\n",
            "listed file : my_dataset/finger_direction/4/0028.jpg\n",
            "listed file : my_dataset/finger_direction/4/0065.jpg\n",
            "listed file : my_dataset/finger_direction/4/0060.jpg\n",
            "listed file : my_dataset/finger_direction/4/0055.jpg\n",
            "listed file : my_dataset/finger_direction/4/0082.jpg\n",
            "listed file : my_dataset/finger_direction/4/0079.jpg\n",
            "listed file : my_dataset/finger_direction/4/0078.jpg\n",
            "listed file : my_dataset/finger_direction/4/0023.jpg\n",
            "listed file : my_dataset/finger_direction/4/0068.jpg\n",
            "listed file : my_dataset/finger_direction/4/0087.jpg\n",
            "listed file : my_dataset/finger_direction/4/0041.jpg\n",
            "listed file : my_dataset/finger_direction/4/0038.jpg\n",
            "listed file : my_dataset/finger_direction/4/0040.jpg\n",
            "listed file : my_dataset/finger_direction/4/0016.jpg\n",
            "listed file : my_dataset/finger_direction/4/0071.jpg\n",
            "listed file : my_dataset/finger_direction/4/0026.jpg\n",
            "listed file : my_dataset/finger_direction/4/0024.jpg\n",
            "listed file : my_dataset/finger_direction/4/0034.jpg\n",
            "listed file : my_dataset/finger_direction/4/0084.jpg\n",
            "listed file : my_dataset/finger_direction/4/0062.jpg\n",
            "listed file : my_dataset/finger_direction/4/0013.jpg\n",
            "listed file : my_dataset/finger_direction/4/0047.jpg\n",
            "listed file : my_dataset/finger_direction/4/0054.jpg\n",
            "listed file : my_dataset/finger_direction/4/0050.jpg\n",
            "listed file : my_dataset/finger_direction/4/0067.jpg\n",
            "listed file : my_dataset/finger_direction/4/0080.jpg\n",
            "listed file : my_dataset/finger_direction/4/0057.jpg\n",
            "listed file : my_dataset/finger_direction/4/0066.jpg\n",
            "listed file : my_dataset/finger_direction/4/0033.jpg\n",
            "listed file : my_dataset/finger_direction/4/0051.jpg\n",
            "listed file : my_dataset/finger_direction/4/0088.jpg\n",
            "listed file : my_dataset/finger_direction/4/0056.jpg\n",
            "listed file : my_dataset/finger_direction/4/0027.jpg\n",
            "listed file : my_dataset/finger_direction/4/0083.jpg\n",
            "The current folder is my_dataset/finger_direction/0\n",
            "listed file : my_dataset/finger_direction/0/0031.jpg\n",
            "listed file : my_dataset/finger_direction/0/0086.jpg\n",
            "listed file : my_dataset/finger_direction/0/0022.jpg\n",
            "listed file : my_dataset/finger_direction/0/0077.jpg\n",
            "listed file : my_dataset/finger_direction/0/0005.jpg\n",
            "listed file : my_dataset/finger_direction/0/0048.jpg\n",
            "listed file : my_dataset/finger_direction/0/0015.jpg\n",
            "listed file : my_dataset/finger_direction/0/0044.jpg\n",
            "listed file : my_dataset/finger_direction/0/0008.jpg\n",
            "listed file : my_dataset/finger_direction/0/0045.jpg\n",
            "listed file : my_dataset/finger_direction/0/0035.jpg\n",
            "listed file : my_dataset/finger_direction/0/0032.jpg\n",
            "listed file : my_dataset/finger_direction/0/0069.jpg\n",
            "listed file : my_dataset/finger_direction/0/0007.jpg\n",
            "listed file : my_dataset/finger_direction/0/0006.jpg\n",
            "listed file : my_dataset/finger_direction/0/0036.jpg\n",
            "listed file : my_dataset/finger_direction/0/0091.jpg\n",
            "listed file : my_dataset/finger_direction/0/0085.jpg\n",
            "listed file : my_dataset/finger_direction/0/0020.jpg\n",
            "listed file : my_dataset/finger_direction/0/0037.jpg\n",
            "listed file : my_dataset/finger_direction/0/0029.jpg\n",
            "listed file : my_dataset/finger_direction/0/0075.jpg\n",
            "listed file : my_dataset/finger_direction/0/0001.jpg\n",
            "listed file : my_dataset/finger_direction/0/0014.jpg\n",
            "listed file : my_dataset/finger_direction/0/0089.jpg\n",
            "listed file : my_dataset/finger_direction/0/0017.jpg\n",
            "listed file : my_dataset/finger_direction/0/0074.jpg\n",
            "listed file : my_dataset/finger_direction/0/0003.jpg\n",
            "listed file : my_dataset/finger_direction/0/0021.jpg\n",
            "listed file : my_dataset/finger_direction/0/0043.jpg\n",
            "listed file : my_dataset/finger_direction/0/0064.jpg\n",
            "listed file : my_dataset/finger_direction/0/0030.jpg\n",
            "listed file : my_dataset/finger_direction/0/0052.jpg\n",
            "listed file : my_dataset/finger_direction/0/0053.jpg\n",
            "listed file : my_dataset/finger_direction/0/0025.jpg\n",
            "listed file : my_dataset/finger_direction/0/0072.jpg\n",
            "listed file : my_dataset/finger_direction/0/0039.jpg\n",
            "listed file : my_dataset/finger_direction/0/0018.jpg\n",
            "listed file : my_dataset/finger_direction/0/0049.jpg\n",
            "listed file : my_dataset/finger_direction/0/0063.jpg\n",
            "listed file : my_dataset/finger_direction/0/0081.jpg\n",
            "listed file : my_dataset/finger_direction/0/0046.jpg\n",
            "listed file : my_dataset/finger_direction/0/0070.jpg\n",
            "listed file : my_dataset/finger_direction/0/0061.jpg\n",
            "listed file : my_dataset/finger_direction/0/0028.jpg\n",
            "listed file : my_dataset/finger_direction/0/0065.jpg\n",
            "listed file : my_dataset/finger_direction/0/0060.jpg\n",
            "listed file : my_dataset/finger_direction/0/0055.jpg\n",
            "listed file : my_dataset/finger_direction/0/0082.jpg\n",
            "listed file : my_dataset/finger_direction/0/0079.jpg\n",
            "listed file : my_dataset/finger_direction/0/0078.jpg\n",
            "listed file : my_dataset/finger_direction/0/0023.jpg\n",
            "listed file : my_dataset/finger_direction/0/0068.jpg\n",
            "listed file : my_dataset/finger_direction/0/0087.jpg\n",
            "listed file : my_dataset/finger_direction/0/0009.jpg\n",
            "listed file : my_dataset/finger_direction/0/0004.jpg\n",
            "listed file : my_dataset/finger_direction/0/0041.jpg\n",
            "listed file : my_dataset/finger_direction/0/0038.jpg\n",
            "listed file : my_dataset/finger_direction/0/0059.jpg\n",
            "listed file : my_dataset/finger_direction/0/0040.jpg\n",
            "listed file : my_dataset/finger_direction/0/0016.jpg\n",
            "listed file : my_dataset/finger_direction/0/0042.jpg\n",
            "listed file : my_dataset/finger_direction/0/0071.jpg\n",
            "listed file : my_dataset/finger_direction/0/0026.jpg\n",
            "listed file : my_dataset/finger_direction/0/0073.jpg\n",
            "listed file : my_dataset/finger_direction/0/0000.jpg\n",
            "listed file : my_dataset/finger_direction/0/0024.jpg\n",
            "listed file : my_dataset/finger_direction/0/0034.jpg\n",
            "listed file : my_dataset/finger_direction/0/0084.jpg\n",
            "listed file : my_dataset/finger_direction/0/0062.jpg\n",
            "listed file : my_dataset/finger_direction/0/0013.jpg\n",
            "listed file : my_dataset/finger_direction/0/0058.jpg\n",
            "listed file : my_dataset/finger_direction/0/0012.jpg\n",
            "listed file : my_dataset/finger_direction/0/0047.jpg\n",
            "listed file : my_dataset/finger_direction/0/0054.jpg\n",
            "listed file : my_dataset/finger_direction/0/0092.jpg\n",
            "listed file : my_dataset/finger_direction/0/0050.jpg\n",
            "listed file : my_dataset/finger_direction/0/0067.jpg\n",
            "listed file : my_dataset/finger_direction/0/0002.jpg\n",
            "listed file : my_dataset/finger_direction/0/0011.jpg\n",
            "listed file : my_dataset/finger_direction/0/0080.jpg\n",
            "listed file : my_dataset/finger_direction/0/0057.jpg\n",
            "listed file : my_dataset/finger_direction/0/0066.jpg\n",
            "listed file : my_dataset/finger_direction/0/0033.jpg\n",
            "listed file : my_dataset/finger_direction/0/0010.jpg\n",
            "listed file : my_dataset/finger_direction/0/0019.jpg\n",
            "listed file : my_dataset/finger_direction/0/0051.jpg\n",
            "listed file : my_dataset/finger_direction/0/0090.jpg\n",
            "listed file : my_dataset/finger_direction/0/0076.jpg\n",
            "listed file : my_dataset/finger_direction/0/0088.jpg\n",
            "listed file : my_dataset/finger_direction/0/0056.jpg\n",
            "listed file : my_dataset/finger_direction/0/0027.jpg\n",
            "listed file : my_dataset/finger_direction/0/0083.jpg\n",
            "The current folder is my_dataset/finger_direction/5\n",
            "listed file : my_dataset/finger_direction/5/0031.jpg\n",
            "listed file : my_dataset/finger_direction/5/0022.jpg\n",
            "listed file : my_dataset/finger_direction/5/0015.jpg\n",
            "listed file : my_dataset/finger_direction/5/0129.jpg\n",
            "listed file : my_dataset/finger_direction/5/0035.jpg\n",
            "listed file : my_dataset/finger_direction/5/0121.jpg\n",
            "listed file : my_dataset/finger_direction/5/0032.jpg\n",
            "listed file : my_dataset/finger_direction/5/0036.jpg\n",
            "listed file : my_dataset/finger_direction/5/0020.jpg\n",
            "listed file : my_dataset/finger_direction/5/0037.jpg\n",
            "listed file : my_dataset/finger_direction/5/0029.jpg\n",
            "listed file : my_dataset/finger_direction/5/0014.jpg\n",
            "listed file : my_dataset/finger_direction/5/0017.jpg\n",
            "listed file : my_dataset/finger_direction/5/0124.jpg\n",
            "listed file : my_dataset/finger_direction/5/0021.jpg\n",
            "listed file : my_dataset/finger_direction/5/0030.jpg\n",
            "listed file : my_dataset/finger_direction/5/0025.jpg\n",
            "listed file : my_dataset/finger_direction/5/0114.jpg\n",
            "listed file : my_dataset/finger_direction/5/0039.jpg\n",
            "listed file : my_dataset/finger_direction/5/0018.jpg\n",
            "listed file : my_dataset/finger_direction/5/0126.jpg\n",
            "listed file : my_dataset/finger_direction/5/0125.jpg\n",
            "listed file : my_dataset/finger_direction/5/0113.jpg\n",
            "listed file : my_dataset/finger_direction/5/0028.jpg\n",
            "listed file : my_dataset/finger_direction/5/0115.jpg\n",
            "listed file : my_dataset/finger_direction/5/0023.jpg\n",
            "listed file : my_dataset/finger_direction/5/0038.jpg\n",
            "listed file : my_dataset/finger_direction/5/0040.jpg\n",
            "listed file : my_dataset/finger_direction/5/0016.jpg\n",
            "listed file : my_dataset/finger_direction/5/0026.jpg\n",
            "listed file : my_dataset/finger_direction/5/0122.jpg\n",
            "listed file : my_dataset/finger_direction/5/0123.jpg\n",
            "listed file : my_dataset/finger_direction/5/0024.jpg\n",
            "listed file : my_dataset/finger_direction/5/0034.jpg\n",
            "listed file : my_dataset/finger_direction/5/0013.jpg\n",
            "listed file : my_dataset/finger_direction/5/0130.jpg\n",
            "listed file : my_dataset/finger_direction/5/0127.jpg\n",
            "listed file : my_dataset/finger_direction/5/0012.jpg\n",
            "listed file : my_dataset/finger_direction/5/0011.jpg\n",
            "listed file : my_dataset/finger_direction/5/0033.jpg\n",
            "listed file : my_dataset/finger_direction/5/0010.jpg\n",
            "listed file : my_dataset/finger_direction/5/0019.jpg\n",
            "listed file : my_dataset/finger_direction/5/0027.jpg\n",
            "The current folder is my_dataset/finger_direction/3\n",
            "listed file : my_dataset/finger_direction/3/0031.jpg\n",
            "listed file : my_dataset/finger_direction/3/0022.jpg\n",
            "listed file : my_dataset/finger_direction/3/0048.jpg\n",
            "listed file : my_dataset/finger_direction/3/0015.jpg\n",
            "listed file : my_dataset/finger_direction/3/0008.jpg\n",
            "listed file : my_dataset/finger_direction/3/0035.jpg\n",
            "listed file : my_dataset/finger_direction/3/0032.jpg\n",
            "listed file : my_dataset/finger_direction/3/0069.jpg\n",
            "listed file : my_dataset/finger_direction/3/0007.jpg\n",
            "listed file : my_dataset/finger_direction/3/0006.jpg\n",
            "listed file : my_dataset/finger_direction/3/0036.jpg\n",
            "listed file : my_dataset/finger_direction/3/0020.jpg\n",
            "listed file : my_dataset/finger_direction/3/0037.jpg\n",
            "listed file : my_dataset/finger_direction/3/0029.jpg\n",
            "listed file : my_dataset/finger_direction/3/0014.jpg\n",
            "listed file : my_dataset/finger_direction/3/0017.jpg\n",
            "listed file : my_dataset/finger_direction/3/0021.jpg\n",
            "listed file : my_dataset/finger_direction/3/0064.jpg\n",
            "listed file : my_dataset/finger_direction/3/0030.jpg\n",
            "listed file : my_dataset/finger_direction/3/0052.jpg\n",
            "listed file : my_dataset/finger_direction/3/0053.jpg\n",
            "listed file : my_dataset/finger_direction/3/0025.jpg\n",
            "listed file : my_dataset/finger_direction/3/0039.jpg\n",
            "listed file : my_dataset/finger_direction/3/0018.jpg\n",
            "listed file : my_dataset/finger_direction/3/0049.jpg\n",
            "listed file : my_dataset/finger_direction/3/0063.jpg\n",
            "listed file : my_dataset/finger_direction/3/0070.jpg\n",
            "listed file : my_dataset/finger_direction/3/0061.jpg\n",
            "listed file : my_dataset/finger_direction/3/0028.jpg\n",
            "listed file : my_dataset/finger_direction/3/0065.jpg\n",
            "listed file : my_dataset/finger_direction/3/0060.jpg\n",
            "listed file : my_dataset/finger_direction/3/0055.jpg\n",
            "listed file : my_dataset/finger_direction/3/0023.jpg\n",
            "listed file : my_dataset/finger_direction/3/0068.jpg\n",
            "listed file : my_dataset/finger_direction/3/0009.jpg\n",
            "listed file : my_dataset/finger_direction/3/0041.jpg\n",
            "listed file : my_dataset/finger_direction/3/0038.jpg\n",
            "listed file : my_dataset/finger_direction/3/0059.jpg\n",
            "listed file : my_dataset/finger_direction/3/0040.jpg\n",
            "listed file : my_dataset/finger_direction/3/0016.jpg\n",
            "listed file : my_dataset/finger_direction/3/0071.jpg\n",
            "listed file : my_dataset/finger_direction/3/0026.jpg\n",
            "listed file : my_dataset/finger_direction/3/0024.jpg\n",
            "listed file : my_dataset/finger_direction/3/0034.jpg\n",
            "listed file : my_dataset/finger_direction/3/0062.jpg\n",
            "listed file : my_dataset/finger_direction/3/0013.jpg\n",
            "listed file : my_dataset/finger_direction/3/0058.jpg\n",
            "listed file : my_dataset/finger_direction/3/0012.jpg\n",
            "listed file : my_dataset/finger_direction/3/0047.jpg\n",
            "listed file : my_dataset/finger_direction/3/0054.jpg\n",
            "listed file : my_dataset/finger_direction/3/0050.jpg\n",
            "listed file : my_dataset/finger_direction/3/0067.jpg\n",
            "listed file : my_dataset/finger_direction/3/0011.jpg\n",
            "listed file : my_dataset/finger_direction/3/0057.jpg\n",
            "listed file : my_dataset/finger_direction/3/0066.jpg\n",
            "listed file : my_dataset/finger_direction/3/0033.jpg\n",
            "listed file : my_dataset/finger_direction/3/0010.jpg\n",
            "listed file : my_dataset/finger_direction/3/0019.jpg\n",
            "listed file : my_dataset/finger_direction/3/0051.jpg\n",
            "listed file : my_dataset/finger_direction/3/0056.jpg\n",
            "listed file : my_dataset/finger_direction/3/0027.jpg\n",
            "The current folder is my_dataset/finger_direction/2\n",
            "listed file : my_dataset/finger_direction/2/0031.jpg\n",
            "listed file : my_dataset/finger_direction/2/0022.jpg\n",
            "listed file : my_dataset/finger_direction/2/0005.jpg\n",
            "listed file : my_dataset/finger_direction/2/0015.jpg\n",
            "listed file : my_dataset/finger_direction/2/0044.jpg\n",
            "listed file : my_dataset/finger_direction/2/0008.jpg\n",
            "listed file : my_dataset/finger_direction/2/0045.jpg\n",
            "listed file : my_dataset/finger_direction/2/0035.jpg\n",
            "listed file : my_dataset/finger_direction/2/0032.jpg\n",
            "listed file : my_dataset/finger_direction/2/0007.jpg\n",
            "listed file : my_dataset/finger_direction/2/0006.jpg\n",
            "listed file : my_dataset/finger_direction/2/0036.jpg\n",
            "listed file : my_dataset/finger_direction/2/0020.jpg\n",
            "listed file : my_dataset/finger_direction/2/0037.jpg\n",
            "listed file : my_dataset/finger_direction/2/0029.jpg\n",
            "listed file : my_dataset/finger_direction/2/0014.jpg\n",
            "listed file : my_dataset/finger_direction/2/0017.jpg\n",
            "listed file : my_dataset/finger_direction/2/0021.jpg\n",
            "listed file : my_dataset/finger_direction/2/0043.jpg\n",
            "listed file : my_dataset/finger_direction/2/0030.jpg\n",
            "listed file : my_dataset/finger_direction/2/0025.jpg\n",
            "listed file : my_dataset/finger_direction/2/0039.jpg\n",
            "listed file : my_dataset/finger_direction/2/0018.jpg\n",
            "listed file : my_dataset/finger_direction/2/0028.jpg\n",
            "listed file : my_dataset/finger_direction/2/0023.jpg\n",
            "listed file : my_dataset/finger_direction/2/0009.jpg\n",
            "listed file : my_dataset/finger_direction/2/0041.jpg\n",
            "listed file : my_dataset/finger_direction/2/0038.jpg\n",
            "listed file : my_dataset/finger_direction/2/0040.jpg\n",
            "listed file : my_dataset/finger_direction/2/0016.jpg\n",
            "listed file : my_dataset/finger_direction/2/0042.jpg\n",
            "listed file : my_dataset/finger_direction/2/0026.jpg\n",
            "listed file : my_dataset/finger_direction/2/0024.jpg\n",
            "listed file : my_dataset/finger_direction/2/0034.jpg\n",
            "listed file : my_dataset/finger_direction/2/0013.jpg\n",
            "listed file : my_dataset/finger_direction/2/0012.jpg\n",
            "listed file : my_dataset/finger_direction/2/0011.jpg\n",
            "listed file : my_dataset/finger_direction/2/0033.jpg\n",
            "listed file : my_dataset/finger_direction/2/0010.jpg\n",
            "listed file : my_dataset/finger_direction/2/0019.jpg\n",
            "listed file : my_dataset/finger_direction/2/0027.jpg\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "train.csv, test.csv, converted_datasetsフォルダが作成されていることを確認する。"
      ],
      "metadata": {
        "id": "4UvfZiD3NUCl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!ls"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z2M39YKWNf3l",
        "outputId": "f5d81b81-8824-48b4-a50e-9f1f1d999f63"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "converted_datasets   nnabla_convert_mnist_model_to_csrc.ipynb\tREADME.md\n",
            "LICENSE\t\t     nnabla_convert_mydata_model_to_csrc.ipynb\ttest.csv\n",
            "make_dataset_csv.py  nnabla-example-modify\t\t\ttrain.csv\n",
            "my_dataset\t     Platformio\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "nnablaのMNISTサンプルコードのディレクトリに、作成したCSV、データセット（変換後）、改造したプログラムをコピー。"
      ],
      "metadata": {
        "id": "7KqdiU04owwU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!cp *.csv ../nnabla-examples/image-classification/mnist-collection/"
      ],
      "metadata": {
        "id": "qLN_kW59pN7S"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!cp -r converted_datasets ../nnabla-examples/image-classification/mnist-collection/"
      ],
      "metadata": {
        "id": "2f-gxaINq_dB"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!cp nnabla-example-modify/classification_mydata.py ../nnabla-examples/image-classification/mnist-collection/"
      ],
      "metadata": {
        "id": "T58Z5AgTreIJ"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "mnistサンプルプログラムのフォルダに移動し、学習を実行。"
      ],
      "metadata": {
        "id": "loUVBhtMZ4IB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%cd ../nnabla-examples/image-classification/mnist-collection/"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yocj4nJVG-sF",
        "outputId": "da00522e-6195-47be-c1be-b2454e0cf528"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/nnabla-examples/image-classification/mnist-collection\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python classification_mydata.py -c cudnn -n lenet -o output"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-CTfat5XH3wl",
        "outputId": "320bed48-364c-4aa3-c073-2a41cadfd0e8"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2023-06-04 15:39:49,857 [nnabla][INFO]: Initializing CPU extension...\n",
            "2023-06-04 15:39:50,667 [nnabla][INFO]: Running in cudnn\n",
            "2023-06-04 15:39:51,219 [nnabla][INFO]: Initializing CUDA extension...\n",
            "2023-06-04 15:39:51,327 [nnabla][INFO]: Initializing cuDNN extension...\n",
            "2023-06-04 15:39:56,787 [nnabla][INFO]: Saving output/lenet_result_epoch0.nnp as nnp\n",
            "2023-06-04 15:39:56,788 [nnabla][INFO]: Saving <_io.StringIO object at 0x7fe68bf35480> as prototxt\n",
            "2023-06-04 15:39:56,796 [nnabla][INFO]: Parameter save (.h5): <_io.BytesIO object at 0x7fe68b909080>\n",
            "2023-06-04 15:39:56,797 [nnabla][INFO]: Model file is saved as (.nnp): output/lenet_result_epoch0.nnp\n",
            "2023-06-04 15:39:56,797 [nnabla][INFO]: DataSource with shuffle(True)\n",
            "2023-06-04 15:39:56,799 [nnabla][INFO]: Using DataSourceWithFileCache\n",
            "2023-06-04 15:39:56,799 [nnabla][INFO]: DataSource with shuffle(True)\n",
            "2023-06-04 15:39:56,800 [nnabla][INFO]: Cache Directory is None\n",
            "2023-06-04 15:39:56,800 [nnabla][INFO]: Cache size is 100\n",
            "2023-06-04 15:39:56,800 [nnabla][INFO]: Num of thread is 10\n",
            "2023-06-04 15:39:56,800 [nnabla][INFO]: Cache file format is .npy\n",
            "2023-06-04 15:39:56,800 [nnabla][INFO]: Tempdir for cache /tmp/tmpr14krk3i created.\n",
            "2023-06-04 15:39:56,859 [nnabla][INFO]: Creating cache file /tmp/tmpr14krk3i/cache_00000000_00000099.npy\n",
            "2023-06-04 15:39:56,891 [nnabla][INFO]: Creating cache file /tmp/tmpr14krk3i/cache_00000100_00000199.npy\n",
            "2023-06-04 15:39:56,917 [nnabla][INFO]: Creating cache file /tmp/tmpr14krk3i/cache_00000200_00000280.npy\n",
            "2023-06-04 15:39:56,919 [nnabla][INFO]: Using DataSourceWithMemoryCache\n",
            "2023-06-04 15:39:56,919 [nnabla][INFO]: DataSource with shuffle(True)\n",
            "2023-06-04 15:39:56,923 [nnabla][INFO]: On-memory\n",
            "2023-06-04 15:39:56,923 [nnabla][INFO]: Using DataIterator\n",
            "2023-06-04 15:39:56,924 [nnabla][INFO]: DataSource with shuffle(False)\n",
            "2023-06-04 15:39:56,925 [nnabla][INFO]: Using DataSourceWithFileCache\n",
            "2023-06-04 15:39:56,925 [nnabla][INFO]: DataSource with shuffle(False)\n",
            "2023-06-04 15:39:56,925 [nnabla][INFO]: Cache Directory is None\n",
            "2023-06-04 15:39:56,925 [nnabla][INFO]: Cache size is 100\n",
            "2023-06-04 15:39:56,926 [nnabla][INFO]: Num of thread is 10\n",
            "2023-06-04 15:39:56,926 [nnabla][INFO]: Cache file format is .npy\n",
            "2023-06-04 15:39:56,926 [nnabla][INFO]: Tempdir for cache /tmp/tmpngt6rz4n created.\n",
            "2023-06-04 15:39:56,949 [nnabla][INFO]: Creating cache file /tmp/tmpngt6rz4n/cache_00000000_00000069.npy\n",
            "2023-06-04 15:39:56,950 [nnabla][INFO]: Using DataSourceWithMemoryCache\n",
            "2023-06-04 15:39:56,950 [nnabla][INFO]: DataSource with shuffle(False)\n",
            "2023-06-04 15:39:56,951 [nnabla][INFO]: On-memory\n",
            "2023-06-04 15:39:56,951 [nnabla][INFO]: Using DataIterator\n",
            "2023-06-04 15:39:59,917 [nnabla][INFO]: Solver state save (.h5): output/states_0.h5\n",
            "2023-06-04 15:39:59,924 [nnabla][INFO]: Parameter save (.h5): output/params_0.h5\n",
            "2023-06-04 15:39:59,924 [nnabla][INFO]: Checkpoint save (.json): output/checkpoint_0.json\n",
            "2023-06-04 15:40:00,322 [nnabla][INFO]: iter=9 {Training loss}=1.9944185018539429\n",
            "2023-06-04 15:40:00,322 [nnabla][INFO]: iter=9 {Training error}=0.740625\n",
            "2023-06-04 15:40:00,362 [nnabla][INFO]: iter=19 {Training loss}=1.276190161705017\n",
            "2023-06-04 15:40:00,362 [nnabla][INFO]: iter=19 {Training error}=0.31875\n",
            "2023-06-04 15:40:00,404 [nnabla][INFO]: iter=29 {Training loss}=0.6314233541488647\n",
            "2023-06-04 15:40:00,404 [nnabla][INFO]: iter=29 {Training error}=0.1015625\n",
            "2023-06-04 15:40:00,445 [nnabla][INFO]: iter=39 {Training loss}=0.2738756537437439\n",
            "2023-06-04 15:40:00,445 [nnabla][INFO]: iter=39 {Training error}=0.06640625\n",
            "2023-06-04 15:40:00,485 [nnabla][INFO]: iter=49 {Training loss}=0.13614660501480103\n",
            "2023-06-04 15:40:00,486 [nnabla][INFO]: iter=49 {Training error}=0.02890625\n",
            "2023-06-04 15:40:00,526 [nnabla][INFO]: iter=59 {Training loss}=0.08218926936388016\n",
            "2023-06-04 15:40:00,526 [nnabla][INFO]: iter=59 {Training error}=0.01796875\n",
            "2023-06-04 15:40:00,567 [nnabla][INFO]: iter=69 {Training loss}=0.05658264830708504\n",
            "2023-06-04 15:40:00,568 [nnabla][INFO]: iter=69 {Training error}=0.015625\n",
            "2023-06-04 15:40:00,605 [nnabla][INFO]: iter=79 {Training loss}=0.039653878659009933\n",
            "2023-06-04 15:40:00,605 [nnabla][INFO]: iter=79 {Training error}=0.01015625\n",
            "2023-06-04 15:40:00,642 [nnabla][INFO]: iter=89 {Training loss}=0.02705954574048519\n",
            "2023-06-04 15:40:00,642 [nnabla][INFO]: iter=89 {Training error}=0.00625\n",
            "2023-06-04 15:40:00,680 [nnabla][INFO]: iter=99 {Training loss}=0.018028903752565384\n",
            "2023-06-04 15:40:00,680 [nnabla][INFO]: iter=99 {Training error}=0.0015625\n",
            "2023-06-04 15:40:00,680 [nnabla][INFO]: iter=99 {Training time}=3.8965163230895996[sec/100iter] 3.8965163230895996[sec]\n",
            "2023-06-04 15:40:00,701 [nnabla][INFO]: iter=100 {Test error}=0.50703125\n",
            "2023-06-04 15:40:00,738 [nnabla][INFO]: iter=109 {Training loss}=0.014512473717331886\n",
            "2023-06-04 15:40:00,738 [nnabla][INFO]: iter=109 {Training error}=0.0\n",
            "2023-06-04 15:40:00,778 [nnabla][INFO]: iter=119 {Training loss}=0.011042879894375801\n",
            "2023-06-04 15:40:00,778 [nnabla][INFO]: iter=119 {Training error}=0.0\n",
            "2023-06-04 15:40:00,815 [nnabla][INFO]: iter=129 {Training loss}=0.008601445704698563\n",
            "2023-06-04 15:40:00,816 [nnabla][INFO]: iter=129 {Training error}=0.0\n",
            "2023-06-04 15:40:00,852 [nnabla][INFO]: iter=139 {Training loss}=0.007467091083526611\n",
            "2023-06-04 15:40:00,853 [nnabla][INFO]: iter=139 {Training error}=0.0\n",
            "2023-06-04 15:40:00,888 [nnabla][INFO]: iter=149 {Training loss}=0.0059880721382796764\n",
            "2023-06-04 15:40:00,888 [nnabla][INFO]: iter=149 {Training error}=0.0\n",
            "2023-06-04 15:40:00,924 [nnabla][INFO]: iter=159 {Training loss}=0.004998634569346905\n",
            "2023-06-04 15:40:00,924 [nnabla][INFO]: iter=159 {Training error}=0.0\n",
            "2023-06-04 15:40:00,961 [nnabla][INFO]: iter=169 {Training loss}=0.00451864767819643\n",
            "2023-06-04 15:40:00,961 [nnabla][INFO]: iter=169 {Training error}=0.0\n",
            "2023-06-04 15:40:00,996 [nnabla][INFO]: iter=179 {Training loss}=0.0038372580893337727\n",
            "2023-06-04 15:40:00,997 [nnabla][INFO]: iter=179 {Training error}=0.0\n",
            "2023-06-04 15:40:01,031 [nnabla][INFO]: iter=189 {Training loss}=0.0034851529635488987\n",
            "2023-06-04 15:40:01,031 [nnabla][INFO]: iter=189 {Training error}=0.0\n",
            "2023-06-04 15:40:01,068 [nnabla][INFO]: iter=199 {Training loss}=0.0030458178371191025\n",
            "2023-06-04 15:40:01,068 [nnabla][INFO]: iter=199 {Training error}=0.0\n",
            "2023-06-04 15:40:01,068 [nnabla][INFO]: iter=199 {Training time}=0.3883488178253174[sec/100iter] 4.284865140914917[sec]\n",
            "2023-06-04 15:40:01,091 [nnabla][INFO]: iter=200 {Test error}=0.0140625\n",
            "2023-06-04 15:40:01,128 [nnabla][INFO]: iter=209 {Training loss}=0.002658762037754059\n",
            "2023-06-04 15:40:01,128 [nnabla][INFO]: iter=209 {Training error}=0.0\n",
            "2023-06-04 15:40:01,163 [nnabla][INFO]: iter=219 {Training loss}=0.0023415463510900736\n",
            "2023-06-04 15:40:01,163 [nnabla][INFO]: iter=219 {Training error}=0.0\n",
            "2023-06-04 15:40:01,206 [nnabla][INFO]: iter=229 {Training loss}=0.0022810979280620813\n",
            "2023-06-04 15:40:01,206 [nnabla][INFO]: iter=229 {Training error}=0.0\n",
            "2023-06-04 15:40:01,241 [nnabla][INFO]: iter=239 {Training loss}=0.0018749473383650184\n",
            "2023-06-04 15:40:01,241 [nnabla][INFO]: iter=239 {Training error}=0.0\n",
            "2023-06-04 15:40:01,280 [nnabla][INFO]: iter=249 {Training loss}=0.0017708095256239176\n",
            "2023-06-04 15:40:01,280 [nnabla][INFO]: iter=249 {Training error}=0.0\n",
            "2023-06-04 15:40:01,316 [nnabla][INFO]: iter=259 {Training loss}=0.0016703952569514513\n",
            "2023-06-04 15:40:01,317 [nnabla][INFO]: iter=259 {Training error}=0.0\n",
            "2023-06-04 15:40:01,355 [nnabla][INFO]: iter=269 {Training loss}=0.0014418730279430747\n",
            "2023-06-04 15:40:01,355 [nnabla][INFO]: iter=269 {Training error}=0.0\n",
            "2023-06-04 15:40:01,391 [nnabla][INFO]: iter=279 {Training loss}=0.0013303962768986821\n",
            "2023-06-04 15:40:01,391 [nnabla][INFO]: iter=279 {Training error}=0.0\n",
            "2023-06-04 15:40:01,427 [nnabla][INFO]: iter=289 {Training loss}=0.0012897612759843469\n",
            "2023-06-04 15:40:01,427 [nnabla][INFO]: iter=289 {Training error}=0.0\n",
            "2023-06-04 15:40:01,464 [nnabla][INFO]: iter=299 {Training loss}=0.0011968535836786032\n",
            "2023-06-04 15:40:01,464 [nnabla][INFO]: iter=299 {Training error}=0.0\n",
            "2023-06-04 15:40:01,464 [nnabla][INFO]: iter=299 {Training time}=0.3959364891052246[sec/100iter] 4.680801630020142[sec]\n",
            "2023-06-04 15:40:01,486 [nnabla][INFO]: iter=300 {Test error}=0.01484375\n",
            "2023-06-04 15:40:01,523 [nnabla][INFO]: iter=309 {Training loss}=0.0010476612951606512\n",
            "2023-06-04 15:40:01,523 [nnabla][INFO]: iter=309 {Training error}=0.0\n",
            "2023-06-04 15:40:01,558 [nnabla][INFO]: iter=319 {Training loss}=0.0009810805786401033\n",
            "2023-06-04 15:40:01,558 [nnabla][INFO]: iter=319 {Training error}=0.0\n",
            "2023-06-04 15:40:01,592 [nnabla][INFO]: iter=329 {Training loss}=0.0009599507902748883\n",
            "2023-06-04 15:40:01,593 [nnabla][INFO]: iter=329 {Training error}=0.0\n",
            "2023-06-04 15:40:01,629 [nnabla][INFO]: iter=339 {Training loss}=0.0008548754267394543\n",
            "2023-06-04 15:40:01,629 [nnabla][INFO]: iter=339 {Training error}=0.0\n",
            "2023-06-04 15:40:01,669 [nnabla][INFO]: iter=349 {Training loss}=0.0008249582606367767\n",
            "2023-06-04 15:40:01,669 [nnabla][INFO]: iter=349 {Training error}=0.0\n",
            "2023-06-04 15:40:01,705 [nnabla][INFO]: iter=359 {Training loss}=0.0007557087810710073\n",
            "2023-06-04 15:40:01,705 [nnabla][INFO]: iter=359 {Training error}=0.0\n",
            "2023-06-04 15:40:01,741 [nnabla][INFO]: iter=369 {Training loss}=0.000733569439034909\n",
            "2023-06-04 15:40:01,741 [nnabla][INFO]: iter=369 {Training error}=0.0\n",
            "2023-06-04 15:40:01,778 [nnabla][INFO]: iter=379 {Training loss}=0.0006517558358609676\n",
            "2023-06-04 15:40:01,778 [nnabla][INFO]: iter=379 {Training error}=0.0\n",
            "2023-06-04 15:40:01,814 [nnabla][INFO]: iter=389 {Training loss}=0.0006413395749405026\n",
            "2023-06-04 15:40:01,815 [nnabla][INFO]: iter=389 {Training error}=0.0\n",
            "2023-06-04 15:40:01,849 [nnabla][INFO]: iter=399 {Training loss}=0.0006113235140219331\n",
            "2023-06-04 15:40:01,849 [nnabla][INFO]: iter=399 {Training error}=0.0\n",
            "2023-06-04 15:40:01,849 [nnabla][INFO]: iter=399 {Training time}=0.3850259780883789[sec/100iter] 5.0658276081085205[sec]\n",
            "2023-06-04 15:40:01,870 [nnabla][INFO]: iter=400 {Test error}=0.0140625\n",
            "2023-06-04 15:40:01,905 [nnabla][INFO]: iter=409 {Training loss}=0.0005880754324607551\n",
            "2023-06-04 15:40:01,905 [nnabla][INFO]: iter=409 {Training error}=0.0\n",
            "2023-06-04 15:40:01,940 [nnabla][INFO]: iter=419 {Training loss}=0.0005565618048422039\n",
            "2023-06-04 15:40:01,941 [nnabla][INFO]: iter=419 {Training error}=0.0\n",
            "2023-06-04 15:40:01,978 [nnabla][INFO]: iter=429 {Training loss}=0.00048107965267263353\n",
            "2023-06-04 15:40:01,978 [nnabla][INFO]: iter=429 {Training error}=0.0\n",
            "2023-06-04 15:40:02,013 [nnabla][INFO]: iter=439 {Training loss}=0.0005071352352388203\n",
            "2023-06-04 15:40:02,013 [nnabla][INFO]: iter=439 {Training error}=0.0\n",
            "2023-06-04 15:40:02,048 [nnabla][INFO]: iter=449 {Training loss}=0.00045234692515805364\n",
            "2023-06-04 15:40:02,049 [nnabla][INFO]: iter=449 {Training error}=0.0\n",
            "2023-06-04 15:40:02,086 [nnabla][INFO]: iter=459 {Training loss}=0.00043756840750575066\n",
            "2023-06-04 15:40:02,086 [nnabla][INFO]: iter=459 {Training error}=0.0\n",
            "2023-06-04 15:40:02,125 [nnabla][INFO]: iter=469 {Training loss}=0.00042551616206765175\n",
            "2023-06-04 15:40:02,125 [nnabla][INFO]: iter=469 {Training error}=0.0\n",
            "2023-06-04 15:40:02,161 [nnabla][INFO]: iter=479 {Training loss}=0.00042091665090993047\n",
            "2023-06-04 15:40:02,161 [nnabla][INFO]: iter=479 {Training error}=0.0\n",
            "2023-06-04 15:40:02,199 [nnabla][INFO]: iter=489 {Training loss}=0.00036449573235586286\n",
            "2023-06-04 15:40:02,199 [nnabla][INFO]: iter=489 {Training error}=0.0\n",
            "2023-06-04 15:40:02,234 [nnabla][INFO]: iter=499 {Training loss}=0.00037995949969626963\n",
            "2023-06-04 15:40:02,235 [nnabla][INFO]: iter=499 {Training error}=0.0\n",
            "2023-06-04 15:40:02,235 [nnabla][INFO]: iter=499 {Training time}=0.38562703132629395[sec/100iter] 5.4514546394348145[sec]\n",
            "2023-06-04 15:40:02,256 [nnabla][INFO]: iter=500 {Test error}=0.0140625\n",
            "2023-06-04 15:40:02,291 [nnabla][INFO]: iter=509 {Training loss}=0.0003447975032031536\n",
            "2023-06-04 15:40:02,291 [nnabla][INFO]: iter=509 {Training error}=0.0\n",
            "2023-06-04 15:40:02,327 [nnabla][INFO]: iter=519 {Training loss}=0.00033196178264915943\n",
            "2023-06-04 15:40:02,327 [nnabla][INFO]: iter=519 {Training error}=0.0\n",
            "2023-06-04 15:40:02,363 [nnabla][INFO]: iter=529 {Training loss}=0.0003197914338670671\n",
            "2023-06-04 15:40:02,363 [nnabla][INFO]: iter=529 {Training error}=0.0\n",
            "2023-06-04 15:40:02,399 [nnabla][INFO]: iter=539 {Training loss}=0.0003128874523099512\n",
            "2023-06-04 15:40:02,399 [nnabla][INFO]: iter=539 {Training error}=0.0\n",
            "2023-06-04 15:40:02,435 [nnabla][INFO]: iter=549 {Training loss}=0.0002944462466984987\n",
            "2023-06-04 15:40:02,436 [nnabla][INFO]: iter=549 {Training error}=0.0\n",
            "2023-06-04 15:40:02,474 [nnabla][INFO]: iter=559 {Training loss}=0.00029041635571047664\n",
            "2023-06-04 15:40:02,474 [nnabla][INFO]: iter=559 {Training error}=0.0\n",
            "2023-06-04 15:40:02,510 [nnabla][INFO]: iter=569 {Training loss}=0.00026679845177568495\n",
            "2023-06-04 15:40:02,510 [nnabla][INFO]: iter=569 {Training error}=0.0\n",
            "2023-06-04 15:40:02,545 [nnabla][INFO]: iter=579 {Training loss}=0.00026431825244799256\n",
            "2023-06-04 15:40:02,545 [nnabla][INFO]: iter=579 {Training error}=0.0\n",
            "2023-06-04 15:40:02,582 [nnabla][INFO]: iter=589 {Training loss}=0.0002460952091496438\n",
            "2023-06-04 15:40:02,582 [nnabla][INFO]: iter=589 {Training error}=0.0\n",
            "2023-06-04 15:40:02,617 [nnabla][INFO]: iter=599 {Training loss}=0.000259555468801409\n",
            "2023-06-04 15:40:02,618 [nnabla][INFO]: iter=599 {Training error}=0.0\n",
            "2023-06-04 15:40:02,618 [nnabla][INFO]: iter=599 {Training time}=0.3828752040863037[sec/100iter] 5.834329843521118[sec]\n",
            "2023-06-04 15:40:02,646 [nnabla][INFO]: iter=600 {Test error}=0.01484375\n",
            "2023-06-04 15:40:02,683 [nnabla][INFO]: iter=609 {Training loss}=0.00023495036293752491\n",
            "2023-06-04 15:40:02,683 [nnabla][INFO]: iter=609 {Training error}=0.0\n",
            "2023-06-04 15:40:02,718 [nnabla][INFO]: iter=619 {Training loss}=0.00022429654200095683\n",
            "2023-06-04 15:40:02,719 [nnabla][INFO]: iter=619 {Training error}=0.0\n",
            "2023-06-04 15:40:02,755 [nnabla][INFO]: iter=629 {Training loss}=0.0002214744163211435\n",
            "2023-06-04 15:40:02,755 [nnabla][INFO]: iter=629 {Training error}=0.0\n",
            "2023-06-04 15:40:02,791 [nnabla][INFO]: iter=639 {Training loss}=0.00021669648413080722\n",
            "2023-06-04 15:40:02,791 [nnabla][INFO]: iter=639 {Training error}=0.0\n",
            "2023-06-04 15:40:02,828 [nnabla][INFO]: iter=649 {Training loss}=0.00020468518778216094\n",
            "2023-06-04 15:40:02,828 [nnabla][INFO]: iter=649 {Training error}=0.0\n",
            "2023-06-04 15:40:02,864 [nnabla][INFO]: iter=659 {Training loss}=0.00019444062490947545\n",
            "2023-06-04 15:40:02,864 [nnabla][INFO]: iter=659 {Training error}=0.0\n",
            "2023-06-04 15:40:02,900 [nnabla][INFO]: iter=669 {Training loss}=0.00018815464864019305\n",
            "2023-06-04 15:40:02,900 [nnabla][INFO]: iter=669 {Training error}=0.0\n",
            "2023-06-04 15:40:02,938 [nnabla][INFO]: iter=679 {Training loss}=0.00017874220793601125\n",
            "2023-06-04 15:40:02,938 [nnabla][INFO]: iter=679 {Training error}=0.0\n",
            "2023-06-04 15:40:02,976 [nnabla][INFO]: iter=689 {Training loss}=0.00019082090875599533\n",
            "2023-06-04 15:40:02,976 [nnabla][INFO]: iter=689 {Training error}=0.0\n",
            "2023-06-04 15:40:03,013 [nnabla][INFO]: iter=699 {Training loss}=0.00017535130609758198\n",
            "2023-06-04 15:40:03,014 [nnabla][INFO]: iter=699 {Training error}=0.0\n",
            "2023-06-04 15:40:03,014 [nnabla][INFO]: iter=699 {Training time}=0.3961660861968994[sec/100iter] 6.230495929718018[sec]\n",
            "2023-06-04 15:40:03,039 [nnabla][INFO]: iter=700 {Test error}=0.0140625\n",
            "2023-06-04 15:40:03,076 [nnabla][INFO]: iter=709 {Training loss}=0.0001636673550819978\n",
            "2023-06-04 15:40:03,077 [nnabla][INFO]: iter=709 {Training error}=0.0\n",
            "2023-06-04 15:40:03,113 [nnabla][INFO]: iter=719 {Training loss}=0.00016760930884629488\n",
            "2023-06-04 15:40:03,113 [nnabla][INFO]: iter=719 {Training error}=0.0\n",
            "2023-06-04 15:40:03,154 [nnabla][INFO]: iter=729 {Training loss}=0.0001643727155169472\n",
            "2023-06-04 15:40:03,154 [nnabla][INFO]: iter=729 {Training error}=0.0\n",
            "2023-06-04 15:40:03,193 [nnabla][INFO]: iter=739 {Training loss}=0.0001478992053307593\n",
            "2023-06-04 15:40:03,193 [nnabla][INFO]: iter=739 {Training error}=0.0\n",
            "2023-06-04 15:40:03,230 [nnabla][INFO]: iter=749 {Training loss}=0.00014937878586351871\n",
            "2023-06-04 15:40:03,230 [nnabla][INFO]: iter=749 {Training error}=0.0\n",
            "2023-06-04 15:40:03,265 [nnabla][INFO]: iter=759 {Training loss}=0.00014760227350052446\n",
            "2023-06-04 15:40:03,266 [nnabla][INFO]: iter=759 {Training error}=0.0\n",
            "2023-06-04 15:40:03,302 [nnabla][INFO]: iter=769 {Training loss}=0.00013840690371580422\n",
            "2023-06-04 15:40:03,302 [nnabla][INFO]: iter=769 {Training error}=0.0\n",
            "2023-06-04 15:40:03,338 [nnabla][INFO]: iter=779 {Training loss}=0.00013322535960469395\n",
            "2023-06-04 15:40:03,339 [nnabla][INFO]: iter=779 {Training error}=0.0\n",
            "2023-06-04 15:40:03,375 [nnabla][INFO]: iter=789 {Training loss}=0.00013446761295199394\n",
            "2023-06-04 15:40:03,375 [nnabla][INFO]: iter=789 {Training error}=0.0\n",
            "2023-06-04 15:40:03,411 [nnabla][INFO]: iter=799 {Training loss}=0.00012947508366778493\n",
            "2023-06-04 15:40:03,411 [nnabla][INFO]: iter=799 {Training error}=0.0\n",
            "2023-06-04 15:40:03,411 [nnabla][INFO]: iter=799 {Training time}=0.39730215072631836[sec/100iter] 6.627798080444336[sec]\n",
            "2023-06-04 15:40:03,437 [nnabla][INFO]: iter=800 {Test error}=0.0140625\n",
            "2023-06-04 15:40:03,474 [nnabla][INFO]: iter=809 {Training loss}=0.0001307974598603323\n",
            "2023-06-04 15:40:03,474 [nnabla][INFO]: iter=809 {Training error}=0.0\n",
            "2023-06-04 15:40:03,511 [nnabla][INFO]: iter=819 {Training loss}=0.00012552141561172903\n",
            "2023-06-04 15:40:03,512 [nnabla][INFO]: iter=819 {Training error}=0.0\n",
            "2023-06-04 15:40:03,547 [nnabla][INFO]: iter=829 {Training loss}=0.00011513106437632814\n",
            "2023-06-04 15:40:03,547 [nnabla][INFO]: iter=829 {Training error}=0.0\n",
            "2023-06-04 15:40:03,583 [nnabla][INFO]: iter=839 {Training loss}=0.00011400411312934011\n",
            "2023-06-04 15:40:03,583 [nnabla][INFO]: iter=839 {Training error}=0.0\n",
            "2023-06-04 15:40:03,617 [nnabla][INFO]: iter=849 {Training loss}=0.00011739980254787952\n",
            "2023-06-04 15:40:03,617 [nnabla][INFO]: iter=849 {Training error}=0.0\n",
            "2023-06-04 15:40:03,652 [nnabla][INFO]: iter=859 {Training loss}=0.00010533827298786491\n",
            "2023-06-04 15:40:03,653 [nnabla][INFO]: iter=859 {Training error}=0.0\n",
            "2023-06-04 15:40:03,691 [nnabla][INFO]: iter=869 {Training loss}=0.00011097950482508168\n",
            "2023-06-04 15:40:03,691 [nnabla][INFO]: iter=869 {Training error}=0.0\n",
            "2023-06-04 15:40:03,728 [nnabla][INFO]: iter=879 {Training loss}=0.00010718934936448932\n",
            "2023-06-04 15:40:03,728 [nnabla][INFO]: iter=879 {Training error}=0.0\n",
            "2023-06-04 15:40:03,763 [nnabla][INFO]: iter=889 {Training loss}=0.0001027086254907772\n",
            "2023-06-04 15:40:03,763 [nnabla][INFO]: iter=889 {Training error}=0.0\n",
            "2023-06-04 15:40:03,799 [nnabla][INFO]: iter=899 {Training loss}=0.00010131590534001589\n",
            "2023-06-04 15:40:03,799 [nnabla][INFO]: iter=899 {Training error}=0.0\n",
            "2023-06-04 15:40:03,799 [nnabla][INFO]: iter=899 {Training time}=0.38813042640686035[sec/100iter] 7.015928506851196[sec]\n",
            "2023-06-04 15:40:03,820 [nnabla][INFO]: iter=900 {Test error}=0.0140625\n",
            "2023-06-04 15:40:03,868 [nnabla][INFO]: iter=909 {Training loss}=9.643143130233511e-05\n",
            "2023-06-04 15:40:03,869 [nnabla][INFO]: iter=909 {Training error}=0.0\n",
            "2023-06-04 15:40:03,907 [nnabla][INFO]: iter=919 {Training loss}=9.590729314368218e-05\n",
            "2023-06-04 15:40:03,907 [nnabla][INFO]: iter=919 {Training error}=0.0\n",
            "2023-06-04 15:40:03,943 [nnabla][INFO]: iter=929 {Training loss}=9.531314572086558e-05\n",
            "2023-06-04 15:40:03,943 [nnabla][INFO]: iter=929 {Training error}=0.0\n",
            "2023-06-04 15:40:03,979 [nnabla][INFO]: iter=939 {Training loss}=8.932939090300351e-05\n",
            "2023-06-04 15:40:03,979 [nnabla][INFO]: iter=939 {Training error}=0.0\n",
            "2023-06-04 15:40:04,016 [nnabla][INFO]: iter=949 {Training loss}=8.838802750688046e-05\n",
            "2023-06-04 15:40:04,016 [nnabla][INFO]: iter=949 {Training error}=0.0\n",
            "2023-06-04 15:40:04,050 [nnabla][INFO]: iter=959 {Training loss}=8.867619908414781e-05\n",
            "2023-06-04 15:40:04,051 [nnabla][INFO]: iter=959 {Training error}=0.0\n",
            "2023-06-04 15:40:04,086 [nnabla][INFO]: iter=969 {Training loss}=8.30481294542551e-05\n",
            "2023-06-04 15:40:04,086 [nnabla][INFO]: iter=969 {Training error}=0.0\n",
            "2023-06-04 15:40:04,123 [nnabla][INFO]: iter=979 {Training loss}=8.597595297032967e-05\n",
            "2023-06-04 15:40:04,123 [nnabla][INFO]: iter=979 {Training error}=0.0\n",
            "2023-06-04 15:40:04,166 [nnabla][INFO]: iter=989 {Training loss}=7.93357176007703e-05\n",
            "2023-06-04 15:40:04,166 [nnabla][INFO]: iter=989 {Training error}=0.0\n",
            "2023-06-04 15:40:04,203 [nnabla][INFO]: iter=999 {Training loss}=8.084969158517197e-05\n",
            "2023-06-04 15:40:04,204 [nnabla][INFO]: iter=999 {Training error}=0.0\n",
            "2023-06-04 15:40:04,204 [nnabla][INFO]: iter=999 {Training time}=0.40439891815185547[sec/100iter] 7.420327425003052[sec]\n",
            "2023-06-04 15:40:04,225 [nnabla][INFO]: iter=1000 {Test error}=0.01484375\n",
            "2023-06-04 15:40:04,238 [nnabla][INFO]: Solver state save (.h5): output/states_1000.h5\n",
            "2023-06-04 15:40:04,245 [nnabla][INFO]: Parameter save (.h5): output/params_1000.h5\n",
            "2023-06-04 15:40:04,245 [nnabla][INFO]: Checkpoint save (.json): output/checkpoint_1000.json\n",
            "2023-06-04 15:40:04,280 [nnabla][INFO]: iter=1009 {Training loss}=7.665254815947264e-05\n",
            "2023-06-04 15:40:04,280 [nnabla][INFO]: iter=1009 {Training error}=0.0\n",
            "2023-06-04 15:40:04,314 [nnabla][INFO]: iter=1019 {Training loss}=7.878780888859183e-05\n",
            "2023-06-04 15:40:04,314 [nnabla][INFO]: iter=1019 {Training error}=0.0\n",
            "2023-06-04 15:40:04,348 [nnabla][INFO]: iter=1029 {Training loss}=7.036788883851841e-05\n",
            "2023-06-04 15:40:04,349 [nnabla][INFO]: iter=1029 {Training error}=0.0\n",
            "2023-06-04 15:40:04,382 [nnabla][INFO]: iter=1039 {Training loss}=7.150865712901577e-05\n",
            "2023-06-04 15:40:04,383 [nnabla][INFO]: iter=1039 {Training error}=0.0\n",
            "2023-06-04 15:40:04,421 [nnabla][INFO]: iter=1049 {Training loss}=7.340735464822501e-05\n",
            "2023-06-04 15:40:04,421 [nnabla][INFO]: iter=1049 {Training error}=0.0\n",
            "2023-06-04 15:40:04,455 [nnabla][INFO]: iter=1059 {Training loss}=7.058164192130789e-05\n",
            "2023-06-04 15:40:04,456 [nnabla][INFO]: iter=1059 {Training error}=0.0\n",
            "2023-06-04 15:40:04,489 [nnabla][INFO]: iter=1069 {Training loss}=6.738522642990574e-05\n",
            "2023-06-04 15:40:04,489 [nnabla][INFO]: iter=1069 {Training error}=0.0\n",
            "2023-06-04 15:40:04,524 [nnabla][INFO]: iter=1079 {Training loss}=6.600288179470226e-05\n",
            "2023-06-04 15:40:04,524 [nnabla][INFO]: iter=1079 {Training error}=0.0\n",
            "2023-06-04 15:40:04,559 [nnabla][INFO]: iter=1089 {Training loss}=6.689174915663898e-05\n",
            "2023-06-04 15:40:04,559 [nnabla][INFO]: iter=1089 {Training error}=0.0\n",
            "2023-06-04 15:40:04,593 [nnabla][INFO]: iter=1099 {Training loss}=6.430968642234802e-05\n",
            "2023-06-04 15:40:04,594 [nnabla][INFO]: iter=1099 {Training error}=0.0\n",
            "2023-06-04 15:40:04,594 [nnabla][INFO]: iter=1099 {Training time}=0.38997459411621094[sec/100iter] 7.810302019119263[sec]\n",
            "2023-06-04 15:40:04,615 [nnabla][INFO]: iter=1100 {Test error}=0.0140625\n",
            "2023-06-04 15:40:04,649 [nnabla][INFO]: iter=1109 {Training loss}=6.302520341705531e-05\n",
            "2023-06-04 15:40:04,650 [nnabla][INFO]: iter=1109 {Training error}=0.0\n",
            "2023-06-04 15:40:04,684 [nnabla][INFO]: iter=1119 {Training loss}=6.130849214969203e-05\n",
            "2023-06-04 15:40:04,684 [nnabla][INFO]: iter=1119 {Training error}=0.0\n",
            "2023-06-04 15:40:04,729 [nnabla][INFO]: iter=1129 {Training loss}=5.833593968418427e-05\n",
            "2023-06-04 15:40:04,729 [nnabla][INFO]: iter=1129 {Training error}=0.0\n",
            "2023-06-04 15:40:04,764 [nnabla][INFO]: iter=1139 {Training loss}=6.093458432587795e-05\n",
            "2023-06-04 15:40:04,764 [nnabla][INFO]: iter=1139 {Training error}=0.0\n",
            "2023-06-04 15:40:04,799 [nnabla][INFO]: iter=1149 {Training loss}=5.916683585382998e-05\n",
            "2023-06-04 15:40:04,799 [nnabla][INFO]: iter=1149 {Training error}=0.0\n",
            "2023-06-04 15:40:04,834 [nnabla][INFO]: iter=1159 {Training loss}=5.607082130154595e-05\n",
            "2023-06-04 15:40:04,834 [nnabla][INFO]: iter=1159 {Training error}=0.0\n",
            "2023-06-04 15:40:04,869 [nnabla][INFO]: iter=1169 {Training loss}=5.510223491000943e-05\n",
            "2023-06-04 15:40:04,869 [nnabla][INFO]: iter=1169 {Training error}=0.0\n",
            "2023-06-04 15:40:04,904 [nnabla][INFO]: iter=1179 {Training loss}=5.8739435189636424e-05\n",
            "2023-06-04 15:40:04,905 [nnabla][INFO]: iter=1179 {Training error}=0.0\n",
            "2023-06-04 15:40:04,940 [nnabla][INFO]: iter=1189 {Training loss}=5.077609603176825e-05\n",
            "2023-06-04 15:40:04,940 [nnabla][INFO]: iter=1189 {Training error}=0.0\n",
            "2023-06-04 15:40:04,981 [nnabla][INFO]: iter=1199 {Training loss}=5.1873437769245356e-05\n",
            "2023-06-04 15:40:04,981 [nnabla][INFO]: iter=1199 {Training error}=0.0\n",
            "2023-06-04 15:40:04,981 [nnabla][INFO]: iter=1199 {Training time}=0.3874650001525879[sec/100iter] 8.19776701927185[sec]\n",
            "2023-06-04 15:40:05,004 [nnabla][INFO]: iter=1200 {Test error}=0.0140625\n",
            "2023-06-04 15:40:05,038 [nnabla][INFO]: iter=1209 {Training loss}=5.306397724780254e-05\n",
            "2023-06-04 15:40:05,038 [nnabla][INFO]: iter=1209 {Training error}=0.0\n",
            "2023-06-04 15:40:05,074 [nnabla][INFO]: iter=1219 {Training loss}=5.329870691639371e-05\n",
            "2023-06-04 15:40:05,074 [nnabla][INFO]: iter=1219 {Training error}=0.0\n",
            "2023-06-04 15:40:05,109 [nnabla][INFO]: iter=1229 {Training loss}=4.988384171156213e-05\n",
            "2023-06-04 15:40:05,109 [nnabla][INFO]: iter=1229 {Training error}=0.0\n",
            "2023-06-04 15:40:05,143 [nnabla][INFO]: iter=1239 {Training loss}=4.714345413958654e-05\n",
            "2023-06-04 15:40:05,144 [nnabla][INFO]: iter=1239 {Training error}=0.0\n",
            "2023-06-04 15:40:05,182 [nnabla][INFO]: iter=1249 {Training loss}=4.8938527470454574e-05\n",
            "2023-06-04 15:40:05,182 [nnabla][INFO]: iter=1249 {Training error}=0.0\n",
            "2023-06-04 15:40:05,219 [nnabla][INFO]: iter=1259 {Training loss}=4.746243212139234e-05\n",
            "2023-06-04 15:40:05,219 [nnabla][INFO]: iter=1259 {Training error}=0.0\n",
            "2023-06-04 15:40:05,257 [nnabla][INFO]: iter=1269 {Training loss}=4.5671109546674415e-05\n",
            "2023-06-04 15:40:05,258 [nnabla][INFO]: iter=1269 {Training error}=0.0\n",
            "2023-06-04 15:40:05,299 [nnabla][INFO]: iter=1279 {Training loss}=4.57306086900644e-05\n",
            "2023-06-04 15:40:05,299 [nnabla][INFO]: iter=1279 {Training error}=0.0\n",
            "2023-06-04 15:40:05,342 [nnabla][INFO]: iter=1289 {Training loss}=4.467795952223241e-05\n",
            "2023-06-04 15:40:05,343 [nnabla][INFO]: iter=1289 {Training error}=0.0\n",
            "2023-06-04 15:40:05,386 [nnabla][INFO]: iter=1299 {Training loss}=4.453505243873224e-05\n",
            "2023-06-04 15:40:05,387 [nnabla][INFO]: iter=1299 {Training error}=0.0\n",
            "2023-06-04 15:40:05,387 [nnabla][INFO]: iter=1299 {Training time}=0.40567779541015625[sec/100iter] 8.603444814682007[sec]\n",
            "2023-06-04 15:40:05,417 [nnabla][INFO]: iter=1300 {Test error}=0.01484375\n",
            "2023-06-04 15:40:05,456 [nnabla][INFO]: iter=1309 {Training loss}=4.319780055084266e-05\n",
            "2023-06-04 15:40:05,456 [nnabla][INFO]: iter=1309 {Training error}=0.0\n",
            "2023-06-04 15:40:05,501 [nnabla][INFO]: iter=1319 {Training loss}=4.379255187814124e-05\n",
            "2023-06-04 15:40:05,501 [nnabla][INFO]: iter=1319 {Training error}=0.0\n",
            "2023-06-04 15:40:05,538 [nnabla][INFO]: iter=1329 {Training loss}=4.086410262971185e-05\n",
            "2023-06-04 15:40:05,539 [nnabla][INFO]: iter=1329 {Training error}=0.0\n",
            "2023-06-04 15:40:05,579 [nnabla][INFO]: iter=1339 {Training loss}=4.152480323682539e-05\n",
            "2023-06-04 15:40:05,579 [nnabla][INFO]: iter=1339 {Training error}=0.0\n",
            "2023-06-04 15:40:05,619 [nnabla][INFO]: iter=1349 {Training loss}=4.0952887502498925e-05\n",
            "2023-06-04 15:40:05,619 [nnabla][INFO]: iter=1349 {Training error}=0.0\n",
            "2023-06-04 15:40:05,659 [nnabla][INFO]: iter=1359 {Training loss}=3.895812187693082e-05\n",
            "2023-06-04 15:40:05,659 [nnabla][INFO]: iter=1359 {Training error}=0.0\n",
            "2023-06-04 15:40:05,700 [nnabla][INFO]: iter=1369 {Training loss}=4.038423503516242e-05\n",
            "2023-06-04 15:40:05,700 [nnabla][INFO]: iter=1369 {Training error}=0.0\n",
            "2023-06-04 15:40:05,743 [nnabla][INFO]: iter=1379 {Training loss}=3.93745576729998e-05\n",
            "2023-06-04 15:40:05,744 [nnabla][INFO]: iter=1379 {Training error}=0.0\n",
            "2023-06-04 15:40:05,782 [nnabla][INFO]: iter=1389 {Training loss}=3.676001142594032e-05\n",
            "2023-06-04 15:40:05,782 [nnabla][INFO]: iter=1389 {Training error}=0.0\n",
            "2023-06-04 15:40:05,820 [nnabla][INFO]: iter=1399 {Training loss}=3.813275179709308e-05\n",
            "2023-06-04 15:40:05,820 [nnabla][INFO]: iter=1399 {Training error}=0.0\n",
            "2023-06-04 15:40:05,820 [nnabla][INFO]: iter=1399 {Training time}=0.43349671363830566[sec/100iter] 9.036941528320312[sec]\n",
            "2023-06-04 15:40:05,845 [nnabla][INFO]: iter=1400 {Test error}=0.0140625\n",
            "2023-06-04 15:40:05,882 [nnabla][INFO]: iter=1409 {Training loss}=3.700334855238907e-05\n",
            "2023-06-04 15:40:05,883 [nnabla][INFO]: iter=1409 {Training error}=0.0\n",
            "2023-06-04 15:40:05,922 [nnabla][INFO]: iter=1419 {Training loss}=3.551078771124594e-05\n",
            "2023-06-04 15:40:05,923 [nnabla][INFO]: iter=1419 {Training error}=0.0\n",
            "2023-06-04 15:40:05,961 [nnabla][INFO]: iter=1429 {Training loss}=3.5163404390914366e-05\n",
            "2023-06-04 15:40:05,961 [nnabla][INFO]: iter=1429 {Training error}=0.0\n",
            "2023-06-04 15:40:05,999 [nnabla][INFO]: iter=1439 {Training loss}=3.5271670640213415e-05\n",
            "2023-06-04 15:40:05,999 [nnabla][INFO]: iter=1439 {Training error}=0.0\n",
            "2023-06-04 15:40:06,039 [nnabla][INFO]: iter=1449 {Training loss}=3.6291203286964446e-05\n",
            "2023-06-04 15:40:06,039 [nnabla][INFO]: iter=1449 {Training error}=0.0\n",
            "2023-06-04 15:40:06,089 [nnabla][INFO]: iter=1459 {Training loss}=3.2415049645351246e-05\n",
            "2023-06-04 15:40:06,090 [nnabla][INFO]: iter=1459 {Training error}=0.0\n",
            "2023-06-04 15:40:06,127 [nnabla][INFO]: iter=1469 {Training loss}=3.5120348911732435e-05\n",
            "2023-06-04 15:40:06,127 [nnabla][INFO]: iter=1469 {Training error}=0.0\n",
            "2023-06-04 15:40:06,165 [nnabla][INFO]: iter=1479 {Training loss}=3.090050449827686e-05\n",
            "2023-06-04 15:40:06,166 [nnabla][INFO]: iter=1479 {Training error}=0.0\n",
            "2023-06-04 15:40:06,218 [nnabla][INFO]: iter=1489 {Training loss}=3.206497058272362e-05\n",
            "2023-06-04 15:40:06,219 [nnabla][INFO]: iter=1489 {Training error}=0.0\n",
            "2023-06-04 15:40:06,256 [nnabla][INFO]: iter=1499 {Training loss}=3.342509444337338e-05\n",
            "2023-06-04 15:40:06,257 [nnabla][INFO]: iter=1499 {Training error}=0.0\n",
            "2023-06-04 15:40:06,257 [nnabla][INFO]: iter=1499 {Training time}=0.436352014541626[sec/100iter] 9.473293542861938[sec]\n",
            "2023-06-04 15:40:06,280 [nnabla][INFO]: iter=1500 {Test error}=0.0140625\n",
            "2023-06-04 15:40:06,318 [nnabla][INFO]: iter=1509 {Training loss}=3.0699738999828696e-05\n",
            "2023-06-04 15:40:06,318 [nnabla][INFO]: iter=1509 {Training error}=0.0\n",
            "2023-06-04 15:40:06,358 [nnabla][INFO]: iter=1519 {Training loss}=2.9220240321592428e-05\n",
            "2023-06-04 15:40:06,358 [nnabla][INFO]: iter=1519 {Training error}=0.0\n",
            "2023-06-04 15:40:06,399 [nnabla][INFO]: iter=1529 {Training loss}=3.150462725898251e-05\n",
            "2023-06-04 15:40:06,399 [nnabla][INFO]: iter=1529 {Training error}=0.0\n",
            "2023-06-04 15:40:06,439 [nnabla][INFO]: iter=1539 {Training loss}=2.8954915251233615e-05\n",
            "2023-06-04 15:40:06,439 [nnabla][INFO]: iter=1539 {Training error}=0.0\n",
            "2023-06-04 15:40:06,478 [nnabla][INFO]: iter=1549 {Training loss}=3.133732025162317e-05\n",
            "2023-06-04 15:40:06,478 [nnabla][INFO]: iter=1549 {Training error}=0.0\n",
            "2023-06-04 15:40:06,522 [nnabla][INFO]: iter=1559 {Training loss}=2.9282131436048076e-05\n",
            "2023-06-04 15:40:06,522 [nnabla][INFO]: iter=1559 {Training error}=0.0\n",
            "2023-06-04 15:40:06,563 [nnabla][INFO]: iter=1569 {Training loss}=2.7959400540567003e-05\n",
            "2023-06-04 15:40:06,563 [nnabla][INFO]: iter=1569 {Training error}=0.0\n",
            "2023-06-04 15:40:06,601 [nnabla][INFO]: iter=1579 {Training loss}=2.714358015509788e-05\n",
            "2023-06-04 15:40:06,601 [nnabla][INFO]: iter=1579 {Training error}=0.0\n",
            "2023-06-04 15:40:06,647 [nnabla][INFO]: iter=1589 {Training loss}=2.8683391064987518e-05\n",
            "2023-06-04 15:40:06,647 [nnabla][INFO]: iter=1589 {Training error}=0.0\n",
            "2023-06-04 15:40:06,685 [nnabla][INFO]: iter=1599 {Training loss}=2.7545122065930627e-05\n",
            "2023-06-04 15:40:06,685 [nnabla][INFO]: iter=1599 {Training error}=0.0\n",
            "2023-06-04 15:40:06,685 [nnabla][INFO]: iter=1599 {Training time}=0.4286339282989502[sec/100iter] 9.901927471160889[sec]\n",
            "2023-06-04 15:40:06,709 [nnabla][INFO]: iter=1600 {Test error}=0.0140625\n",
            "2023-06-04 15:40:06,748 [nnabla][INFO]: iter=1609 {Training loss}=2.7666017558658496e-05\n",
            "2023-06-04 15:40:06,748 [nnabla][INFO]: iter=1609 {Training error}=0.0\n",
            "2023-06-04 15:40:06,786 [nnabla][INFO]: iter=1619 {Training loss}=2.640252023411449e-05\n",
            "2023-06-04 15:40:06,786 [nnabla][INFO]: iter=1619 {Training error}=0.0\n",
            "2023-06-04 15:40:06,828 [nnabla][INFO]: iter=1629 {Training loss}=2.6065821657539345e-05\n",
            "2023-06-04 15:40:06,828 [nnabla][INFO]: iter=1629 {Training error}=0.0\n",
            "2023-06-04 15:40:06,868 [nnabla][INFO]: iter=1639 {Training loss}=2.6420277208671905e-05\n",
            "2023-06-04 15:40:06,869 [nnabla][INFO]: iter=1639 {Training error}=0.0\n",
            "2023-06-04 15:40:06,908 [nnabla][INFO]: iter=1649 {Training loss}=2.486478115315549e-05\n",
            "2023-06-04 15:40:06,908 [nnabla][INFO]: iter=1649 {Training error}=0.0\n",
            "2023-06-04 15:40:06,949 [nnabla][INFO]: iter=1659 {Training loss}=2.619810948090162e-05\n",
            "2023-06-04 15:40:06,950 [nnabla][INFO]: iter=1659 {Training error}=0.0\n",
            "2023-06-04 15:40:06,989 [nnabla][INFO]: iter=1669 {Training loss}=2.338791273359675e-05\n",
            "2023-06-04 15:40:06,989 [nnabla][INFO]: iter=1669 {Training error}=0.0\n",
            "2023-06-04 15:40:07,028 [nnabla][INFO]: iter=1679 {Training loss}=2.6488356525078416e-05\n",
            "2023-06-04 15:40:07,029 [nnabla][INFO]: iter=1679 {Training error}=0.0\n",
            "2023-06-04 15:40:07,069 [nnabla][INFO]: iter=1689 {Training loss}=2.3056441932567395e-05\n",
            "2023-06-04 15:40:07,069 [nnabla][INFO]: iter=1689 {Training error}=0.0\n",
            "2023-06-04 15:40:07,109 [nnabla][INFO]: iter=1699 {Training loss}=2.4896353352232836e-05\n",
            "2023-06-04 15:40:07,109 [nnabla][INFO]: iter=1699 {Training error}=0.0\n",
            "2023-06-04 15:40:07,109 [nnabla][INFO]: iter=1699 {Training time}=0.4238293170928955[sec/100iter] 10.325756788253784[sec]\n",
            "2023-06-04 15:40:07,142 [nnabla][INFO]: iter=1700 {Test error}=0.01484375\n",
            "2023-06-04 15:40:07,182 [nnabla][INFO]: iter=1709 {Training loss}=2.297190803801641e-05\n",
            "2023-06-04 15:40:07,182 [nnabla][INFO]: iter=1709 {Training error}=0.0\n",
            "2023-06-04 15:40:07,223 [nnabla][INFO]: iter=1719 {Training loss}=2.3411183065036312e-05\n",
            "2023-06-04 15:40:07,224 [nnabla][INFO]: iter=1719 {Training error}=0.0\n",
            "2023-06-04 15:40:07,270 [nnabla][INFO]: iter=1729 {Training loss}=2.27037126023788e-05\n",
            "2023-06-04 15:40:07,270 [nnabla][INFO]: iter=1729 {Training error}=0.0\n",
            "2023-06-04 15:40:07,310 [nnabla][INFO]: iter=1739 {Training loss}=2.3003018213785253e-05\n",
            "2023-06-04 15:40:07,310 [nnabla][INFO]: iter=1739 {Training error}=0.0\n",
            "2023-06-04 15:40:07,354 [nnabla][INFO]: iter=1749 {Training loss}=2.1831850972375832e-05\n",
            "2023-06-04 15:40:07,354 [nnabla][INFO]: iter=1749 {Training error}=0.0\n",
            "2023-06-04 15:40:07,401 [nnabla][INFO]: iter=1759 {Training loss}=2.243729613837786e-05\n",
            "2023-06-04 15:40:07,401 [nnabla][INFO]: iter=1759 {Training error}=0.0\n",
            "2023-06-04 15:40:07,442 [nnabla][INFO]: iter=1769 {Training loss}=2.171433879993856e-05\n",
            "2023-06-04 15:40:07,442 [nnabla][INFO]: iter=1769 {Training error}=0.0\n",
            "2023-06-04 15:40:07,483 [nnabla][INFO]: iter=1779 {Training loss}=2.1527244825847447e-05\n",
            "2023-06-04 15:40:07,483 [nnabla][INFO]: iter=1779 {Training error}=0.0\n",
            "2023-06-04 15:40:07,525 [nnabla][INFO]: iter=1789 {Training loss}=2.0518951714620925e-05\n",
            "2023-06-04 15:40:07,525 [nnabla][INFO]: iter=1789 {Training error}=0.0\n",
            "2023-06-04 15:40:07,570 [nnabla][INFO]: iter=1799 {Training loss}=2.123797094100155e-05\n",
            "2023-06-04 15:40:07,571 [nnabla][INFO]: iter=1799 {Training error}=0.0\n",
            "2023-06-04 15:40:07,571 [nnabla][INFO]: iter=1799 {Training time}=0.46219778060913086[sec/100iter] 10.787954568862915[sec]\n",
            "2023-06-04 15:40:07,597 [nnabla][INFO]: iter=1800 {Test error}=0.0140625\n",
            "2023-06-04 15:40:07,639 [nnabla][INFO]: iter=1809 {Training loss}=2.067851528408937e-05\n",
            "2023-06-04 15:40:07,639 [nnabla][INFO]: iter=1809 {Training error}=0.0\n",
            "2023-06-04 15:40:07,681 [nnabla][INFO]: iter=1819 {Training loss}=2.013494486163836e-05\n",
            "2023-06-04 15:40:07,682 [nnabla][INFO]: iter=1819 {Training error}=0.0\n",
            "2023-06-04 15:40:07,728 [nnabla][INFO]: iter=1829 {Training loss}=1.9195958884665743e-05\n",
            "2023-06-04 15:40:07,729 [nnabla][INFO]: iter=1829 {Training error}=0.0\n",
            "2023-06-04 15:40:07,772 [nnabla][INFO]: iter=1839 {Training loss}=2.053585740213748e-05\n",
            "2023-06-04 15:40:07,773 [nnabla][INFO]: iter=1839 {Training error}=0.0\n",
            "2023-06-04 15:40:07,813 [nnabla][INFO]: iter=1849 {Training loss}=1.951700141944457e-05\n",
            "2023-06-04 15:40:07,814 [nnabla][INFO]: iter=1849 {Training error}=0.0\n",
            "2023-06-04 15:40:07,857 [nnabla][INFO]: iter=1859 {Training loss}=1.9311497453600168e-05\n",
            "2023-06-04 15:40:07,857 [nnabla][INFO]: iter=1859 {Training error}=0.0\n",
            "2023-06-04 15:40:07,898 [nnabla][INFO]: iter=1869 {Training loss}=1.9152103050146252e-05\n",
            "2023-06-04 15:40:07,899 [nnabla][INFO]: iter=1869 {Training error}=0.0\n",
            "2023-06-04 15:40:07,942 [nnabla][INFO]: iter=1879 {Training loss}=1.9661161786643788e-05\n",
            "2023-06-04 15:40:07,942 [nnabla][INFO]: iter=1879 {Training error}=0.0\n",
            "2023-06-04 15:40:07,987 [nnabla][INFO]: iter=1889 {Training loss}=1.7829956050263718e-05\n",
            "2023-06-04 15:40:07,987 [nnabla][INFO]: iter=1889 {Training error}=0.0\n",
            "2023-06-04 15:40:08,030 [nnabla][INFO]: iter=1899 {Training loss}=1.8429038391332142e-05\n",
            "2023-06-04 15:40:08,030 [nnabla][INFO]: iter=1899 {Training error}=0.0\n",
            "2023-06-04 15:40:08,030 [nnabla][INFO]: iter=1899 {Training time}=0.4585692882537842[sec/100iter] 11.2465238571167[sec]\n",
            "2023-06-04 15:40:08,056 [nnabla][INFO]: iter=1900 {Test error}=0.0140625\n",
            "2023-06-04 15:40:08,098 [nnabla][INFO]: iter=1909 {Training loss}=1.7694908819976263e-05\n",
            "2023-06-04 15:40:08,098 [nnabla][INFO]: iter=1909 {Training error}=0.0\n",
            "2023-06-04 15:40:08,147 [nnabla][INFO]: iter=1919 {Training loss}=1.8311846361029893e-05\n",
            "2023-06-04 15:40:08,147 [nnabla][INFO]: iter=1919 {Training error}=0.0\n",
            "2023-06-04 15:40:08,188 [nnabla][INFO]: iter=1929 {Training loss}=1.7276726794079877e-05\n",
            "2023-06-04 15:40:08,188 [nnabla][INFO]: iter=1929 {Training error}=0.0\n",
            "2023-06-04 15:40:08,230 [nnabla][INFO]: iter=1939 {Training loss}=1.6724834495107643e-05\n",
            "2023-06-04 15:40:08,231 [nnabla][INFO]: iter=1939 {Training error}=0.0\n",
            "2023-06-04 15:40:08,277 [nnabla][INFO]: iter=1949 {Training loss}=1.768972106219735e-05\n",
            "2023-06-04 15:40:08,277 [nnabla][INFO]: iter=1949 {Training error}=0.0\n",
            "2023-06-04 15:40:08,321 [nnabla][INFO]: iter=1959 {Training loss}=1.7649885194259696e-05\n",
            "2023-06-04 15:40:08,321 [nnabla][INFO]: iter=1959 {Training error}=0.0\n",
            "2023-06-04 15:40:08,367 [nnabla][INFO]: iter=1969 {Training loss}=1.646060627535917e-05\n",
            "2023-06-04 15:40:08,367 [nnabla][INFO]: iter=1969 {Training error}=0.0\n",
            "2023-06-04 15:40:08,408 [nnabla][INFO]: iter=1979 {Training loss}=1.5961704775691032e-05\n",
            "2023-06-04 15:40:08,408 [nnabla][INFO]: iter=1979 {Training error}=0.0\n",
            "2023-06-04 15:40:08,449 [nnabla][INFO]: iter=1989 {Training loss}=1.6876201698323712e-05\n",
            "2023-06-04 15:40:08,449 [nnabla][INFO]: iter=1989 {Training error}=0.0\n",
            "2023-06-04 15:40:08,491 [nnabla][INFO]: iter=1999 {Training loss}=1.6239197066170163e-05\n",
            "2023-06-04 15:40:08,491 [nnabla][INFO]: iter=1999 {Training error}=0.0\n",
            "2023-06-04 15:40:08,491 [nnabla][INFO]: iter=1999 {Training time}=0.46095967292785645[sec/100iter] 11.707483530044556[sec]\n",
            "2023-06-04 15:40:08,516 [nnabla][INFO]: iter=2000 {Test error}=0.01484375\n",
            "2023-06-04 15:40:08,536 [nnabla][INFO]: Solver state save (.h5): output/states_2000.h5\n",
            "2023-06-04 15:40:08,548 [nnabla][INFO]: Parameter save (.h5): output/params_2000.h5\n",
            "2023-06-04 15:40:08,552 [nnabla][INFO]: Checkpoint save (.json): output/checkpoint_2000.json\n",
            "2023-06-04 15:40:08,595 [nnabla][INFO]: iter=2009 {Training loss}=1.6234982467722148e-05\n",
            "2023-06-04 15:40:08,595 [nnabla][INFO]: iter=2009 {Training error}=0.0\n",
            "2023-06-04 15:40:08,637 [nnabla][INFO]: iter=2019 {Training loss}=1.5582525520585477e-05\n",
            "2023-06-04 15:40:08,637 [nnabla][INFO]: iter=2019 {Training error}=0.0\n",
            "2023-06-04 15:40:08,681 [nnabla][INFO]: iter=2029 {Training loss}=1.5103740224731155e-05\n",
            "2023-06-04 15:40:08,681 [nnabla][INFO]: iter=2029 {Training error}=0.0\n",
            "2023-06-04 15:40:08,724 [nnabla][INFO]: iter=2039 {Training loss}=1.5772766346344724e-05\n",
            "2023-06-04 15:40:08,725 [nnabla][INFO]: iter=2039 {Training error}=0.0\n",
            "2023-06-04 15:40:08,765 [nnabla][INFO]: iter=2049 {Training loss}=1.5494171748287044e-05\n",
            "2023-06-04 15:40:08,765 [nnabla][INFO]: iter=2049 {Training error}=0.0\n",
            "2023-06-04 15:40:08,806 [nnabla][INFO]: iter=2059 {Training loss}=1.5653444279450923e-05\n",
            "2023-06-04 15:40:08,806 [nnabla][INFO]: iter=2059 {Training error}=0.0\n",
            "2023-06-04 15:40:08,846 [nnabla][INFO]: iter=2069 {Training loss}=1.4139603081275709e-05\n",
            "2023-06-04 15:40:08,846 [nnabla][INFO]: iter=2069 {Training error}=0.0\n",
            "2023-06-04 15:40:08,891 [nnabla][INFO]: iter=2079 {Training loss}=1.4555419511452783e-05\n",
            "2023-06-04 15:40:08,892 [nnabla][INFO]: iter=2079 {Training error}=0.0\n",
            "2023-06-04 15:40:08,933 [nnabla][INFO]: iter=2089 {Training loss}=1.4832819942967035e-05\n",
            "2023-06-04 15:40:08,933 [nnabla][INFO]: iter=2089 {Training error}=0.0\n",
            "2023-06-04 15:40:08,974 [nnabla][INFO]: iter=2099 {Training loss}=1.4430595911107957e-05\n",
            "2023-06-04 15:40:08,975 [nnabla][INFO]: iter=2099 {Training error}=0.0\n",
            "2023-06-04 15:40:08,975 [nnabla][INFO]: iter=2099 {Training time}=0.4839634895324707[sec/100iter] 12.191447019577026[sec]\n",
            "2023-06-04 15:40:09,001 [nnabla][INFO]: iter=2100 {Test error}=0.0140625\n",
            "2023-06-04 15:40:09,042 [nnabla][INFO]: iter=2109 {Training loss}=1.4529456166201271e-05\n",
            "2023-06-04 15:40:09,043 [nnabla][INFO]: iter=2109 {Training error}=0.0\n",
            "2023-06-04 15:40:09,083 [nnabla][INFO]: iter=2119 {Training loss}=1.2929990589327645e-05\n",
            "2023-06-04 15:40:09,084 [nnabla][INFO]: iter=2119 {Training error}=0.0\n",
            "2023-06-04 15:40:09,124 [nnabla][INFO]: iter=2129 {Training loss}=1.5315319615183398e-05\n",
            "2023-06-04 15:40:09,125 [nnabla][INFO]: iter=2129 {Training error}=0.0\n",
            "2023-06-04 15:40:09,177 [nnabla][INFO]: iter=2139 {Training loss}=1.3035934898653068e-05\n",
            "2023-06-04 15:40:09,177 [nnabla][INFO]: iter=2139 {Training error}=0.0\n",
            "2023-06-04 15:40:09,218 [nnabla][INFO]: iter=2149 {Training loss}=1.367719960398972e-05\n",
            "2023-06-04 15:40:09,218 [nnabla][INFO]: iter=2149 {Training error}=0.0\n",
            "2023-06-04 15:40:09,261 [nnabla][INFO]: iter=2159 {Training loss}=1.3434442735160701e-05\n",
            "2023-06-04 15:40:09,261 [nnabla][INFO]: iter=2159 {Training error}=0.0\n",
            "2023-06-04 15:40:09,308 [nnabla][INFO]: iter=2169 {Training loss}=1.3574528566095978e-05\n",
            "2023-06-04 15:40:09,308 [nnabla][INFO]: iter=2169 {Training error}=0.0\n",
            "2023-06-04 15:40:09,351 [nnabla][INFO]: iter=2179 {Training loss}=1.2714153854176402e-05\n",
            "2023-06-04 15:40:09,351 [nnabla][INFO]: iter=2179 {Training error}=0.0\n",
            "2023-06-04 15:40:09,394 [nnabla][INFO]: iter=2189 {Training loss}=1.3078894880891312e-05\n",
            "2023-06-04 15:40:09,395 [nnabla][INFO]: iter=2189 {Training error}=0.0\n",
            "2023-06-04 15:40:09,436 [nnabla][INFO]: iter=2199 {Training loss}=1.2886383046861738e-05\n",
            "2023-06-04 15:40:09,436 [nnabla][INFO]: iter=2199 {Training error}=0.0\n",
            "2023-06-04 15:40:09,436 [nnabla][INFO]: iter=2199 {Training time}=0.4616062641143799[sec/100iter] 12.653053283691406[sec]\n",
            "2023-06-04 15:40:09,469 [nnabla][INFO]: iter=2200 {Test error}=0.0140625\n",
            "2023-06-04 15:40:09,512 [nnabla][INFO]: iter=2209 {Training loss}=1.3025934094912373e-05\n",
            "2023-06-04 15:40:09,512 [nnabla][INFO]: iter=2209 {Training error}=0.0\n",
            "2023-06-04 15:40:09,558 [nnabla][INFO]: iter=2219 {Training loss}=1.2074521691829432e-05\n",
            "2023-06-04 15:40:09,558 [nnabla][INFO]: iter=2219 {Training error}=0.0\n",
            "2023-06-04 15:40:09,595 [nnabla][INFO]: iter=2229 {Training loss}=1.1805410395027138e-05\n",
            "2023-06-04 15:40:09,595 [nnabla][INFO]: iter=2229 {Training error}=0.0\n",
            "2023-06-04 15:40:09,641 [nnabla][INFO]: iter=2239 {Training loss}=1.264605361939175e-05\n",
            "2023-06-04 15:40:09,641 [nnabla][INFO]: iter=2239 {Training error}=0.0\n",
            "2023-06-04 15:40:09,678 [nnabla][INFO]: iter=2249 {Training loss}=1.2348493328318e-05\n",
            "2023-06-04 15:40:09,678 [nnabla][INFO]: iter=2249 {Training error}=0.0\n",
            "2023-06-04 15:40:09,714 [nnabla][INFO]: iter=2259 {Training loss}=1.2441790204320569e-05\n",
            "2023-06-04 15:40:09,714 [nnabla][INFO]: iter=2259 {Training error}=0.0\n",
            "2023-06-04 15:40:09,751 [nnabla][INFO]: iter=2269 {Training loss}=1.1362086297594942e-05\n",
            "2023-06-04 15:40:09,752 [nnabla][INFO]: iter=2269 {Training error}=0.0\n",
            "2023-06-04 15:40:09,793 [nnabla][INFO]: iter=2279 {Training loss}=1.157476617663633e-05\n",
            "2023-06-04 15:40:09,793 [nnabla][INFO]: iter=2279 {Training error}=0.0\n",
            "2023-06-04 15:40:09,829 [nnabla][INFO]: iter=2289 {Training loss}=1.16055807666271e-05\n",
            "2023-06-04 15:40:09,830 [nnabla][INFO]: iter=2289 {Training error}=0.0\n",
            "2023-06-04 15:40:09,866 [nnabla][INFO]: iter=2299 {Training loss}=1.1817997801699676e-05\n",
            "2023-06-04 15:40:09,866 [nnabla][INFO]: iter=2299 {Training error}=0.0\n",
            "2023-06-04 15:40:09,866 [nnabla][INFO]: iter=2299 {Training time}=0.42989468574523926[sec/100iter] 13.082947969436646[sec]\n",
            "2023-06-04 15:40:09,888 [nnabla][INFO]: iter=2300 {Test error}=0.0140625\n",
            "2023-06-04 15:40:09,924 [nnabla][INFO]: iter=2309 {Training loss}=1.1141779395984486e-05\n",
            "2023-06-04 15:40:09,924 [nnabla][INFO]: iter=2309 {Training error}=0.0\n",
            "2023-06-04 15:40:09,959 [nnabla][INFO]: iter=2319 {Training loss}=1.1315510164422449e-05\n",
            "2023-06-04 15:40:09,960 [nnabla][INFO]: iter=2319 {Training error}=0.0\n",
            "2023-06-04 15:40:09,995 [nnabla][INFO]: iter=2329 {Training loss}=1.1120093404315412e-05\n",
            "2023-06-04 15:40:09,995 [nnabla][INFO]: iter=2329 {Training error}=0.0\n",
            "2023-06-04 15:40:10,033 [nnabla][INFO]: iter=2339 {Training loss}=1.1066799743275624e-05\n",
            "2023-06-04 15:40:10,033 [nnabla][INFO]: iter=2339 {Training error}=0.0\n",
            "2023-06-04 15:40:10,075 [nnabla][INFO]: iter=2349 {Training loss}=1.079198864317732e-05\n",
            "2023-06-04 15:40:10,075 [nnabla][INFO]: iter=2349 {Training error}=0.0\n",
            "2023-06-04 15:40:10,110 [nnabla][INFO]: iter=2359 {Training loss}=1.0691566785681061e-05\n",
            "2023-06-04 15:40:10,110 [nnabla][INFO]: iter=2359 {Training error}=0.0\n",
            "2023-06-04 15:40:10,145 [nnabla][INFO]: iter=2369 {Training loss}=1.0696104254748207e-05\n",
            "2023-06-04 15:40:10,145 [nnabla][INFO]: iter=2369 {Training error}=0.0\n",
            "2023-06-04 15:40:10,180 [nnabla][INFO]: iter=2379 {Training loss}=1.0277713954565115e-05\n",
            "2023-06-04 15:40:10,180 [nnabla][INFO]: iter=2379 {Training error}=0.0\n",
            "2023-06-04 15:40:10,216 [nnabla][INFO]: iter=2389 {Training loss}=1.0791212844196707e-05\n",
            "2023-06-04 15:40:10,216 [nnabla][INFO]: iter=2389 {Training error}=0.0\n",
            "2023-06-04 15:40:10,250 [nnabla][INFO]: iter=2399 {Training loss}=1.0222461241937708e-05\n",
            "2023-06-04 15:40:10,250 [nnabla][INFO]: iter=2399 {Training error}=0.0\n",
            "2023-06-04 15:40:10,250 [nnabla][INFO]: iter=2399 {Training time}=0.3840012550354004[sec/100iter] 13.466949224472046[sec]\n",
            "2023-06-04 15:40:10,271 [nnabla][INFO]: iter=2400 {Test error}=0.01484375\n",
            "2023-06-04 15:40:10,307 [nnabla][INFO]: iter=2409 {Training loss}=1.0136462151422165e-05\n",
            "2023-06-04 15:40:10,307 [nnabla][INFO]: iter=2409 {Training error}=0.0\n",
            "2023-06-04 15:40:10,349 [nnabla][INFO]: iter=2419 {Training loss}=1.0298506822437048e-05\n",
            "2023-06-04 15:40:10,349 [nnabla][INFO]: iter=2419 {Training error}=0.0\n",
            "2023-06-04 15:40:10,386 [nnabla][INFO]: iter=2429 {Training loss}=9.949477316695265e-06\n",
            "2023-06-04 15:40:10,387 [nnabla][INFO]: iter=2429 {Training error}=0.0\n",
            "2023-06-04 15:40:10,422 [nnabla][INFO]: iter=2439 {Training loss}=1.0014466170105152e-05\n",
            "2023-06-04 15:40:10,422 [nnabla][INFO]: iter=2439 {Training error}=0.0\n",
            "2023-06-04 15:40:10,457 [nnabla][INFO]: iter=2449 {Training loss}=9.49384138948517e-06\n",
            "2023-06-04 15:40:10,458 [nnabla][INFO]: iter=2449 {Training error}=0.0\n",
            "2023-06-04 15:40:10,493 [nnabla][INFO]: iter=2459 {Training loss}=9.808938557398506e-06\n",
            "2023-06-04 15:40:10,493 [nnabla][INFO]: iter=2459 {Training error}=0.0\n",
            "2023-06-04 15:40:10,528 [nnabla][INFO]: iter=2469 {Training loss}=9.359580872114748e-06\n",
            "2023-06-04 15:40:10,529 [nnabla][INFO]: iter=2469 {Training error}=0.0\n",
            "2023-06-04 15:40:10,563 [nnabla][INFO]: iter=2479 {Training loss}=9.24324649531627e-06\n",
            "2023-06-04 15:40:10,563 [nnabla][INFO]: iter=2479 {Training error}=0.0\n",
            "2023-06-04 15:40:10,603 [nnabla][INFO]: iter=2489 {Training loss}=9.453053280594759e-06\n",
            "2023-06-04 15:40:10,603 [nnabla][INFO]: iter=2489 {Training error}=0.0\n",
            "2023-06-04 15:40:10,637 [nnabla][INFO]: iter=2499 {Training loss}=9.473725185671356e-06\n",
            "2023-06-04 15:40:10,637 [nnabla][INFO]: iter=2499 {Training error}=0.0\n",
            "2023-06-04 15:40:10,637 [nnabla][INFO]: iter=2499 {Training time}=0.38657641410827637[sec/100iter] 13.853525638580322[sec]\n",
            "2023-06-04 15:40:10,660 [nnabla][INFO]: iter=2500 {Test error}=0.0140625\n",
            "2023-06-04 15:40:10,694 [nnabla][INFO]: iter=2509 {Training loss}=9.432294973521493e-06\n",
            "2023-06-04 15:40:10,694 [nnabla][INFO]: iter=2509 {Training error}=0.0\n",
            "2023-06-04 15:40:10,728 [nnabla][INFO]: iter=2519 {Training loss}=9.134868378168903e-06\n",
            "2023-06-04 15:40:10,729 [nnabla][INFO]: iter=2519 {Training error}=0.0\n",
            "2023-06-04 15:40:10,763 [nnabla][INFO]: iter=2529 {Training loss}=9.139713256445248e-06\n",
            "2023-06-04 15:40:10,763 [nnabla][INFO]: iter=2529 {Training error}=0.0\n",
            "2023-06-04 15:40:10,797 [nnabla][INFO]: iter=2539 {Training loss}=8.654064004076645e-06\n",
            "2023-06-04 15:40:10,798 [nnabla][INFO]: iter=2539 {Training error}=0.0\n",
            "2023-06-04 15:40:10,832 [nnabla][INFO]: iter=2549 {Training loss}=8.759210686548613e-06\n",
            "2023-06-04 15:40:10,832 [nnabla][INFO]: iter=2549 {Training error}=0.0\n",
            "2023-06-04 15:40:10,866 [nnabla][INFO]: iter=2559 {Training loss}=8.890429853636306e-06\n",
            "2023-06-04 15:40:10,866 [nnabla][INFO]: iter=2559 {Training error}=0.0\n",
            "2023-06-04 15:40:10,900 [nnabla][INFO]: iter=2569 {Training loss}=8.261448783741798e-06\n",
            "2023-06-04 15:40:10,901 [nnabla][INFO]: iter=2569 {Training error}=0.0\n",
            "2023-06-04 15:40:10,936 [nnabla][INFO]: iter=2579 {Training loss}=8.877300388121512e-06\n",
            "2023-06-04 15:40:10,936 [nnabla][INFO]: iter=2579 {Training error}=0.0\n",
            "2023-06-04 15:40:10,976 [nnabla][INFO]: iter=2589 {Training loss}=8.791268555796705e-06\n",
            "2023-06-04 15:40:10,976 [nnabla][INFO]: iter=2589 {Training error}=0.0\n",
            "2023-06-04 15:40:11,010 [nnabla][INFO]: iter=2599 {Training loss}=7.892518624430522e-06\n",
            "2023-06-04 15:40:11,011 [nnabla][INFO]: iter=2599 {Training error}=0.0\n",
            "2023-06-04 15:40:11,011 [nnabla][INFO]: iter=2599 {Training time}=0.373720645904541[sec/100iter] 14.227246284484863[sec]\n",
            "2023-06-04 15:40:11,031 [nnabla][INFO]: iter=2600 {Test error}=0.0140625\n",
            "2023-06-04 15:40:11,065 [nnabla][INFO]: iter=2609 {Training loss}=8.580231224186718e-06\n",
            "2023-06-04 15:40:11,066 [nnabla][INFO]: iter=2609 {Training error}=0.0\n",
            "2023-06-04 15:40:11,099 [nnabla][INFO]: iter=2619 {Training loss}=8.323407200805377e-06\n",
            "2023-06-04 15:40:11,099 [nnabla][INFO]: iter=2619 {Training error}=0.0\n",
            "2023-06-04 15:40:11,132 [nnabla][INFO]: iter=2629 {Training loss}=8.067319868132472e-06\n",
            "2023-06-04 15:40:11,132 [nnabla][INFO]: iter=2629 {Training error}=0.0\n",
            "2023-06-04 15:40:11,167 [nnabla][INFO]: iter=2639 {Training loss}=7.835444193915464e-06\n",
            "2023-06-04 15:40:11,167 [nnabla][INFO]: iter=2639 {Training error}=0.0\n",
            "2023-06-04 15:40:11,200 [nnabla][INFO]: iter=2649 {Training loss}=8.236344910983462e-06\n",
            "2023-06-04 15:40:11,200 [nnabla][INFO]: iter=2649 {Training error}=0.0\n",
            "2023-06-04 15:40:11,237 [nnabla][INFO]: iter=2659 {Training loss}=7.81040307629155e-06\n",
            "2023-06-04 15:40:11,237 [nnabla][INFO]: iter=2659 {Training error}=0.0\n",
            "2023-06-04 15:40:11,273 [nnabla][INFO]: iter=2669 {Training loss}=8.072815035120584e-06\n",
            "2023-06-04 15:40:11,273 [nnabla][INFO]: iter=2669 {Training error}=0.0\n",
            "2023-06-04 15:40:11,307 [nnabla][INFO]: iter=2679 {Training loss}=7.848002496757545e-06\n",
            "2023-06-04 15:40:11,307 [nnabla][INFO]: iter=2679 {Training error}=0.0\n",
            "2023-06-04 15:40:11,344 [nnabla][INFO]: iter=2689 {Training loss}=7.554414423793787e-06\n",
            "2023-06-04 15:40:11,344 [nnabla][INFO]: iter=2689 {Training error}=0.0\n",
            "2023-06-04 15:40:11,380 [nnabla][INFO]: iter=2699 {Training loss}=7.589590495626908e-06\n",
            "2023-06-04 15:40:11,380 [nnabla][INFO]: iter=2699 {Training error}=0.0\n",
            "2023-06-04 15:40:11,380 [nnabla][INFO]: iter=2699 {Training time}=0.3695557117462158[sec/100iter] 14.596801996231079[sec]\n",
            "2023-06-04 15:40:11,401 [nnabla][INFO]: iter=2700 {Test error}=0.01484375\n",
            "2023-06-04 15:40:11,434 [nnabla][INFO]: iter=2709 {Training loss}=7.706285032327287e-06\n",
            "2023-06-04 15:40:11,434 [nnabla][INFO]: iter=2709 {Training error}=0.0\n",
            "2023-06-04 15:40:11,470 [nnabla][INFO]: iter=2719 {Training loss}=7.339010153373238e-06\n",
            "2023-06-04 15:40:11,470 [nnabla][INFO]: iter=2719 {Training error}=0.0\n",
            "2023-06-04 15:40:11,509 [nnabla][INFO]: iter=2729 {Training loss}=7.500401807192247e-06\n",
            "2023-06-04 15:40:11,510 [nnabla][INFO]: iter=2729 {Training error}=0.0\n",
            "2023-06-04 15:40:11,543 [nnabla][INFO]: iter=2739 {Training loss}=7.1694216785544995e-06\n",
            "2023-06-04 15:40:11,543 [nnabla][INFO]: iter=2739 {Training error}=0.0\n",
            "2023-06-04 15:40:11,578 [nnabla][INFO]: iter=2749 {Training loss}=7.270566129591316e-06\n",
            "2023-06-04 15:40:11,578 [nnabla][INFO]: iter=2749 {Training error}=0.0\n",
            "2023-06-04 15:40:11,611 [nnabla][INFO]: iter=2759 {Training loss}=7.363774784607813e-06\n",
            "2023-06-04 15:40:11,611 [nnabla][INFO]: iter=2759 {Training error}=0.0\n",
            "2023-06-04 15:40:11,646 [nnabla][INFO]: iter=2769 {Training loss}=7.158169864851516e-06\n",
            "2023-06-04 15:40:11,646 [nnabla][INFO]: iter=2769 {Training error}=0.0\n",
            "2023-06-04 15:40:11,684 [nnabla][INFO]: iter=2779 {Training loss}=7.188253221102059e-06\n",
            "2023-06-04 15:40:11,684 [nnabla][INFO]: iter=2779 {Training error}=0.0\n",
            "2023-06-04 15:40:11,720 [nnabla][INFO]: iter=2789 {Training loss}=6.867333922855323e-06\n",
            "2023-06-04 15:40:11,721 [nnabla][INFO]: iter=2789 {Training error}=0.0\n",
            "2023-06-04 15:40:11,756 [nnabla][INFO]: iter=2799 {Training loss}=6.876920906506712e-06\n",
            "2023-06-04 15:40:11,756 [nnabla][INFO]: iter=2799 {Training error}=0.0\n",
            "2023-06-04 15:40:11,756 [nnabla][INFO]: iter=2799 {Training time}=0.3759775161743164[sec/100iter] 14.972779512405396[sec]\n",
            "2023-06-04 15:40:11,781 [nnabla][INFO]: iter=2800 {Test error}=0.0140625\n",
            "2023-06-04 15:40:11,821 [nnabla][INFO]: iter=2809 {Training loss}=6.792198746552458e-06\n",
            "2023-06-04 15:40:11,821 [nnabla][INFO]: iter=2809 {Training error}=0.0\n",
            "2023-06-04 15:40:11,855 [nnabla][INFO]: iter=2819 {Training loss}=6.459175892814528e-06\n",
            "2023-06-04 15:40:11,856 [nnabla][INFO]: iter=2819 {Training error}=0.0\n",
            "2023-06-04 15:40:11,890 [nnabla][INFO]: iter=2829 {Training loss}=7.0798546403239015e-06\n",
            "2023-06-04 15:40:11,890 [nnabla][INFO]: iter=2829 {Training error}=0.0\n",
            "2023-06-04 15:40:11,925 [nnabla][INFO]: iter=2839 {Training loss}=6.879540251247818e-06\n",
            "2023-06-04 15:40:11,925 [nnabla][INFO]: iter=2839 {Training error}=0.0\n",
            "2023-06-04 15:40:11,959 [nnabla][INFO]: iter=2849 {Training loss}=6.331133135972777e-06\n",
            "2023-06-04 15:40:11,960 [nnabla][INFO]: iter=2849 {Training error}=0.0\n",
            "2023-06-04 15:40:11,998 [nnabla][INFO]: iter=2859 {Training loss}=6.5611420723143965e-06\n",
            "2023-06-04 15:40:11,998 [nnabla][INFO]: iter=2859 {Training error}=0.0\n",
            "2023-06-04 15:40:12,032 [nnabla][INFO]: iter=2869 {Training loss}=6.460305939981481e-06\n",
            "2023-06-04 15:40:12,032 [nnabla][INFO]: iter=2869 {Training error}=0.0\n",
            "2023-06-04 15:40:12,067 [nnabla][INFO]: iter=2879 {Training loss}=6.630245479755104e-06\n",
            "2023-06-04 15:40:12,067 [nnabla][INFO]: iter=2879 {Training error}=0.0\n",
            "2023-06-04 15:40:12,101 [nnabla][INFO]: iter=2889 {Training loss}=6.138453954918077e-06\n",
            "2023-06-04 15:40:12,102 [nnabla][INFO]: iter=2889 {Training error}=0.0\n",
            "2023-06-04 15:40:12,135 [nnabla][INFO]: iter=2899 {Training loss}=6.434505394281587e-06\n",
            "2023-06-04 15:40:12,135 [nnabla][INFO]: iter=2899 {Training error}=0.0\n",
            "2023-06-04 15:40:12,135 [nnabla][INFO]: iter=2899 {Training time}=0.3790144920349121[sec/100iter] 15.351794004440308[sec]\n",
            "2023-06-04 15:40:12,155 [nnabla][INFO]: iter=2900 {Test error}=0.0140625\n",
            "2023-06-04 15:40:12,193 [nnabla][INFO]: iter=2909 {Training loss}=6.305621354840696e-06\n",
            "2023-06-04 15:40:12,194 [nnabla][INFO]: iter=2909 {Training error}=0.0\n",
            "2023-06-04 15:40:12,233 [nnabla][INFO]: iter=2919 {Training loss}=6.082022537157172e-06\n",
            "2023-06-04 15:40:12,233 [nnabla][INFO]: iter=2919 {Training error}=0.0\n",
            "2023-06-04 15:40:12,267 [nnabla][INFO]: iter=2929 {Training loss}=6.152330570330378e-06\n",
            "2023-06-04 15:40:12,268 [nnabla][INFO]: iter=2929 {Training error}=0.0\n",
            "2023-06-04 15:40:12,301 [nnabla][INFO]: iter=2939 {Training loss}=6.196104550326709e-06\n",
            "2023-06-04 15:40:12,301 [nnabla][INFO]: iter=2939 {Training error}=0.0\n",
            "2023-06-04 15:40:12,335 [nnabla][INFO]: iter=2949 {Training loss}=5.827225322718732e-06\n",
            "2023-06-04 15:40:12,336 [nnabla][INFO]: iter=2949 {Training error}=0.0\n",
            "2023-06-04 15:40:12,376 [nnabla][INFO]: iter=2959 {Training loss}=6.201987162057776e-06\n",
            "2023-06-04 15:40:12,376 [nnabla][INFO]: iter=2959 {Training error}=0.0\n",
            "2023-06-04 15:40:12,411 [nnabla][INFO]: iter=2969 {Training loss}=5.803111889690626e-06\n",
            "2023-06-04 15:40:12,412 [nnabla][INFO]: iter=2969 {Training error}=0.0\n",
            "2023-06-04 15:40:12,445 [nnabla][INFO]: iter=2979 {Training loss}=5.898855306440964e-06\n",
            "2023-06-04 15:40:12,445 [nnabla][INFO]: iter=2979 {Training error}=0.0\n",
            "2023-06-04 15:40:12,478 [nnabla][INFO]: iter=2989 {Training loss}=5.7950128393713385e-06\n",
            "2023-06-04 15:40:12,479 [nnabla][INFO]: iter=2989 {Training error}=0.0\n",
            "2023-06-04 15:40:12,513 [nnabla][INFO]: iter=2999 {Training loss}=5.566015715885442e-06\n",
            "2023-06-04 15:40:12,513 [nnabla][INFO]: iter=2999 {Training error}=0.0\n",
            "2023-06-04 15:40:12,514 [nnabla][INFO]: iter=2999 {Training time}=0.3783411979675293[sec/100iter] 15.730135202407837[sec]\n",
            "2023-06-04 15:40:12,534 [nnabla][INFO]: iter=3000 {Test error}=0.0140625\n",
            "2023-06-04 15:40:12,547 [nnabla][INFO]: Solver state save (.h5): output/states_3000.h5\n",
            "2023-06-04 15:40:12,554 [nnabla][INFO]: Parameter save (.h5): output/params_3000.h5\n",
            "2023-06-04 15:40:12,554 [nnabla][INFO]: Checkpoint save (.json): output/checkpoint_3000.json\n",
            "2023-06-04 15:40:12,588 [nnabla][INFO]: iter=3009 {Training loss}=5.827240784128662e-06\n",
            "2023-06-04 15:40:12,588 [nnabla][INFO]: iter=3009 {Training error}=0.0\n",
            "2023-06-04 15:40:12,621 [nnabla][INFO]: iter=3019 {Training loss}=5.695002073480282e-06\n",
            "2023-06-04 15:40:12,621 [nnabla][INFO]: iter=3019 {Training error}=0.0\n",
            "2023-06-04 15:40:12,654 [nnabla][INFO]: iter=3029 {Training loss}=5.455010523292003e-06\n",
            "2023-06-04 15:40:12,654 [nnabla][INFO]: iter=3029 {Training error}=0.0\n",
            "2023-06-04 15:40:12,695 [nnabla][INFO]: iter=3039 {Training loss}=5.7329943956574425e-06\n",
            "2023-06-04 15:40:12,695 [nnabla][INFO]: iter=3039 {Training error}=0.0\n",
            "2023-06-04 15:40:12,729 [nnabla][INFO]: iter=3049 {Training loss}=5.340011284715729e-06\n",
            "2023-06-04 15:40:12,729 [nnabla][INFO]: iter=3049 {Training error}=0.0\n",
            "2023-06-04 15:40:12,762 [nnabla][INFO]: iter=3059 {Training loss}=5.5949831221369095e-06\n",
            "2023-06-04 15:40:12,762 [nnabla][INFO]: iter=3059 {Training error}=0.0\n",
            "2023-06-04 15:40:12,796 [nnabla][INFO]: iter=3069 {Training loss}=5.331896772986511e-06\n",
            "2023-06-04 15:40:12,796 [nnabla][INFO]: iter=3069 {Training error}=0.0\n",
            "2023-06-04 15:40:12,829 [nnabla][INFO]: iter=3079 {Training loss}=5.457631687022513e-06\n",
            "2023-06-04 15:40:12,830 [nnabla][INFO]: iter=3079 {Training error}=0.0\n",
            "2023-06-04 15:40:12,863 [nnabla][INFO]: iter=3089 {Training loss}=5.235516255197581e-06\n",
            "2023-06-04 15:40:12,863 [nnabla][INFO]: iter=3089 {Training error}=0.0\n",
            "2023-06-04 15:40:12,899 [nnabla][INFO]: iter=3099 {Training loss}=5.4021229516365565e-06\n",
            "2023-06-04 15:40:12,899 [nnabla][INFO]: iter=3099 {Training error}=0.0\n",
            "2023-06-04 15:40:12,899 [nnabla][INFO]: iter=3099 {Training time}=0.38585638999938965[sec/100iter] 16.115991592407227[sec]\n",
            "2023-06-04 15:40:12,920 [nnabla][INFO]: iter=3100 {Test error}=0.01484375\n",
            "2023-06-04 15:40:12,958 [nnabla][INFO]: iter=3109 {Training loss}=5.118828084960114e-06\n",
            "2023-06-04 15:40:12,959 [nnabla][INFO]: iter=3109 {Training error}=0.0\n",
            "2023-06-04 15:40:12,992 [nnabla][INFO]: iter=3119 {Training loss}=5.126566065882798e-06\n",
            "2023-06-04 15:40:12,992 [nnabla][INFO]: iter=3119 {Training error}=0.0\n",
            "2023-06-04 15:40:13,026 [nnabla][INFO]: iter=3129 {Training loss}=5.070591669209534e-06\n",
            "2023-06-04 15:40:13,026 [nnabla][INFO]: iter=3129 {Training error}=0.0\n",
            "2023-06-04 15:40:13,059 [nnabla][INFO]: iter=3139 {Training loss}=5.169311407371424e-06\n",
            "2023-06-04 15:40:13,060 [nnabla][INFO]: iter=3139 {Training error}=0.0\n",
            "2023-06-04 15:40:13,094 [nnabla][INFO]: iter=3149 {Training loss}=5.029337444284465e-06\n",
            "2023-06-04 15:40:13,094 [nnabla][INFO]: iter=3149 {Training error}=0.0\n",
            "2023-06-04 15:40:13,127 [nnabla][INFO]: iter=3159 {Training loss}=4.929413535137428e-06\n",
            "2023-06-04 15:40:13,127 [nnabla][INFO]: iter=3159 {Training error}=0.0\n",
            "2023-06-04 15:40:13,161 [nnabla][INFO]: iter=3169 {Training loss}=5.013598638470285e-06\n",
            "2023-06-04 15:40:13,161 [nnabla][INFO]: iter=3169 {Training error}=0.0\n",
            "2023-06-04 15:40:13,198 [nnabla][INFO]: iter=3179 {Training loss}=4.954367341269972e-06\n",
            "2023-06-04 15:40:13,198 [nnabla][INFO]: iter=3179 {Training error}=0.0\n",
            "2023-06-04 15:40:13,238 [nnabla][INFO]: iter=3189 {Training loss}=4.843001079279929e-06\n",
            "2023-06-04 15:40:13,238 [nnabla][INFO]: iter=3189 {Training error}=0.0\n",
            "2023-06-04 15:40:13,271 [nnabla][INFO]: iter=3199 {Training loss}=4.585685019264929e-06\n",
            "2023-06-04 15:40:13,271 [nnabla][INFO]: iter=3199 {Training error}=0.0\n",
            "2023-06-04 15:40:13,271 [nnabla][INFO]: iter=3199 {Training time}=0.37175655364990234[sec/100iter] 16.48774814605713[sec]\n",
            "2023-06-04 15:40:13,292 [nnabla][INFO]: iter=3200 {Test error}=0.0140625\n",
            "2023-06-04 15:40:13,325 [nnabla][INFO]: iter=3209 {Training loss}=4.953718871547608e-06\n",
            "2023-06-04 15:40:13,325 [nnabla][INFO]: iter=3209 {Training error}=0.0\n",
            "2023-06-04 15:40:13,358 [nnabla][INFO]: iter=3219 {Training loss}=4.8587376113573555e-06\n",
            "2023-06-04 15:40:13,358 [nnabla][INFO]: iter=3219 {Training error}=0.0\n",
            "2023-06-04 15:40:13,399 [nnabla][INFO]: iter=3229 {Training loss}=4.746233571495395e-06\n",
            "2023-06-04 15:40:13,399 [nnabla][INFO]: iter=3229 {Training error}=0.0\n",
            "2023-06-04 15:40:13,432 [nnabla][INFO]: iter=3239 {Training loss}=4.639233793568565e-06\n",
            "2023-06-04 15:40:13,432 [nnabla][INFO]: iter=3239 {Training error}=0.0\n",
            "2023-06-04 15:40:13,465 [nnabla][INFO]: iter=3249 {Training loss}=4.765331595990574e-06\n",
            "2023-06-04 15:40:13,465 [nnabla][INFO]: iter=3249 {Training error}=0.0\n",
            "2023-06-04 15:40:13,499 [nnabla][INFO]: iter=3259 {Training loss}=4.295216513128253e-06\n",
            "2023-06-04 15:40:13,499 [nnabla][INFO]: iter=3259 {Training error}=0.0\n",
            "2023-06-04 15:40:13,533 [nnabla][INFO]: iter=3269 {Training loss}=4.496937890507979e-06\n",
            "2023-06-04 15:40:13,533 [nnabla][INFO]: iter=3269 {Training error}=0.0\n",
            "2023-06-04 15:40:13,568 [nnabla][INFO]: iter=3279 {Training loss}=4.694738436228363e-06\n",
            "2023-06-04 15:40:13,568 [nnabla][INFO]: iter=3279 {Training error}=0.0\n",
            "2023-06-04 15:40:13,603 [nnabla][INFO]: iter=3289 {Training loss}=4.441804321686504e-06\n",
            "2023-06-04 15:40:13,603 [nnabla][INFO]: iter=3289 {Training error}=0.0\n",
            "2023-06-04 15:40:13,636 [nnabla][INFO]: iter=3299 {Training loss}=4.479338258533971e-06\n",
            "2023-06-04 15:40:13,636 [nnabla][INFO]: iter=3299 {Training error}=0.0\n",
            "2023-06-04 15:40:13,636 [nnabla][INFO]: iter=3299 {Training time}=0.36503148078918457[sec/100iter] 16.852779626846313[sec]\n",
            "2023-06-04 15:40:13,657 [nnabla][INFO]: iter=3300 {Test error}=0.0140625\n",
            "2023-06-04 15:40:13,695 [nnabla][INFO]: iter=3309 {Training loss}=4.221184099151287e-06\n",
            "2023-06-04 15:40:13,695 [nnabla][INFO]: iter=3309 {Training error}=0.0\n",
            "2023-06-04 15:40:13,733 [nnabla][INFO]: iter=3319 {Training loss}=4.3119916881551035e-06\n",
            "2023-06-04 15:40:13,734 [nnabla][INFO]: iter=3319 {Training error}=0.0\n",
            "2023-06-04 15:40:13,768 [nnabla][INFO]: iter=3329 {Training loss}=4.5049478103464935e-06\n",
            "2023-06-04 15:40:13,768 [nnabla][INFO]: iter=3329 {Training error}=0.0\n",
            "2023-06-04 15:40:13,802 [nnabla][INFO]: iter=3339 {Training loss}=4.119863206142327e-06\n",
            "2023-06-04 15:40:13,802 [nnabla][INFO]: iter=3339 {Training error}=0.0\n",
            "2023-06-04 15:40:13,835 [nnabla][INFO]: iter=3349 {Training loss}=4.296436600270681e-06\n",
            "2023-06-04 15:40:13,835 [nnabla][INFO]: iter=3349 {Training error}=0.0\n",
            "2023-06-04 15:40:13,868 [nnabla][INFO]: iter=3359 {Training loss}=4.28209796154988e-06\n",
            "2023-06-04 15:40:13,868 [nnabla][INFO]: iter=3359 {Training error}=0.0\n",
            "2023-06-04 15:40:13,901 [nnabla][INFO]: iter=3369 {Training loss}=4.08857249567518e-06\n",
            "2023-06-04 15:40:13,901 [nnabla][INFO]: iter=3369 {Training error}=0.0\n",
            "2023-06-04 15:40:13,936 [nnabla][INFO]: iter=3379 {Training loss}=4.02468958782265e-06\n",
            "2023-06-04 15:40:13,936 [nnabla][INFO]: iter=3379 {Training error}=0.0\n",
            "2023-06-04 15:40:13,969 [nnabla][INFO]: iter=3389 {Training loss}=4.433331923792139e-06\n",
            "2023-06-04 15:40:13,969 [nnabla][INFO]: iter=3389 {Training error}=0.0\n",
            "2023-06-04 15:40:14,006 [nnabla][INFO]: iter=3399 {Training loss}=3.932869276468409e-06\n",
            "2023-06-04 15:40:14,006 [nnabla][INFO]: iter=3399 {Training error}=0.0\n",
            "2023-06-04 15:40:14,006 [nnabla][INFO]: iter=3399 {Training time}=0.36974501609802246[sec/100iter] 17.222524642944336[sec]\n",
            "2023-06-04 15:40:14,026 [nnabla][INFO]: iter=3400 {Test error}=0.01484375\n",
            "2023-06-04 15:40:14,062 [nnabla][INFO]: iter=3409 {Training loss}=3.946836841350887e-06\n",
            "2023-06-04 15:40:14,062 [nnabla][INFO]: iter=3409 {Training error}=0.0\n",
            "2023-06-04 15:40:14,095 [nnabla][INFO]: iter=3419 {Training loss}=4.008112682640785e-06\n",
            "2023-06-04 15:40:14,095 [nnabla][INFO]: iter=3419 {Training error}=0.0\n",
            "2023-06-04 15:40:14,127 [nnabla][INFO]: iter=3429 {Training loss}=3.9554979593958706e-06\n",
            "2023-06-04 15:40:14,128 [nnabla][INFO]: iter=3429 {Training error}=0.0\n",
            "2023-06-04 15:40:14,160 [nnabla][INFO]: iter=3439 {Training loss}=3.930726052203681e-06\n",
            "2023-06-04 15:40:14,160 [nnabla][INFO]: iter=3439 {Training error}=0.0\n",
            "2023-06-04 15:40:14,193 [nnabla][INFO]: iter=3449 {Training loss}=3.949815436499193e-06\n",
            "2023-06-04 15:40:14,193 [nnabla][INFO]: iter=3449 {Training error}=0.0\n",
            "2023-06-04 15:40:14,227 [nnabla][INFO]: iter=3459 {Training loss}=3.9342662603303324e-06\n",
            "2023-06-04 15:40:14,228 [nnabla][INFO]: iter=3459 {Training error}=0.0\n",
            "2023-06-04 15:40:14,262 [nnabla][INFO]: iter=3469 {Training loss}=3.7157849419600097e-06\n",
            "2023-06-04 15:40:14,262 [nnabla][INFO]: iter=3469 {Training error}=0.0\n",
            "2023-06-04 15:40:14,296 [nnabla][INFO]: iter=3479 {Training loss}=3.7951340345898643e-06\n",
            "2023-06-04 15:40:14,296 [nnabla][INFO]: iter=3479 {Training error}=0.0\n",
            "2023-06-04 15:40:14,330 [nnabla][INFO]: iter=3489 {Training loss}=3.727149305632338e-06\n",
            "2023-06-04 15:40:14,330 [nnabla][INFO]: iter=3489 {Training error}=0.0\n",
            "2023-06-04 15:40:14,367 [nnabla][INFO]: iter=3499 {Training loss}=3.7888003134867176e-06\n",
            "2023-06-04 15:40:14,367 [nnabla][INFO]: iter=3499 {Training error}=0.0\n",
            "2023-06-04 15:40:14,367 [nnabla][INFO]: iter=3499 {Training time}=0.3610544204711914[sec/100iter] 17.583579063415527[sec]\n",
            "2023-06-04 15:40:14,394 [nnabla][INFO]: iter=3500 {Test error}=0.0140625\n",
            "2023-06-04 15:40:14,428 [nnabla][INFO]: iter=3509 {Training loss}=3.7335732940846356e-06\n",
            "2023-06-04 15:40:14,428 [nnabla][INFO]: iter=3509 {Training error}=0.0\n",
            "2023-06-04 15:40:14,461 [nnabla][INFO]: iter=3519 {Training loss}=3.651805400295416e-06\n",
            "2023-06-04 15:40:14,461 [nnabla][INFO]: iter=3519 {Training error}=0.0\n",
            "2023-06-04 15:40:14,500 [nnabla][INFO]: iter=3529 {Training loss}=3.489400569378631e-06\n",
            "2023-06-04 15:40:14,500 [nnabla][INFO]: iter=3529 {Training error}=0.0\n",
            "2023-06-04 15:40:14,535 [nnabla][INFO]: iter=3539 {Training loss}=3.85063231078675e-06\n",
            "2023-06-04 15:40:14,535 [nnabla][INFO]: iter=3539 {Training error}=0.0\n",
            "2023-06-04 15:40:14,570 [nnabla][INFO]: iter=3549 {Training loss}=3.473750211924198e-06\n",
            "2023-06-04 15:40:14,571 [nnabla][INFO]: iter=3549 {Training error}=0.0\n",
            "2023-06-04 15:40:14,605 [nnabla][INFO]: iter=3559 {Training loss}=3.6492033359536435e-06\n",
            "2023-06-04 15:40:14,605 [nnabla][INFO]: iter=3559 {Training error}=0.0\n",
            "2023-06-04 15:40:14,643 [nnabla][INFO]: iter=3569 {Training loss}=3.5713462693820475e-06\n",
            "2023-06-04 15:40:14,643 [nnabla][INFO]: iter=3569 {Training error}=0.0\n",
            "2023-06-04 15:40:14,677 [nnabla][INFO]: iter=3579 {Training loss}=3.4392919587844517e-06\n",
            "2023-06-04 15:40:14,678 [nnabla][INFO]: iter=3579 {Training error}=0.0\n",
            "2023-06-04 15:40:14,710 [nnabla][INFO]: iter=3589 {Training loss}=3.4139593481086195e-06\n",
            "2023-06-04 15:40:14,710 [nnabla][INFO]: iter=3589 {Training error}=0.0\n",
            "2023-06-04 15:40:14,743 [nnabla][INFO]: iter=3599 {Training loss}=3.530743470037123e-06\n",
            "2023-06-04 15:40:14,743 [nnabla][INFO]: iter=3599 {Training error}=0.0\n",
            "2023-06-04 15:40:14,743 [nnabla][INFO]: iter=3599 {Training time}=0.37584972381591797[sec/100iter] 17.959428787231445[sec]\n",
            "2023-06-04 15:40:14,763 [nnabla][INFO]: iter=3600 {Test error}=0.0140625\n",
            "2023-06-04 15:40:14,796 [nnabla][INFO]: iter=3609 {Training loss}=3.3173878364323173e-06\n",
            "2023-06-04 15:40:14,796 [nnabla][INFO]: iter=3609 {Training error}=0.0\n",
            "2023-06-04 15:40:14,828 [nnabla][INFO]: iter=3619 {Training loss}=3.4487889024603646e-06\n",
            "2023-06-04 15:40:14,829 [nnabla][INFO]: iter=3619 {Training error}=0.0\n",
            "2023-06-04 15:40:14,861 [nnabla][INFO]: iter=3629 {Training loss}=3.316923539387062e-06\n",
            "2023-06-04 15:40:14,861 [nnabla][INFO]: iter=3629 {Training error}=0.0\n",
            "2023-06-04 15:40:14,897 [nnabla][INFO]: iter=3639 {Training loss}=3.346720404806547e-06\n",
            "2023-06-04 15:40:14,897 [nnabla][INFO]: iter=3639 {Training error}=0.0\n",
            "2023-06-04 15:40:14,930 [nnabla][INFO]: iter=3649 {Training loss}=3.470584033493651e-06\n",
            "2023-06-04 15:40:14,930 [nnabla][INFO]: iter=3649 {Training error}=0.0\n",
            "2023-06-04 15:40:14,963 [nnabla][INFO]: iter=3659 {Training loss}=3.0669634725200012e-06\n",
            "2023-06-04 15:40:14,963 [nnabla][INFO]: iter=3659 {Training error}=0.0\n",
            "2023-06-04 15:40:14,996 [nnabla][INFO]: iter=3669 {Training loss}=3.3630203688517213e-06\n",
            "2023-06-04 15:40:14,997 [nnabla][INFO]: iter=3669 {Training error}=0.0\n",
            "2023-06-04 15:40:15,029 [nnabla][INFO]: iter=3679 {Training loss}=3.094344720011577e-06\n",
            "2023-06-04 15:40:15,029 [nnabla][INFO]: iter=3679 {Training error}=0.0\n",
            "2023-06-04 15:40:15,063 [nnabla][INFO]: iter=3689 {Training loss}=3.346443463669857e-06\n",
            "2023-06-04 15:40:15,063 [nnabla][INFO]: iter=3689 {Training error}=0.0\n",
            "2023-06-04 15:40:15,099 [nnabla][INFO]: iter=3699 {Training loss}=3.136998202535324e-06\n",
            "2023-06-04 15:40:15,099 [nnabla][INFO]: iter=3699 {Training error}=0.0\n",
            "2023-06-04 15:40:15,099 [nnabla][INFO]: iter=3699 {Training time}=0.3563356399536133[sec/100iter] 18.31576442718506[sec]\n",
            "2023-06-04 15:40:15,119 [nnabla][INFO]: iter=3700 {Test error}=0.0140625\n",
            "2023-06-04 15:40:15,151 [nnabla][INFO]: iter=3709 {Training loss}=3.1481729365623323e-06\n",
            "2023-06-04 15:40:15,151 [nnabla][INFO]: iter=3709 {Training error}=0.0\n",
            "2023-06-04 15:40:15,184 [nnabla][INFO]: iter=3719 {Training loss}=3.130107415927341e-06\n",
            "2023-06-04 15:40:15,184 [nnabla][INFO]: iter=3719 {Training error}=0.0\n",
            "2023-06-04 15:40:15,222 [nnabla][INFO]: iter=3729 {Training loss}=3.120700512226904e-06\n",
            "2023-06-04 15:40:15,222 [nnabla][INFO]: iter=3729 {Training error}=0.0\n",
            "2023-06-04 15:40:15,255 [nnabla][INFO]: iter=3739 {Training loss}=3.1294571272155736e-06\n",
            "2023-06-04 15:40:15,255 [nnabla][INFO]: iter=3739 {Training error}=0.0\n",
            "2023-06-04 15:40:15,287 [nnabla][INFO]: iter=3749 {Training loss}=3.223511612304719e-06\n",
            "2023-06-04 15:40:15,287 [nnabla][INFO]: iter=3749 {Training error}=0.0\n",
            "2023-06-04 15:40:15,323 [nnabla][INFO]: iter=3759 {Training loss}=2.7299352041154634e-06\n",
            "2023-06-04 15:40:15,323 [nnabla][INFO]: iter=3759 {Training error}=0.0\n",
            "2023-06-04 15:40:15,356 [nnabla][INFO]: iter=3769 {Training loss}=3.1070107979758177e-06\n",
            "2023-06-04 15:40:15,357 [nnabla][INFO]: iter=3769 {Training error}=0.0\n",
            "2023-06-04 15:40:15,391 [nnabla][INFO]: iter=3779 {Training loss}=2.992740292029339e-06\n",
            "2023-06-04 15:40:15,391 [nnabla][INFO]: iter=3779 {Training error}=0.0\n",
            "2023-06-04 15:40:15,427 [nnabla][INFO]: iter=3789 {Training loss}=2.8905810722790193e-06\n",
            "2023-06-04 15:40:15,427 [nnabla][INFO]: iter=3789 {Training error}=0.0\n",
            "2023-06-04 15:40:15,461 [nnabla][INFO]: iter=3799 {Training loss}=2.972812126245117e-06\n",
            "2023-06-04 15:40:15,461 [nnabla][INFO]: iter=3799 {Training error}=0.0\n",
            "2023-06-04 15:40:15,461 [nnabla][INFO]: iter=3799 {Training time}=0.3616666793823242[sec/100iter] 18.677431106567383[sec]\n",
            "2023-06-04 15:40:15,481 [nnabla][INFO]: iter=3800 {Test error}=0.01484375\n",
            "2023-06-04 15:40:15,518 [nnabla][INFO]: iter=3809 {Training loss}=2.926716433648835e-06\n",
            "2023-06-04 15:40:15,518 [nnabla][INFO]: iter=3809 {Training error}=0.0\n",
            "2023-06-04 15:40:15,553 [nnabla][INFO]: iter=3819 {Training loss}=2.8828487756982213e-06\n",
            "2023-06-04 15:40:15,554 [nnabla][INFO]: iter=3819 {Training error}=0.0\n",
            "2023-06-04 15:40:15,586 [nnabla][INFO]: iter=3829 {Training loss}=2.8592908165592235e-06\n",
            "2023-06-04 15:40:15,586 [nnabla][INFO]: iter=3829 {Training error}=0.0\n",
            "2023-06-04 15:40:15,618 [nnabla][INFO]: iter=3839 {Training loss}=2.875492782550282e-06\n",
            "2023-06-04 15:40:15,619 [nnabla][INFO]: iter=3839 {Training error}=0.0\n",
            "2023-06-04 15:40:15,651 [nnabla][INFO]: iter=3849 {Training loss}=2.903338554460788e-06\n",
            "2023-06-04 15:40:15,651 [nnabla][INFO]: iter=3849 {Training error}=0.0\n",
            "2023-06-04 15:40:15,684 [nnabla][INFO]: iter=3859 {Training loss}=2.702740403037751e-06\n",
            "2023-06-04 15:40:15,684 [nnabla][INFO]: iter=3859 {Training error}=0.0\n",
            "2023-06-04 15:40:15,717 [nnabla][INFO]: iter=3869 {Training loss}=2.797542947519105e-06\n",
            "2023-06-04 15:40:15,717 [nnabla][INFO]: iter=3869 {Training error}=0.0\n",
            "2023-06-04 15:40:15,750 [nnabla][INFO]: iter=3879 {Training loss}=2.7451162623037817e-06\n",
            "2023-06-04 15:40:15,750 [nnabla][INFO]: iter=3879 {Training error}=0.0\n",
            "2023-06-04 15:40:15,783 [nnabla][INFO]: iter=3889 {Training loss}=2.7598262022365816e-06\n",
            "2023-06-04 15:40:15,783 [nnabla][INFO]: iter=3889 {Training error}=0.0\n",
            "2023-06-04 15:40:15,822 [nnabla][INFO]: iter=3899 {Training loss}=2.7383164251659764e-06\n",
            "2023-06-04 15:40:15,822 [nnabla][INFO]: iter=3899 {Training error}=0.0\n",
            "2023-06-04 15:40:15,822 [nnabla][INFO]: iter=3899 {Training time}=0.36133646965026855[sec/100iter] 19.03876757621765[sec]\n",
            "2023-06-04 15:40:15,842 [nnabla][INFO]: iter=3900 {Test error}=0.0140625\n",
            "2023-06-04 15:40:15,875 [nnabla][INFO]: iter=3909 {Training loss}=2.7530300030775834e-06\n",
            "2023-06-04 15:40:15,876 [nnabla][INFO]: iter=3909 {Training error}=0.0\n",
            "2023-06-04 15:40:15,908 [nnabla][INFO]: iter=3919 {Training loss}=2.626467448862968e-06\n",
            "2023-06-04 15:40:15,908 [nnabla][INFO]: iter=3919 {Training error}=0.0\n",
            "2023-06-04 15:40:15,941 [nnabla][INFO]: iter=3929 {Training loss}=2.5666772671684157e-06\n",
            "2023-06-04 15:40:15,941 [nnabla][INFO]: iter=3929 {Training error}=0.0\n",
            "2023-06-04 15:40:15,974 [nnabla][INFO]: iter=3939 {Training loss}=2.770819264696911e-06\n",
            "2023-06-04 15:40:15,974 [nnabla][INFO]: iter=3939 {Training error}=0.0\n",
            "2023-06-04 15:40:16,007 [nnabla][INFO]: iter=3949 {Training loss}=2.6568284283712273e-06\n",
            "2023-06-04 15:40:16,007 [nnabla][INFO]: iter=3949 {Training error}=0.0\n",
            "2023-06-04 15:40:16,040 [nnabla][INFO]: iter=3959 {Training loss}=2.5096815079450607e-06\n",
            "2023-06-04 15:40:16,040 [nnabla][INFO]: iter=3959 {Training error}=0.0\n",
            "2023-06-04 15:40:16,073 [nnabla][INFO]: iter=3969 {Training loss}=2.527749074943131e-06\n",
            "2023-06-04 15:40:16,073 [nnabla][INFO]: iter=3969 {Training error}=0.0\n",
            "2023-06-04 15:40:16,109 [nnabla][INFO]: iter=3979 {Training loss}=2.61380523625121e-06\n",
            "2023-06-04 15:40:16,109 [nnabla][INFO]: iter=3979 {Training error}=0.0\n",
            "2023-06-04 15:40:16,148 [nnabla][INFO]: iter=3989 {Training loss}=2.485842060195864e-06\n",
            "2023-06-04 15:40:16,149 [nnabla][INFO]: iter=3989 {Training error}=0.0\n",
            "2023-06-04 15:40:16,181 [nnabla][INFO]: iter=3999 {Training loss}=2.583535888334154e-06\n",
            "2023-06-04 15:40:16,181 [nnabla][INFO]: iter=3999 {Training error}=0.0\n",
            "2023-06-04 15:40:16,181 [nnabla][INFO]: iter=3999 {Training time}=0.35884737968444824[sec/100iter] 19.3976149559021[sec]\n",
            "2023-06-04 15:40:16,202 [nnabla][INFO]: iter=4000 {Test error}=0.0140625\n",
            "2023-06-04 15:40:16,215 [nnabla][INFO]: Solver state save (.h5): output/states_4000.h5\n",
            "2023-06-04 15:40:16,221 [nnabla][INFO]: Parameter save (.h5): output/params_4000.h5\n",
            "2023-06-04 15:40:16,221 [nnabla][INFO]: Checkpoint save (.json): output/checkpoint_4000.json\n",
            "2023-06-04 15:40:16,254 [nnabla][INFO]: iter=4009 {Training loss}=2.446075995976571e-06\n",
            "2023-06-04 15:40:16,254 [nnabla][INFO]: iter=4009 {Training error}=0.0\n",
            "2023-06-04 15:40:16,287 [nnabla][INFO]: iter=4019 {Training loss}=2.4651687908772146e-06\n",
            "2023-06-04 15:40:16,287 [nnabla][INFO]: iter=4019 {Training error}=0.0\n",
            "2023-06-04 15:40:16,321 [nnabla][INFO]: iter=4029 {Training loss}=2.4036110062297666e-06\n",
            "2023-06-04 15:40:16,321 [nnabla][INFO]: iter=4029 {Training error}=0.0\n",
            "2023-06-04 15:40:16,354 [nnabla][INFO]: iter=4039 {Training loss}=2.624420176289277e-06\n",
            "2023-06-04 15:40:16,354 [nnabla][INFO]: iter=4039 {Training error}=0.0\n",
            "2023-06-04 15:40:16,388 [nnabla][INFO]: iter=4049 {Training loss}=2.38479628933419e-06\n",
            "2023-06-04 15:40:16,388 [nnabla][INFO]: iter=4049 {Training error}=0.0\n",
            "2023-06-04 15:40:16,425 [nnabla][INFO]: iter=4059 {Training loss}=2.394203875155654e-06\n",
            "2023-06-04 15:40:16,425 [nnabla][INFO]: iter=4059 {Training error}=0.0\n",
            "2023-06-04 15:40:16,460 [nnabla][INFO]: iter=4069 {Training loss}=2.317092821613187e-06\n",
            "2023-06-04 15:40:16,460 [nnabla][INFO]: iter=4069 {Training error}=0.0\n",
            "2023-06-04 15:40:16,493 [nnabla][INFO]: iter=4079 {Training loss}=2.375484882577439e-06\n",
            "2023-06-04 15:40:16,493 [nnabla][INFO]: iter=4079 {Training error}=0.0\n",
            "2023-06-04 15:40:16,530 [nnabla][INFO]: iter=4089 {Training loss}=2.362725354032591e-06\n",
            "2023-06-04 15:40:16,530 [nnabla][INFO]: iter=4089 {Training error}=0.0\n",
            "2023-06-04 15:40:16,564 [nnabla][INFO]: iter=4099 {Training loss}=2.4201876840379555e-06\n",
            "2023-06-04 15:40:16,564 [nnabla][INFO]: iter=4099 {Training error}=0.0\n",
            "2023-06-04 15:40:16,564 [nnabla][INFO]: iter=4099 {Training time}=0.3834357261657715[sec/100iter] 19.78105068206787[sec]\n",
            "2023-06-04 15:40:16,584 [nnabla][INFO]: iter=4100 {Test error}=0.01484375\n",
            "2023-06-04 15:40:16,618 [nnabla][INFO]: iter=4109 {Training loss}=2.307872819073964e-06\n",
            "2023-06-04 15:40:16,618 [nnabla][INFO]: iter=4109 {Training error}=0.0\n",
            "2023-06-04 15:40:16,652 [nnabla][INFO]: iter=4119 {Training loss}=2.181776380894007e-06\n",
            "2023-06-04 15:40:16,653 [nnabla][INFO]: iter=4119 {Training error}=0.0\n",
            "2023-06-04 15:40:16,688 [nnabla][INFO]: iter=4129 {Training loss}=2.2991193873167504e-06\n",
            "2023-06-04 15:40:16,688 [nnabla][INFO]: iter=4129 {Training error}=0.0\n",
            "2023-06-04 15:40:16,721 [nnabla][INFO]: iter=4139 {Training loss}=2.2632643776887562e-06\n",
            "2023-06-04 15:40:16,721 [nnabla][INFO]: iter=4139 {Training error}=0.0\n",
            "2023-06-04 15:40:16,754 [nnabla][INFO]: iter=4149 {Training loss}=2.3213774511532392e-06\n",
            "2023-06-04 15:40:16,754 [nnabla][INFO]: iter=4149 {Training error}=0.0\n",
            "2023-06-04 15:40:16,788 [nnabla][INFO]: iter=4159 {Training loss}=2.2252668259170605e-06\n",
            "2023-06-04 15:40:16,789 [nnabla][INFO]: iter=4159 {Training error}=0.0\n",
            "2023-06-04 15:40:16,825 [nnabla][INFO]: iter=4169 {Training loss}=2.2042192995286314e-06\n",
            "2023-06-04 15:40:16,825 [nnabla][INFO]: iter=4169 {Training error}=0.0\n",
            "2023-06-04 15:40:16,858 [nnabla][INFO]: iter=4179 {Training loss}=2.187644440709846e-06\n",
            "2023-06-04 15:40:16,858 [nnabla][INFO]: iter=4179 {Training error}=0.0\n",
            "2023-06-04 15:40:16,891 [nnabla][INFO]: iter=4189 {Training loss}=2.2094350242696237e-06\n",
            "2023-06-04 15:40:16,892 [nnabla][INFO]: iter=4189 {Training error}=0.0\n",
            "2023-06-04 15:40:16,930 [nnabla][INFO]: iter=4199 {Training loss}=2.0963766473869327e-06\n",
            "2023-06-04 15:40:16,930 [nnabla][INFO]: iter=4199 {Training error}=0.0\n",
            "2023-06-04 15:40:16,930 [nnabla][INFO]: iter=4199 {Training time}=0.3654971122741699[sec/100iter] 20.14654779434204[sec]\n",
            "2023-06-04 15:40:16,951 [nnabla][INFO]: iter=4200 {Test error}=0.0140625\n",
            "2023-06-04 15:40:16,985 [nnabla][INFO]: iter=4209 {Training loss}=2.2384938347386196e-06\n",
            "2023-06-04 15:40:16,985 [nnabla][INFO]: iter=4209 {Training error}=0.0\n",
            "2023-06-04 15:40:17,018 [nnabla][INFO]: iter=4219 {Training loss}=2.0925576791341882e-06\n",
            "2023-06-04 15:40:17,018 [nnabla][INFO]: iter=4219 {Training error}=0.0\n",
            "2023-06-04 15:40:17,052 [nnabla][INFO]: iter=4229 {Training loss}=2.1483419914147817e-06\n",
            "2023-06-04 15:40:17,052 [nnabla][INFO]: iter=4229 {Training error}=0.0\n",
            "2023-06-04 15:40:17,084 [nnabla][INFO]: iter=4239 {Training loss}=2.0566105831676396e-06\n",
            "2023-06-04 15:40:17,084 [nnabla][INFO]: iter=4239 {Training error}=0.0\n",
            "2023-06-04 15:40:17,119 [nnabla][INFO]: iter=4249 {Training loss}=2.100008714478463e-06\n",
            "2023-06-04 15:40:17,119 [nnabla][INFO]: iter=4249 {Training error}=0.0\n",
            "2023-06-04 15:40:17,153 [nnabla][INFO]: iter=4259 {Training loss}=2.0878073883068282e-06\n",
            "2023-06-04 15:40:17,153 [nnabla][INFO]: iter=4259 {Training error}=0.0\n",
            "2023-06-04 15:40:17,186 [nnabla][INFO]: iter=4269 {Training loss}=2.0696486444649054e-06\n",
            "2023-06-04 15:40:17,186 [nnabla][INFO]: iter=4269 {Training error}=0.0\n",
            "2023-06-04 15:40:17,219 [nnabla][INFO]: iter=4279 {Training loss}=2.0177756141492864e-06\n",
            "2023-06-04 15:40:17,219 [nnabla][INFO]: iter=4279 {Training error}=0.0\n",
            "2023-06-04 15:40:17,251 [nnabla][INFO]: iter=4289 {Training loss}=2.051487626886228e-06\n",
            "2023-06-04 15:40:17,251 [nnabla][INFO]: iter=4289 {Training error}=0.0\n",
            "2023-06-04 15:40:17,284 [nnabla][INFO]: iter=4299 {Training loss}=1.97558665604447e-06\n",
            "2023-06-04 15:40:17,284 [nnabla][INFO]: iter=4299 {Training error}=0.0\n",
            "2023-06-04 15:40:17,284 [nnabla][INFO]: iter=4299 {Training time}=0.3539557456970215[sec/100iter] 20.500503540039062[sec]\n",
            "2023-06-04 15:40:17,304 [nnabla][INFO]: iter=4300 {Test error}=0.0140625\n",
            "2023-06-04 15:40:17,340 [nnabla][INFO]: iter=4309 {Training loss}=2.094607452818309e-06\n",
            "2023-06-04 15:40:17,341 [nnabla][INFO]: iter=4309 {Training error}=0.0\n",
            "2023-06-04 15:40:17,373 [nnabla][INFO]: iter=4319 {Training loss}=1.8900943814514903e-06\n",
            "2023-06-04 15:40:17,374 [nnabla][INFO]: iter=4319 {Training error}=0.0\n",
            "2023-06-04 15:40:17,406 [nnabla][INFO]: iter=4329 {Training loss}=1.961989710252965e-06\n",
            "2023-06-04 15:40:17,407 [nnabla][INFO]: iter=4329 {Training error}=0.0\n",
            "2023-06-04 15:40:17,446 [nnabla][INFO]: iter=4339 {Training loss}=2.0064130694663618e-06\n",
            "2023-06-04 15:40:17,446 [nnabla][INFO]: iter=4339 {Training error}=0.0\n",
            "2023-06-04 15:40:17,479 [nnabla][INFO]: iter=4349 {Training loss}=1.9235271793149877e-06\n",
            "2023-06-04 15:40:17,479 [nnabla][INFO]: iter=4349 {Training error}=0.0\n",
            "2023-06-04 15:40:17,512 [nnabla][INFO]: iter=4359 {Training loss}=1.9198960217181593e-06\n",
            "2023-06-04 15:40:17,512 [nnabla][INFO]: iter=4359 {Training error}=0.0\n",
            "2023-06-04 15:40:17,545 [nnabla][INFO]: iter=4369 {Training loss}=2.0015709196741227e-06\n",
            "2023-06-04 15:40:17,545 [nnabla][INFO]: iter=4369 {Training error}=0.0\n",
            "2023-06-04 15:40:17,578 [nnabla][INFO]: iter=4379 {Training loss}=1.8653215647645993e-06\n",
            "2023-06-04 15:40:17,578 [nnabla][INFO]: iter=4379 {Training error}=0.0\n",
            "2023-06-04 15:40:17,616 [nnabla][INFO]: iter=4389 {Training loss}=1.8442742657498457e-06\n",
            "2023-06-04 15:40:17,616 [nnabla][INFO]: iter=4389 {Training error}=0.0\n",
            "2023-06-04 15:40:17,651 [nnabla][INFO]: iter=4399 {Training loss}=1.8657868849913939e-06\n",
            "2023-06-04 15:40:17,652 [nnabla][INFO]: iter=4399 {Training error}=0.0\n",
            "2023-06-04 15:40:17,652 [nnabla][INFO]: iter=4399 {Training time}=0.367708683013916[sec/100iter] 20.86821222305298[sec]\n",
            "2023-06-04 15:40:17,673 [nnabla][INFO]: iter=4400 {Test error}=0.0140625\n",
            "2023-06-04 15:40:17,707 [nnabla][INFO]: iter=4409 {Training loss}=1.7890490653371671e-06\n",
            "2023-06-04 15:40:17,707 [nnabla][INFO]: iter=4409 {Training error}=0.0\n",
            "2023-06-04 15:40:17,742 [nnabla][INFO]: iter=4419 {Training loss}=1.882084916360327e-06\n",
            "2023-06-04 15:40:17,742 [nnabla][INFO]: iter=4419 {Training error}=0.0\n",
            "2023-06-04 15:40:17,792 [nnabla][INFO]: iter=4429 {Training loss}=1.8778940784613951e-06\n",
            "2023-06-04 15:40:17,792 [nnabla][INFO]: iter=4429 {Training error}=0.0\n",
            "2023-06-04 15:40:17,834 [nnabla][INFO]: iter=4439 {Training loss}=1.8351478274780675e-06\n",
            "2023-06-04 15:40:17,835 [nnabla][INFO]: iter=4439 {Training error}=0.0\n",
            "2023-06-04 15:40:17,868 [nnabla][INFO]: iter=4449 {Training loss}=1.770608264450857e-06\n",
            "2023-06-04 15:40:17,868 [nnabla][INFO]: iter=4449 {Training error}=0.0\n",
            "2023-06-04 15:40:17,907 [nnabla][INFO]: iter=4459 {Training loss}=1.7318664049525978e-06\n",
            "2023-06-04 15:40:17,907 [nnabla][INFO]: iter=4459 {Training error}=0.0\n",
            "2023-06-04 15:40:17,940 [nnabla][INFO]: iter=4469 {Training loss}=1.8440878193359822e-06\n",
            "2023-06-04 15:40:17,941 [nnabla][INFO]: iter=4469 {Training error}=0.0\n",
            "2023-06-04 15:40:17,974 [nnabla][INFO]: iter=4479 {Training loss}=1.7812257055993541e-06\n",
            "2023-06-04 15:40:17,974 [nnabla][INFO]: iter=4479 {Training error}=0.0\n",
            "2023-06-04 15:40:18,018 [nnabla][INFO]: iter=4489 {Training loss}=1.7156619378511095e-06\n",
            "2023-06-04 15:40:18,019 [nnabla][INFO]: iter=4489 {Training error}=0.0\n",
            "2023-06-04 15:40:18,053 [nnabla][INFO]: iter=4499 {Training loss}=1.7235777249879902e-06\n",
            "2023-06-04 15:40:18,053 [nnabla][INFO]: iter=4499 {Training error}=0.0\n",
            "2023-06-04 15:40:18,053 [nnabla][INFO]: iter=4499 {Training time}=0.40145254135131836[sec/100iter] 21.269664764404297[sec]\n",
            "2023-06-04 15:40:18,074 [nnabla][INFO]: iter=4500 {Test error}=0.01484375\n",
            "2023-06-04 15:40:18,109 [nnabla][INFO]: iter=4509 {Training loss}=1.8186635770689463e-06\n",
            "2023-06-04 15:40:18,110 [nnabla][INFO]: iter=4509 {Training error}=0.0\n",
            "2023-06-04 15:40:18,144 [nnabla][INFO]: iter=4519 {Training loss}=1.6729154594941065e-06\n",
            "2023-06-04 15:40:18,144 [nnabla][INFO]: iter=4519 {Training error}=0.0\n",
            "2023-06-04 15:40:18,177 [nnabla][INFO]: iter=4529 {Training loss}=1.664999217609875e-06\n",
            "2023-06-04 15:40:18,177 [nnabla][INFO]: iter=4529 {Training error}=0.0\n",
            "2023-06-04 15:40:18,210 [nnabla][INFO]: iter=4539 {Training loss}=1.7114707588916644e-06\n",
            "2023-06-04 15:40:18,210 [nnabla][INFO]: iter=4539 {Training error}=0.0\n",
            "2023-06-04 15:40:18,244 [nnabla][INFO]: iter=4549 {Training loss}=1.6161053508767509e-06\n",
            "2023-06-04 15:40:18,244 [nnabla][INFO]: iter=4549 {Training error}=0.0\n",
            "2023-06-04 15:40:18,278 [nnabla][INFO]: iter=4559 {Training loss}=1.7598060821910622e-06\n",
            "2023-06-04 15:40:18,278 [nnabla][INFO]: iter=4559 {Training error}=0.0\n",
            "2023-06-04 15:40:18,311 [nnabla][INFO]: iter=4569 {Training loss}=1.657362417972763e-06\n",
            "2023-06-04 15:40:18,311 [nnabla][INFO]: iter=4569 {Training error}=0.0\n",
            "2023-06-04 15:40:18,347 [nnabla][INFO]: iter=4579 {Training loss}=1.5503555914619938e-06\n",
            "2023-06-04 15:40:18,347 [nnabla][INFO]: iter=4579 {Training error}=0.0\n",
            "2023-06-04 15:40:18,380 [nnabla][INFO]: iter=4589 {Training loss}=1.6331489405274624e-06\n",
            "2023-06-04 15:40:18,380 [nnabla][INFO]: iter=4589 {Training error}=0.0\n",
            "2023-06-04 15:40:18,413 [nnabla][INFO]: iter=4599 {Training loss}=1.652053470024839e-06\n",
            "2023-06-04 15:40:18,413 [nnabla][INFO]: iter=4599 {Training error}=0.0\n",
            "2023-06-04 15:40:18,413 [nnabla][INFO]: iter=4599 {Training time}=0.3598928451538086[sec/100iter] 21.629557609558105[sec]\n",
            "2023-06-04 15:40:18,433 [nnabla][INFO]: iter=4600 {Test error}=0.0140625\n",
            "2023-06-04 15:40:18,477 [nnabla][INFO]: iter=4609 {Training loss}=1.6346391475963173e-06\n",
            "2023-06-04 15:40:18,477 [nnabla][INFO]: iter=4609 {Training error}=0.0\n",
            "2023-06-04 15:40:18,510 [nnabla][INFO]: iter=4619 {Training loss}=1.5444882137671812e-06\n",
            "2023-06-04 15:40:18,511 [nnabla][INFO]: iter=4619 {Training error}=0.0\n",
            "2023-06-04 15:40:18,548 [nnabla][INFO]: iter=4629 {Training loss}=1.6072586959126056e-06\n",
            "2023-06-04 15:40:18,549 [nnabla][INFO]: iter=4629 {Training error}=0.0\n",
            "2023-06-04 15:40:18,582 [nnabla][INFO]: iter=4639 {Training loss}=1.5983183629941777e-06\n",
            "2023-06-04 15:40:18,582 [nnabla][INFO]: iter=4639 {Training error}=0.0\n",
            "2023-06-04 15:40:18,615 [nnabla][INFO]: iter=4649 {Training loss}=1.529773498987197e-06\n",
            "2023-06-04 15:40:18,616 [nnabla][INFO]: iter=4649 {Training error}=0.0\n",
            "2023-06-04 15:40:18,650 [nnabla][INFO]: iter=4659 {Training loss}=1.5472821814910276e-06\n",
            "2023-06-04 15:40:18,650 [nnabla][INFO]: iter=4659 {Training error}=0.0\n",
            "2023-06-04 15:40:18,683 [nnabla][INFO]: iter=4669 {Training loss}=1.5319161548177362e-06\n",
            "2023-06-04 15:40:18,683 [nnabla][INFO]: iter=4669 {Training error}=0.0\n",
            "2023-06-04 15:40:18,716 [nnabla][INFO]: iter=4679 {Training loss}=1.482743073211168e-06\n",
            "2023-06-04 15:40:18,717 [nnabla][INFO]: iter=4679 {Training error}=0.0\n",
            "2023-06-04 15:40:18,752 [nnabla][INFO]: iter=4689 {Training loss}=1.608469801794854e-06\n",
            "2023-06-04 15:40:18,752 [nnabla][INFO]: iter=4689 {Training error}=0.0\n",
            "2023-06-04 15:40:18,786 [nnabla][INFO]: iter=4699 {Training loss}=1.4807867501076544e-06\n",
            "2023-06-04 15:40:18,786 [nnabla][INFO]: iter=4699 {Training error}=0.0\n",
            "2023-06-04 15:40:18,786 [nnabla][INFO]: iter=4699 {Training time}=0.3732185363769531[sec/100iter] 22.00277614593506[sec]\n",
            "2023-06-04 15:40:18,807 [nnabla][INFO]: iter=4700 {Test error}=0.0140625\n",
            "2023-06-04 15:40:18,843 [nnabla][INFO]: iter=4709 {Training loss}=1.4614167866966454e-06\n",
            "2023-06-04 15:40:18,843 [nnabla][INFO]: iter=4709 {Training error}=0.0\n",
            "2023-06-04 15:40:18,877 [nnabla][INFO]: iter=4719 {Training loss}=1.5130101473914692e-06\n",
            "2023-06-04 15:40:18,877 [nnabla][INFO]: iter=4719 {Training error}=0.0\n",
            "2023-06-04 15:40:18,911 [nnabla][INFO]: iter=4729 {Training loss}=1.4266785228755907e-06\n",
            "2023-06-04 15:40:18,911 [nnabla][INFO]: iter=4729 {Training error}=0.0\n",
            "2023-06-04 15:40:18,944 [nnabla][INFO]: iter=4739 {Training loss}=1.4982030052124173e-06\n",
            "2023-06-04 15:40:18,945 [nnabla][INFO]: iter=4739 {Training error}=0.0\n",
            "2023-06-04 15:40:18,979 [nnabla][INFO]: iter=4749 {Training loss}=1.4242566521716071e-06\n",
            "2023-06-04 15:40:18,979 [nnabla][INFO]: iter=4749 {Training error}=0.0\n",
            "2023-06-04 15:40:19,018 [nnabla][INFO]: iter=4759 {Training loss}=1.5538025763817132e-06\n",
            "2023-06-04 15:40:19,018 [nnabla][INFO]: iter=4759 {Training error}=0.0\n",
            "2023-06-04 15:40:19,053 [nnabla][INFO]: iter=4769 {Training loss}=1.3784368775304756e-06\n",
            "2023-06-04 15:40:19,053 [nnabla][INFO]: iter=4769 {Training error}=0.0\n",
            "2023-06-04 15:40:19,086 [nnabla][INFO]: iter=4779 {Training loss}=1.422209038537403e-06\n",
            "2023-06-04 15:40:19,086 [nnabla][INFO]: iter=4779 {Training error}=0.0\n",
            "2023-06-04 15:40:19,119 [nnabla][INFO]: iter=4789 {Training loss}=1.4076796333029051e-06\n",
            "2023-06-04 15:40:19,120 [nnabla][INFO]: iter=4789 {Training error}=0.0\n",
            "2023-06-04 15:40:19,155 [nnabla][INFO]: iter=4799 {Training loss}=1.376481350234826e-06\n",
            "2023-06-04 15:40:19,155 [nnabla][INFO]: iter=4799 {Training error}=0.0\n",
            "2023-06-04 15:40:19,156 [nnabla][INFO]: iter=4799 {Training time}=0.3693580627441406[sec/100iter] 22.3721342086792[sec]\n",
            "2023-06-04 15:40:19,176 [nnabla][INFO]: iter=4800 {Test error}=0.01484375\n",
            "2023-06-04 15:40:19,209 [nnabla][INFO]: iter=4809 {Training loss}=1.4293793810793431e-06\n",
            "2023-06-04 15:40:19,209 [nnabla][INFO]: iter=4809 {Training error}=0.0\n",
            "2023-06-04 15:40:19,242 [nnabla][INFO]: iter=4819 {Training loss}=1.3794616506856983e-06\n",
            "2023-06-04 15:40:19,243 [nnabla][INFO]: iter=4819 {Training error}=0.0\n",
            "2023-06-04 15:40:19,275 [nnabla][INFO]: iter=4829 {Training loss}=1.3394158031587722e-06\n",
            "2023-06-04 15:40:19,275 [nnabla][INFO]: iter=4829 {Training error}=0.0\n",
            "2023-06-04 15:40:19,309 [nnabla][INFO]: iter=4839 {Training loss}=1.4086112969380338e-06\n",
            "2023-06-04 15:40:19,309 [nnabla][INFO]: iter=4839 {Training error}=0.0\n",
            "2023-06-04 15:40:19,342 [nnabla][INFO]: iter=4849 {Training loss}=1.3395089126788662e-06\n",
            "2023-06-04 15:40:19,342 [nnabla][INFO]: iter=4849 {Training error}=0.0\n",
            "2023-06-04 15:40:19,375 [nnabla][INFO]: iter=4859 {Training loss}=1.3437922916637035e-06\n",
            "2023-06-04 15:40:19,376 [nnabla][INFO]: iter=4859 {Training error}=0.0\n",
            "2023-06-04 15:40:19,410 [nnabla][INFO]: iter=4869 {Training loss}=1.284747895624605e-06\n",
            "2023-06-04 15:40:19,410 [nnabla][INFO]: iter=4869 {Training error}=0.0\n",
            "2023-06-04 15:40:19,450 [nnabla][INFO]: iter=4879 {Training loss}=1.3487286878444138e-06\n",
            "2023-06-04 15:40:19,450 [nnabla][INFO]: iter=4879 {Training error}=0.0\n",
            "2023-06-04 15:40:19,486 [nnabla][INFO]: iter=4889 {Training loss}=1.3536637197830714e-06\n",
            "2023-06-04 15:40:19,486 [nnabla][INFO]: iter=4889 {Training error}=0.0\n",
            "2023-06-04 15:40:19,521 [nnabla][INFO]: iter=4899 {Training loss}=1.279812181564921e-06\n",
            "2023-06-04 15:40:19,522 [nnabla][INFO]: iter=4899 {Training error}=0.0\n",
            "2023-06-04 15:40:19,522 [nnabla][INFO]: iter=4899 {Training time}=0.3660862445831299[sec/100iter] 22.73822045326233[sec]\n",
            "2023-06-04 15:40:19,542 [nnabla][INFO]: iter=4900 {Test error}=0.0140625\n",
            "2023-06-04 15:40:19,583 [nnabla][INFO]: iter=4909 {Training loss}=1.3491949175659101e-06\n",
            "2023-06-04 15:40:19,583 [nnabla][INFO]: iter=4909 {Training error}=0.0\n",
            "2023-06-04 15:40:19,623 [nnabla][INFO]: iter=4919 {Training loss}=1.2637926829484059e-06\n",
            "2023-06-04 15:40:19,624 [nnabla][INFO]: iter=4919 {Training error}=0.0\n",
            "2023-06-04 15:40:19,662 [nnabla][INFO]: iter=4929 {Training loss}=1.3018841400480596e-06\n",
            "2023-06-04 15:40:19,662 [nnabla][INFO]: iter=4929 {Training error}=0.0\n",
            "2023-06-04 15:40:19,700 [nnabla][INFO]: iter=4939 {Training loss}=1.2227230854477966e-06\n",
            "2023-06-04 15:40:19,701 [nnabla][INFO]: iter=4939 {Training error}=0.0\n",
            "2023-06-04 15:40:19,752 [nnabla][INFO]: iter=4949 {Training loss}=1.2806499398720916e-06\n",
            "2023-06-04 15:40:19,753 [nnabla][INFO]: iter=4949 {Training error}=0.0\n",
            "2023-06-04 15:40:19,796 [nnabla][INFO]: iter=4959 {Training loss}=1.2305456493777456e-06\n",
            "2023-06-04 15:40:19,797 [nnabla][INFO]: iter=4959 {Training error}=0.0\n",
            "2023-06-04 15:40:19,836 [nnabla][INFO]: iter=4969 {Training loss}=1.229987219630857e-06\n",
            "2023-06-04 15:40:19,836 [nnabla][INFO]: iter=4969 {Training error}=0.0\n",
            "2023-06-04 15:40:19,876 [nnabla][INFO]: iter=4979 {Training loss}=1.2288693369555403e-06\n",
            "2023-06-04 15:40:19,876 [nnabla][INFO]: iter=4979 {Training error}=0.0\n",
            "2023-06-04 15:40:19,925 [nnabla][INFO]: iter=4989 {Training loss}=1.2630483752218424e-06\n",
            "2023-06-04 15:40:19,925 [nnabla][INFO]: iter=4989 {Training error}=0.0\n",
            "2023-06-04 15:40:19,969 [nnabla][INFO]: iter=4999 {Training loss}=1.2452603641577298e-06\n",
            "2023-06-04 15:40:19,970 [nnabla][INFO]: iter=4999 {Training error}=0.0\n",
            "2023-06-04 15:40:19,970 [nnabla][INFO]: iter=4999 {Training time}=0.44817304611206055[sec/100iter] 23.18639349937439[sec]\n",
            "2023-06-04 15:40:19,997 [nnabla][INFO]: iter=5000 {Test error}=0.0140625\n",
            "2023-06-04 15:40:20,016 [nnabla][INFO]: Solver state save (.h5): output/states_5000.h5\n",
            "2023-06-04 15:40:20,028 [nnabla][INFO]: Parameter save (.h5): output/params_5000.h5\n",
            "2023-06-04 15:40:20,032 [nnabla][INFO]: Checkpoint save (.json): output/checkpoint_5000.json\n",
            "2023-06-04 15:40:20,086 [nnabla][INFO]: iter=5009 {Training loss}=1.1684276159940055e-06\n",
            "2023-06-04 15:40:20,087 [nnabla][INFO]: iter=5009 {Training error}=0.0\n",
            "2023-06-04 15:40:20,122 [nnabla][INFO]: iter=5019 {Training loss}=1.1827696653199382e-06\n",
            "2023-06-04 15:40:20,122 [nnabla][INFO]: iter=5019 {Training error}=0.0\n",
            "2023-06-04 15:40:20,161 [nnabla][INFO]: iter=5029 {Training loss}=1.2447017070371658e-06\n",
            "2023-06-04 15:40:20,162 [nnabla][INFO]: iter=5029 {Training error}=0.0\n",
            "2023-06-04 15:40:20,200 [nnabla][INFO]: iter=5039 {Training loss}=1.1701970379363047e-06\n",
            "2023-06-04 15:40:20,201 [nnabla][INFO]: iter=5039 {Training error}=0.0\n",
            "2023-06-04 15:40:20,239 [nnabla][INFO]: iter=5049 {Training loss}=1.1763436305045616e-06\n",
            "2023-06-04 15:40:20,240 [nnabla][INFO]: iter=5049 {Training error}=0.0\n",
            "2023-06-04 15:40:20,277 [nnabla][INFO]: iter=5059 {Training loss}=1.180720914817357e-06\n",
            "2023-06-04 15:40:20,277 [nnabla][INFO]: iter=5059 {Training error}=0.0\n",
            "2023-06-04 15:40:20,314 [nnabla][INFO]: iter=5069 {Training loss}=1.119813305194839e-06\n",
            "2023-06-04 15:40:20,314 [nnabla][INFO]: iter=5069 {Training error}=0.0\n",
            "2023-06-04 15:40:20,369 [nnabla][INFO]: iter=5079 {Training loss}=1.1897545846295543e-06\n",
            "2023-06-04 15:40:20,369 [nnabla][INFO]: iter=5079 {Training error}=0.0\n",
            "2023-06-04 15:40:20,407 [nnabla][INFO]: iter=5089 {Training loss}=1.0942954986603581e-06\n",
            "2023-06-04 15:40:20,407 [nnabla][INFO]: iter=5089 {Training error}=0.0\n",
            "2023-06-04 15:40:20,449 [nnabla][INFO]: iter=5099 {Training loss}=1.1934798749280162e-06\n",
            "2023-06-04 15:40:20,449 [nnabla][INFO]: iter=5099 {Training error}=0.0\n",
            "2023-06-04 15:40:20,449 [nnabla][INFO]: iter=5099 {Training time}=0.47946953773498535[sec/100iter] 23.665863037109375[sec]\n",
            "2023-06-04 15:40:20,473 [nnabla][INFO]: iter=5100 {Test error}=0.0140625\n",
            "2023-06-04 15:40:20,523 [nnabla][INFO]: iter=5109 {Training loss}=1.116088014896377e-06\n",
            "2023-06-04 15:40:20,523 [nnabla][INFO]: iter=5109 {Training error}=0.0\n",
            "2023-06-04 15:40:20,560 [nnabla][INFO]: iter=5119 {Training loss}=1.1769959655794082e-06\n",
            "2023-06-04 15:40:20,560 [nnabla][INFO]: iter=5119 {Training error}=0.0\n",
            "2023-06-04 15:40:20,599 [nnabla][INFO]: iter=5129 {Training loss}=1.0779042440844933e-06\n",
            "2023-06-04 15:40:20,599 [nnabla][INFO]: iter=5129 {Training error}=0.0\n",
            "2023-06-04 15:40:20,644 [nnabla][INFO]: iter=5139 {Training loss}=1.128940198213968e-06\n",
            "2023-06-04 15:40:20,645 [nnabla][INFO]: iter=5139 {Training error}=0.0\n",
            "2023-06-04 15:40:20,685 [nnabla][INFO]: iter=5149 {Training loss}=1.081815867109981e-06\n",
            "2023-06-04 15:40:20,685 [nnabla][INFO]: iter=5149 {Training error}=0.0\n",
            "2023-06-04 15:40:20,723 [nnabla][INFO]: iter=5159 {Training loss}=1.0510823358345078e-06\n",
            "2023-06-04 15:40:20,723 [nnabla][INFO]: iter=5159 {Training error}=0.0\n",
            "2023-06-04 15:40:20,766 [nnabla][INFO]: iter=5169 {Training loss}=1.13443536520208e-06\n",
            "2023-06-04 15:40:20,766 [nnabla][INFO]: iter=5169 {Training error}=0.0\n",
            "2023-06-04 15:40:20,802 [nnabla][INFO]: iter=5179 {Training loss}=1.1171125606779242e-06\n",
            "2023-06-04 15:40:20,803 [nnabla][INFO]: iter=5179 {Training error}=0.0\n",
            "2023-06-04 15:40:20,841 [nnabla][INFO]: iter=5189 {Training loss}=1.0329224551242078e-06\n",
            "2023-06-04 15:40:20,841 [nnabla][INFO]: iter=5189 {Training error}=0.0\n",
            "2023-06-04 15:40:20,877 [nnabla][INFO]: iter=5199 {Training loss}=1.0874038025576738e-06\n",
            "2023-06-04 15:40:20,878 [nnabla][INFO]: iter=5199 {Training error}=0.0\n",
            "2023-06-04 15:40:20,878 [nnabla][INFO]: iter=5199 {Training time}=0.4284396171569824[sec/100iter] 24.094302654266357[sec]\n",
            "2023-06-04 15:40:20,902 [nnabla][INFO]: iter=5200 {Test error}=0.01484375\n",
            "2023-06-04 15:40:20,946 [nnabla][INFO]: iter=5209 {Training loss}=1.0373923942097463e-06\n",
            "2023-06-04 15:40:20,946 [nnabla][INFO]: iter=5209 {Training error}=0.0\n",
            "2023-06-04 15:40:20,986 [nnabla][INFO]: iter=5219 {Training loss}=1.0167178743358818e-06\n",
            "2023-06-04 15:40:20,986 [nnabla][INFO]: iter=5219 {Training error}=0.0\n",
            "2023-06-04 15:40:21,023 [nnabla][INFO]: iter=5229 {Training loss}=1.0747380656539463e-06\n",
            "2023-06-04 15:40:21,023 [nnabla][INFO]: iter=5229 {Training error}=0.0\n",
            "2023-06-04 15:40:21,062 [nnabla][INFO]: iter=5239 {Training loss}=1.0303141380063607e-06\n",
            "2023-06-04 15:40:21,062 [nnabla][INFO]: iter=5239 {Training error}=0.0\n",
            "2023-06-04 15:40:21,101 [nnabla][INFO]: iter=5249 {Training loss}=1.0570432777967653e-06\n",
            "2023-06-04 15:40:21,101 [nnabla][INFO]: iter=5249 {Training error}=0.0\n",
            "2023-06-04 15:40:21,138 [nnabla][INFO]: iter=5259 {Training loss}=9.925965969159734e-07\n",
            "2023-06-04 15:40:21,140 [nnabla][INFO]: iter=5259 {Training error}=0.0\n",
            "2023-06-04 15:40:21,180 [nnabla][INFO]: iter=5269 {Training loss}=1.0156929874938214e-06\n",
            "2023-06-04 15:40:21,180 [nnabla][INFO]: iter=5269 {Training error}=0.0\n",
            "2023-06-04 15:40:21,221 [nnabla][INFO]: iter=5279 {Training loss}=1.0181147445109673e-06\n",
            "2023-06-04 15:40:21,221 [nnabla][INFO]: iter=5279 {Training error}=0.0\n",
            "2023-06-04 15:40:21,265 [nnabla][INFO]: iter=5289 {Training loss}=1.0192319450652576e-06\n",
            "2023-06-04 15:40:21,265 [nnabla][INFO]: iter=5289 {Training error}=0.0\n",
            "2023-06-04 15:40:21,303 [nnabla][INFO]: iter=5299 {Training loss}=9.705244110591593e-07\n",
            "2023-06-04 15:40:21,304 [nnabla][INFO]: iter=5299 {Training error}=0.0\n",
            "2023-06-04 15:40:21,304 [nnabla][INFO]: iter=5299 {Training time}=0.42601466178894043[sec/100iter] 24.520317316055298[sec]\n",
            "2023-06-04 15:40:21,328 [nnabla][INFO]: iter=5300 {Test error}=0.0140625\n",
            "2023-06-04 15:40:21,372 [nnabla][INFO]: iter=5309 {Training loss}=9.695932021713816e-07\n",
            "2023-06-04 15:40:21,373 [nnabla][INFO]: iter=5309 {Training error}=0.0\n",
            "2023-06-04 15:40:21,411 [nnabla][INFO]: iter=5319 {Training loss}=9.898025155052892e-07\n",
            "2023-06-04 15:40:21,412 [nnabla][INFO]: iter=5319 {Training error}=0.0\n",
            "2023-06-04 15:40:21,453 [nnabla][INFO]: iter=5329 {Training loss}=9.611184168534237e-07\n",
            "2023-06-04 15:40:21,453 [nnabla][INFO]: iter=5329 {Training error}=0.0\n",
            "2023-06-04 15:40:21,493 [nnabla][INFO]: iter=5339 {Training loss}=9.653090273786802e-07\n",
            "2023-06-04 15:40:21,493 [nnabla][INFO]: iter=5339 {Training error}=0.0\n",
            "2023-06-04 15:40:21,532 [nnabla][INFO]: iter=5349 {Training loss}=9.700590908323647e-07\n",
            "2023-06-04 15:40:21,532 [nnabla][INFO]: iter=5349 {Training error}=0.0\n",
            "2023-06-04 15:40:21,575 [nnabla][INFO]: iter=5359 {Training loss}=9.535747267364059e-07\n",
            "2023-06-04 15:40:21,575 [nnabla][INFO]: iter=5359 {Training error}=0.0\n",
            "2023-06-04 15:40:21,614 [nnabla][INFO]: iter=5369 {Training loss}=9.323407539341133e-07\n",
            "2023-06-04 15:40:21,615 [nnabla][INFO]: iter=5369 {Training error}=0.0\n",
            "2023-06-04 15:40:21,657 [nnabla][INFO]: iter=5379 {Training loss}=9.361591537526692e-07\n",
            "2023-06-04 15:40:21,658 [nnabla][INFO]: iter=5379 {Training error}=0.0\n",
            "2023-06-04 15:40:21,696 [nnabla][INFO]: iter=5389 {Training loss}=9.423059168511827e-07\n",
            "2023-06-04 15:40:21,696 [nnabla][INFO]: iter=5389 {Training error}=0.0\n",
            "2023-06-04 15:40:21,735 [nnabla][INFO]: iter=5399 {Training loss}=9.452859330849606e-07\n",
            "2023-06-04 15:40:21,736 [nnabla][INFO]: iter=5399 {Training error}=0.0\n",
            "2023-06-04 15:40:21,736 [nnabla][INFO]: iter=5399 {Training time}=0.43213891983032227[sec/100iter] 24.95245623588562[sec]\n",
            "2023-06-04 15:40:21,765 [nnabla][INFO]: iter=5400 {Test error}=0.0140625\n",
            "2023-06-04 15:40:21,808 [nnabla][INFO]: iter=5409 {Training loss}=9.350416689812846e-07\n",
            "2023-06-04 15:40:21,809 [nnabla][INFO]: iter=5409 {Training error}=0.0\n",
            "2023-06-04 15:40:21,851 [nnabla][INFO]: iter=5419 {Training loss}=9.067297241927008e-07\n",
            "2023-06-04 15:40:21,851 [nnabla][INFO]: iter=5419 {Training error}=0.0\n",
            "2023-06-04 15:40:21,892 [nnabla][INFO]: iter=5429 {Training loss}=8.93411936431221e-07\n",
            "2023-06-04 15:40:21,892 [nnabla][INFO]: iter=5429 {Training error}=0.0\n",
            "2023-06-04 15:40:21,933 [nnabla][INFO]: iter=5439 {Training loss}=9.167879397864453e-07\n",
            "2023-06-04 15:40:21,933 [nnabla][INFO]: iter=5439 {Training error}=0.0\n",
            "2023-06-04 15:40:21,977 [nnabla][INFO]: iter=5449 {Training loss}=9.151117410510778e-07\n",
            "2023-06-04 15:40:21,977 [nnabla][INFO]: iter=5449 {Training error}=0.0\n",
            "2023-06-04 15:40:22,018 [nnabla][INFO]: iter=5459 {Training loss}=8.605366019764915e-07\n",
            "2023-06-04 15:40:22,018 [nnabla][INFO]: iter=5459 {Training error}=0.0\n",
            "2023-06-04 15:40:22,065 [nnabla][INFO]: iter=5469 {Training loss}=9.179055382446677e-07\n",
            "2023-06-04 15:40:22,065 [nnabla][INFO]: iter=5469 {Training error}=0.0\n",
            "2023-06-04 15:40:22,105 [nnabla][INFO]: iter=5479 {Training loss}=8.624923566458165e-07\n",
            "2023-06-04 15:40:22,105 [nnabla][INFO]: iter=5479 {Training error}=0.0\n",
            "2023-06-04 15:40:22,144 [nnabla][INFO]: iter=5489 {Training loss}=9.064506230060942e-07\n",
            "2023-06-04 15:40:22,144 [nnabla][INFO]: iter=5489 {Training error}=0.0\n",
            "2023-06-04 15:40:22,184 [nnabla][INFO]: iter=5499 {Training loss}=8.83540110407921e-07\n",
            "2023-06-04 15:40:22,184 [nnabla][INFO]: iter=5499 {Training error}=0.0\n",
            "2023-06-04 15:40:22,184 [nnabla][INFO]: iter=5499 {Training time}=0.4485812187194824[sec/100iter] 25.401037454605103[sec]\n",
            "2023-06-04 15:40:22,210 [nnabla][INFO]: iter=5500 {Test error}=0.01484375\n",
            "2023-06-04 15:40:22,251 [nnabla][INFO]: iter=5509 {Training loss}=8.353912335223868e-07\n",
            "2023-06-04 15:40:22,251 [nnabla][INFO]: iter=5509 {Training error}=0.0\n",
            "2023-06-04 15:40:22,293 [nnabla][INFO]: iter=5519 {Training loss}=8.422828727816523e-07\n",
            "2023-06-04 15:40:22,293 [nnabla][INFO]: iter=5519 {Training error}=0.0\n",
            "2023-06-04 15:40:22,335 [nnabla][INFO]: iter=5529 {Training loss}=8.519688208252774e-07\n",
            "2023-06-04 15:40:22,335 [nnabla][INFO]: iter=5529 {Training error}=0.0\n",
            "2023-06-04 15:40:22,377 [nnabla][INFO]: iter=5539 {Training loss}=8.56718429531611e-07\n",
            "2023-06-04 15:40:22,377 [nnabla][INFO]: iter=5539 {Training error}=0.0\n",
            "2023-06-04 15:40:22,418 [nnabla][INFO]: iter=5549 {Training loss}=8.163924576365389e-07\n",
            "2023-06-04 15:40:22,419 [nnabla][INFO]: iter=5549 {Training error}=0.0\n",
            "2023-06-04 15:40:22,461 [nnabla][INFO]: iter=5559 {Training loss}=8.486160254506103e-07\n",
            "2023-06-04 15:40:22,461 [nnabla][INFO]: iter=5559 {Training error}=0.0\n",
            "2023-06-04 15:40:22,500 [nnabla][INFO]: iter=5569 {Training loss}=8.244951459346339e-07\n",
            "2023-06-04 15:40:22,500 [nnabla][INFO]: iter=5569 {Training error}=0.0\n",
            "2023-06-04 15:40:22,541 [nnabla][INFO]: iter=5579 {Training loss}=8.087555443125893e-07\n",
            "2023-06-04 15:40:22,541 [nnabla][INFO]: iter=5579 {Training error}=0.0\n",
            "2023-06-04 15:40:22,590 [nnabla][INFO]: iter=5589 {Training loss}=8.472193258057814e-07\n",
            "2023-06-04 15:40:22,590 [nnabla][INFO]: iter=5589 {Training error}=0.0\n",
            "2023-06-04 15:40:22,636 [nnabla][INFO]: iter=5599 {Training loss}=7.986044465724262e-07\n",
            "2023-06-04 15:40:22,636 [nnabla][INFO]: iter=5599 {Training error}=0.0\n",
            "2023-06-04 15:40:22,637 [nnabla][INFO]: iter=5599 {Training time}=0.452070951461792[sec/100iter] 25.853108406066895[sec]\n",
            "2023-06-04 15:40:22,662 [nnabla][INFO]: iter=5600 {Test error}=0.0140625\n",
            "2023-06-04 15:40:22,702 [nnabla][INFO]: iter=5609 {Training loss}=8.352982376891305e-07\n",
            "2023-06-04 15:40:22,702 [nnabla][INFO]: iter=5609 {Training error}=0.0\n",
            "2023-06-04 15:40:22,744 [nnabla][INFO]: iter=5619 {Training loss}=7.930167953418277e-07\n",
            "2023-06-04 15:40:22,745 [nnabla][INFO]: iter=5619 {Training error}=0.0\n",
            "2023-06-04 15:40:22,790 [nnabla][INFO]: iter=5629 {Training loss}=8.038199439397431e-07\n",
            "2023-06-04 15:40:22,790 [nnabla][INFO]: iter=5629 {Training error}=0.0\n",
            "2023-06-04 15:40:22,831 [nnabla][INFO]: iter=5639 {Training loss}=7.925509635242634e-07\n",
            "2023-06-04 15:40:22,831 [nnabla][INFO]: iter=5639 {Training error}=0.0\n",
            "2023-06-04 15:40:22,872 [nnabla][INFO]: iter=5649 {Training loss}=7.549260203632002e-07\n",
            "2023-06-04 15:40:22,873 [nnabla][INFO]: iter=5649 {Training error}=0.0\n",
            "2023-06-04 15:40:22,918 [nnabla][INFO]: iter=5659 {Training loss}=8.225391638916335e-07\n",
            "2023-06-04 15:40:22,919 [nnabla][INFO]: iter=5659 {Training error}=0.0\n",
            "2023-06-04 15:40:22,961 [nnabla][INFO]: iter=5669 {Training loss}=7.71875988903048e-07\n",
            "2023-06-04 15:40:22,962 [nnabla][INFO]: iter=5669 {Training error}=0.0\n",
            "2023-06-04 15:40:23,002 [nnabla][INFO]: iter=5679 {Training loss}=7.898501053205109e-07\n",
            "2023-06-04 15:40:23,003 [nnabla][INFO]: iter=5679 {Training error}=0.0\n",
            "2023-06-04 15:40:23,043 [nnabla][INFO]: iter=5689 {Training loss}=7.633078666913207e-07\n",
            "2023-06-04 15:40:23,043 [nnabla][INFO]: iter=5689 {Training error}=0.0\n",
            "2023-06-04 15:40:23,086 [nnabla][INFO]: iter=5699 {Training loss}=7.55484791170602e-07\n",
            "2023-06-04 15:40:23,086 [nnabla][INFO]: iter=5699 {Training error}=0.0\n",
            "2023-06-04 15:40:23,086 [nnabla][INFO]: iter=5699 {Training time}=0.4498322010040283[sec/100iter] 26.302940607070923[sec]\n",
            "2023-06-04 15:40:23,111 [nnabla][INFO]: iter=5700 {Test error}=0.0140625\n",
            "2023-06-04 15:40:23,153 [nnabla][INFO]: iter=5709 {Training loss}=7.563229473817046e-07\n",
            "2023-06-04 15:40:23,153 [nnabla][INFO]: iter=5709 {Training error}=0.0\n",
            "2023-06-04 15:40:23,194 [nnabla][INFO]: iter=5719 {Training loss}=7.670331001463637e-07\n",
            "2023-06-04 15:40:23,195 [nnabla][INFO]: iter=5719 {Training error}=0.0\n",
            "2023-06-04 15:40:23,238 [nnabla][INFO]: iter=5729 {Training loss}=7.210260832835047e-07\n",
            "2023-06-04 15:40:23,238 [nnabla][INFO]: iter=5729 {Training error}=0.0\n",
            "2023-06-04 15:40:23,280 [nnabla][INFO]: iter=5739 {Training loss}=7.455197419403703e-07\n",
            "2023-06-04 15:40:23,280 [nnabla][INFO]: iter=5739 {Training error}=0.0\n",
            "2023-06-04 15:40:23,323 [nnabla][INFO]: iter=5749 {Training loss}=7.585582011415681e-07\n",
            "2023-06-04 15:40:23,323 [nnabla][INFO]: iter=5749 {Training error}=0.0\n",
            "2023-06-04 15:40:23,370 [nnabla][INFO]: iter=5759 {Training loss}=7.156244237194187e-07\n",
            "2023-06-04 15:40:23,370 [nnabla][INFO]: iter=5759 {Training error}=0.0\n",
            "2023-06-04 15:40:23,411 [nnabla][INFO]: iter=5769 {Training loss}=7.463580118383106e-07\n",
            "2023-06-04 15:40:23,411 [nnabla][INFO]: iter=5769 {Training error}=0.0\n",
            "2023-06-04 15:40:23,451 [nnabla][INFO]: iter=5779 {Training loss}=7.262416374942404e-07\n",
            "2023-06-04 15:40:23,451 [nnabla][INFO]: iter=5779 {Training error}=0.0\n",
            "2023-06-04 15:40:23,502 [nnabla][INFO]: iter=5789 {Training loss}=7.060320399432385e-07\n",
            "2023-06-04 15:40:23,502 [nnabla][INFO]: iter=5789 {Training error}=0.0\n",
            "2023-06-04 15:40:23,544 [nnabla][INFO]: iter=5799 {Training loss}=7.222370186354965e-07\n",
            "2023-06-04 15:40:23,544 [nnabla][INFO]: iter=5799 {Training error}=0.0\n",
            "2023-06-04 15:40:23,544 [nnabla][INFO]: iter=5799 {Training time}=0.45779943466186523[sec/100iter] 26.760740041732788[sec]\n",
            "2023-06-04 15:40:23,571 [nnabla][INFO]: iter=5800 {Test error}=0.0140625\n",
            "2023-06-04 15:40:23,615 [nnabla][INFO]: iter=5809 {Training loss}=6.923418141013826e-07\n",
            "2023-06-04 15:40:23,615 [nnabla][INFO]: iter=5809 {Training error}=0.0\n",
            "2023-06-04 15:40:23,657 [nnabla][INFO]: iter=5819 {Training loss}=7.092916121109738e-07\n",
            "2023-06-04 15:40:23,657 [nnabla][INFO]: iter=5819 {Training error}=0.0\n",
            "2023-06-04 15:40:23,705 [nnabla][INFO]: iter=5829 {Training loss}=6.97743303135212e-07\n",
            "2023-06-04 15:40:23,705 [nnabla][INFO]: iter=5829 {Training error}=0.0\n",
            "2023-06-04 15:40:23,752 [nnabla][INFO]: iter=5839 {Training loss}=6.856362233520485e-07\n",
            "2023-06-04 15:40:23,752 [nnabla][INFO]: iter=5839 {Training error}=0.0\n",
            "2023-06-04 15:40:23,795 [nnabla][INFO]: iter=5849 {Training loss}=7.111542572602048e-07\n",
            "2023-06-04 15:40:23,795 [nnabla][INFO]: iter=5849 {Training error}=0.0\n",
            "2023-06-04 15:40:23,841 [nnabla][INFO]: iter=5859 {Training loss}=6.63657374389004e-07\n",
            "2023-06-04 15:40:23,841 [nnabla][INFO]: iter=5859 {Training error}=0.0\n",
            "2023-06-04 15:40:23,892 [nnabla][INFO]: iter=5869 {Training loss}=7.196293267952569e-07\n",
            "2023-06-04 15:40:23,892 [nnabla][INFO]: iter=5869 {Training error}=0.0\n",
            "2023-06-04 15:40:23,930 [nnabla][INFO]: iter=5879 {Training loss}=6.428889491871814e-07\n",
            "2023-06-04 15:40:23,930 [nnabla][INFO]: iter=5879 {Training error}=0.0\n",
            "2023-06-04 15:40:23,967 [nnabla][INFO]: iter=5889 {Training loss}=6.977433599786309e-07\n",
            "2023-06-04 15:40:23,967 [nnabla][INFO]: iter=5889 {Training error}=0.0\n",
            "2023-06-04 15:40:24,009 [nnabla][INFO]: iter=5899 {Training loss}=6.709214517286455e-07\n",
            "2023-06-04 15:40:24,010 [nnabla][INFO]: iter=5899 {Training error}=0.0\n",
            "2023-06-04 15:40:24,010 [nnabla][INFO]: iter=5899 {Training time}=0.46560001373291016[sec/100iter] 27.2263400554657[sec]\n",
            "2023-06-04 15:40:24,032 [nnabla][INFO]: iter=5900 {Test error}=0.01484375\n",
            "2023-06-04 15:40:24,072 [nnabla][INFO]: iter=5909 {Training loss}=6.521089517264045e-07\n",
            "2023-06-04 15:40:24,072 [nnabla][INFO]: iter=5909 {Training error}=0.0\n",
            "2023-06-04 15:40:24,110 [nnabla][INFO]: iter=5919 {Training loss}=6.901997267050319e-07\n",
            "2023-06-04 15:40:24,110 [nnabla][INFO]: iter=5919 {Training error}=0.0\n",
            "2023-06-04 15:40:24,147 [nnabla][INFO]: iter=5929 {Training loss}=6.597456945200975e-07\n",
            "2023-06-04 15:40:24,147 [nnabla][INFO]: iter=5929 {Training error}=0.0\n",
            "2023-06-04 15:40:24,187 [nnabla][INFO]: iter=5939 {Training loss}=6.251941613300005e-07\n",
            "2023-06-04 15:40:24,187 [nnabla][INFO]: iter=5939 {Training error}=0.0\n",
            "2023-06-04 15:40:24,223 [nnabla][INFO]: iter=5949 {Training loss}=6.875919780213735e-07\n",
            "2023-06-04 15:40:24,224 [nnabla][INFO]: iter=5949 {Training error}=0.0\n",
            "2023-06-04 15:40:24,260 [nnabla][INFO]: iter=5959 {Training loss}=6.210963192643248e-07\n",
            "2023-06-04 15:40:24,260 [nnabla][INFO]: iter=5959 {Training error}=0.0\n",
            "2023-06-04 15:40:24,303 [nnabla][INFO]: iter=5969 {Training loss}=6.430752250707883e-07\n",
            "2023-06-04 15:40:24,303 [nnabla][INFO]: iter=5969 {Training error}=0.0\n",
            "2023-06-04 15:40:24,339 [nnabla][INFO]: iter=5979 {Training loss}=6.320857437458471e-07\n",
            "2023-06-04 15:40:24,339 [nnabla][INFO]: iter=5979 {Training error}=0.0\n",
            "2023-06-04 15:40:24,375 [nnabla][INFO]: iter=5989 {Training loss}=6.507120247079001e-07\n",
            "2023-06-04 15:40:24,375 [nnabla][INFO]: iter=5989 {Training error}=0.0\n",
            "2023-06-04 15:40:24,412 [nnabla][INFO]: iter=5999 {Training loss}=5.995828473714937e-07\n",
            "2023-06-04 15:40:24,412 [nnabla][INFO]: iter=5999 {Training error}=0.0\n",
            "2023-06-04 15:40:24,412 [nnabla][INFO]: iter=5999 {Training time}=0.40224146842956543[sec/100iter] 27.628581523895264[sec]\n",
            "2023-06-04 15:40:24,434 [nnabla][INFO]: iter=6000 {Test error}=0.0140625\n",
            "2023-06-04 15:40:24,448 [nnabla][INFO]: Solver state save (.h5): output/states_6000.h5\n",
            "2023-06-04 15:40:24,454 [nnabla][INFO]: Parameter save (.h5): output/params_6000.h5\n",
            "2023-06-04 15:40:24,455 [nnabla][INFO]: Checkpoint save (.json): output/checkpoint_6000.json\n",
            "2023-06-04 15:40:24,494 [nnabla][INFO]: iter=6009 {Training loss}=6.425164542633865e-07\n",
            "2023-06-04 15:40:24,494 [nnabla][INFO]: iter=6009 {Training error}=0.0\n",
            "2023-06-04 15:40:24,528 [nnabla][INFO]: iter=6019 {Training loss}=6.075921987758193e-07\n",
            "2023-06-04 15:40:24,529 [nnabla][INFO]: iter=6019 {Training error}=0.0\n",
            "2023-06-04 15:40:24,565 [nnabla][INFO]: iter=6029 {Training loss}=6.250078286029748e-07\n",
            "2023-06-04 15:40:24,565 [nnabla][INFO]: iter=6029 {Training error}=0.0\n",
            "2023-06-04 15:40:24,602 [nnabla][INFO]: iter=6039 {Training loss}=6.068472657716484e-07\n",
            "2023-06-04 15:40:24,603 [nnabla][INFO]: iter=6039 {Training error}=0.0\n",
            "2023-06-04 15:40:24,643 [nnabla][INFO]: iter=6049 {Training loss}=6.172778057589312e-07\n",
            "2023-06-04 15:40:24,643 [nnabla][INFO]: iter=6049 {Training error}=0.0\n",
            "2023-06-04 15:40:24,682 [nnabla][INFO]: iter=6059 {Training loss}=5.805840714856458e-07\n",
            "2023-06-04 15:40:24,682 [nnabla][INFO]: iter=6059 {Training error}=0.0\n",
            "2023-06-04 15:40:24,724 [nnabla][INFO]: iter=6069 {Training loss}=6.154151606097003e-07\n",
            "2023-06-04 15:40:24,724 [nnabla][INFO]: iter=6069 {Training error}=0.0\n",
            "2023-06-04 15:40:24,759 [nnabla][INFO]: iter=6079 {Training loss}=5.947400723016472e-07\n",
            "2023-06-04 15:40:24,760 [nnabla][INFO]: iter=6079 {Training error}=0.0\n",
            "2023-06-04 15:40:24,795 [nnabla][INFO]: iter=6089 {Training loss}=5.965095510873653e-07\n",
            "2023-06-04 15:40:24,795 [nnabla][INFO]: iter=6089 {Training error}=0.0\n",
            "2023-06-04 15:40:24,831 [nnabla][INFO]: iter=6099 {Training loss}=5.913872200835613e-07\n",
            "2023-06-04 15:40:24,831 [nnabla][INFO]: iter=6099 {Training error}=0.0\n",
            "2023-06-04 15:40:24,831 [nnabla][INFO]: iter=6099 {Training time}=0.4191739559173584[sec/100iter] 28.047755479812622[sec]\n",
            "2023-06-04 15:40:24,852 [nnabla][INFO]: iter=6100 {Test error}=0.0140625\n",
            "2023-06-04 15:40:24,890 [nnabla][INFO]: iter=6109 {Training loss}=5.70991517179209e-07\n",
            "2023-06-04 15:40:24,890 [nnabla][INFO]: iter=6109 {Training error}=0.0\n",
            "2023-06-04 15:40:24,926 [nnabla][INFO]: iter=6119 {Training loss}=5.912010578867921e-07\n",
            "2023-06-04 15:40:24,926 [nnabla][INFO]: iter=6119 {Training error}=0.0\n",
            "2023-06-04 15:40:24,961 [nnabla][INFO]: iter=6129 {Training loss}=5.696876428373798e-07\n",
            "2023-06-04 15:40:24,962 [nnabla][INFO]: iter=6129 {Training error}=0.0\n",
            "2023-06-04 15:40:24,999 [nnabla][INFO]: iter=6139 {Training loss}=5.850544084751164e-07\n",
            "2023-06-04 15:40:24,999 [nnabla][INFO]: iter=6139 {Training error}=0.0\n",
            "2023-06-04 15:40:25,035 [nnabla][INFO]: iter=6149 {Training loss}=5.38022902674129e-07\n",
            "2023-06-04 15:40:25,035 [nnabla][INFO]: iter=6149 {Training error}=0.0\n",
            "2023-06-04 15:40:25,071 [nnabla][INFO]: iter=6159 {Training loss}=5.944607437413651e-07\n",
            "2023-06-04 15:40:25,072 [nnabla][INFO]: iter=6159 {Training error}=0.0\n",
            "2023-06-04 15:40:25,106 [nnabla][INFO]: iter=6169 {Training loss}=5.61771514639986e-07\n",
            "2023-06-04 15:40:25,106 [nnabla][INFO]: iter=6169 {Training error}=0.0\n",
            "2023-06-04 15:40:25,143 [nnabla][INFO]: iter=6179 {Training loss}=5.639134315060801e-07\n",
            "2023-06-04 15:40:25,143 [nnabla][INFO]: iter=6179 {Training error}=0.0\n",
            "2023-06-04 15:40:25,179 [nnabla][INFO]: iter=6189 {Training loss}=5.384886776482745e-07\n",
            "2023-06-04 15:40:25,179 [nnabla][INFO]: iter=6189 {Training error}=0.0\n",
            "2023-06-04 15:40:25,218 [nnabla][INFO]: iter=6199 {Training loss}=5.502232056642242e-07\n",
            "2023-06-04 15:40:25,218 [nnabla][INFO]: iter=6199 {Training error}=0.0\n",
            "2023-06-04 15:40:25,219 [nnabla][INFO]: iter=6199 {Training time}=0.38744425773620605[sec/100iter] 28.435199737548828[sec]\n",
            "2023-06-04 15:40:25,240 [nnabla][INFO]: iter=6200 {Test error}=0.01484375\n",
            "2023-06-04 15:40:25,275 [nnabla][INFO]: iter=6209 {Training loss}=5.655899713019608e-07\n",
            "2023-06-04 15:40:25,275 [nnabla][INFO]: iter=6209 {Training error}=0.0\n",
            "2023-06-04 15:40:25,310 [nnabla][INFO]: iter=6219 {Training loss}=5.559041937885922e-07\n",
            "2023-06-04 15:40:25,310 [nnabla][INFO]: iter=6219 {Training error}=0.0\n",
            "2023-06-04 15:40:25,346 [nnabla][INFO]: iter=6229 {Training loss}=5.285236852614617e-07\n",
            "2023-06-04 15:40:25,346 [nnabla][INFO]: iter=6229 {Training error}=0.0\n",
            "2023-06-04 15:40:25,381 [nnabla][INFO]: iter=6239 {Training loss}=5.438902235255227e-07\n",
            "2023-06-04 15:40:25,383 [nnabla][INFO]: iter=6239 {Training error}=0.0\n",
            "2023-06-04 15:40:25,420 [nnabla][INFO]: iter=6249 {Training loss}=5.404443754741806e-07\n",
            "2023-06-04 15:40:25,420 [nnabla][INFO]: iter=6249 {Training error}=0.0\n",
            "2023-06-04 15:40:25,456 [nnabla][INFO]: iter=6259 {Training loss}=5.201416684030846e-07\n",
            "2023-06-04 15:40:25,457 [nnabla][INFO]: iter=6259 {Training error}=0.0\n",
            "2023-06-04 15:40:25,493 [nnabla][INFO]: iter=6269 {Training loss}=5.435177854451467e-07\n",
            "2023-06-04 15:40:25,493 [nnabla][INFO]: iter=6269 {Training error}=0.0\n",
            "2023-06-04 15:40:25,528 [nnabla][INFO]: iter=6279 {Training loss}=5.052405640526558e-07\n",
            "2023-06-04 15:40:25,528 [nnabla][INFO]: iter=6279 {Training error}=0.0\n",
            "2023-06-04 15:40:25,562 [nnabla][INFO]: iter=6289 {Training loss}=5.497576012203353e-07\n",
            "2023-06-04 15:40:25,562 [nnabla][INFO]: iter=6289 {Training error}=0.0\n",
            "2023-06-04 15:40:25,597 [nnabla][INFO]: iter=6299 {Training loss}=5.046819069320918e-07\n",
            "2023-06-04 15:40:25,597 [nnabla][INFO]: iter=6299 {Training error}=0.0\n",
            "2023-06-04 15:40:25,597 [nnabla][INFO]: iter=6299 {Training time}=0.3785409927368164[sec/100iter] 28.813740730285645[sec]\n",
            "2023-06-04 15:40:25,618 [nnabla][INFO]: iter=6300 {Test error}=0.0140625\n",
            "2023-06-04 15:40:25,655 [nnabla][INFO]: iter=6309 {Training loss}=5.089659680379555e-07\n",
            "2023-06-04 15:40:25,655 [nnabla][INFO]: iter=6309 {Training error}=0.0\n",
            "2023-06-04 15:40:25,692 [nnabla][INFO]: iter=6319 {Training loss}=5.295480036693334e-07\n",
            "2023-06-04 15:40:25,692 [nnabla][INFO]: iter=6319 {Training error}=0.0\n",
            "2023-06-04 15:40:25,726 [nnabla][INFO]: iter=6329 {Training loss}=5.040299129177583e-07\n",
            "2023-06-04 15:40:25,726 [nnabla][INFO]: iter=6329 {Training error}=0.0\n",
            "2023-06-04 15:40:25,761 [nnabla][INFO]: iter=6339 {Training loss}=4.849380275118165e-07\n",
            "2023-06-04 15:40:25,762 [nnabla][INFO]: iter=6339 {Training error}=0.0\n",
            "2023-06-04 15:40:25,796 [nnabla][INFO]: iter=6349 {Training loss}=5.230288593338628e-07\n",
            "2023-06-04 15:40:25,797 [nnabla][INFO]: iter=6349 {Training error}=0.0\n",
            "2023-06-04 15:40:25,837 [nnabla][INFO]: iter=6359 {Training loss}=4.983488679499715e-07\n",
            "2023-06-04 15:40:25,837 [nnabla][INFO]: iter=6359 {Training error}=0.0\n",
            "2023-06-04 15:40:25,872 [nnabla][INFO]: iter=6369 {Training loss}=5.078483695797331e-07\n",
            "2023-06-04 15:40:25,872 [nnabla][INFO]: iter=6369 {Training error}=0.0\n",
            "2023-06-04 15:40:25,907 [nnabla][INFO]: iter=6379 {Training loss}=4.80747019082628e-07\n",
            "2023-06-04 15:40:25,907 [nnabla][INFO]: iter=6379 {Training error}=0.0\n",
            "2023-06-04 15:40:25,942 [nnabla][INFO]: iter=6389 {Training loss}=4.912709528070991e-07\n",
            "2023-06-04 15:40:25,942 [nnabla][INFO]: iter=6389 {Training error}=0.0\n",
            "2023-06-04 15:40:25,976 [nnabla][INFO]: iter=6399 {Training loss}=4.84099814457295e-07\n",
            "2023-06-04 15:40:25,977 [nnabla][INFO]: iter=6399 {Training error}=0.0\n",
            "2023-06-04 15:40:25,977 [nnabla][INFO]: iter=6399 {Training time}=0.3796079158782959[sec/100iter] 29.19334864616394[sec]\n",
            "2023-06-04 15:40:26,002 [nnabla][INFO]: iter=6400 {Test error}=0.0140625\n",
            "2023-06-04 15:40:26,038 [nnabla][INFO]: iter=6409 {Training loss}=4.893150844509364e-07\n",
            "2023-06-04 15:40:26,038 [nnabla][INFO]: iter=6409 {Training error}=0.0\n",
            "2023-06-04 15:40:26,072 [nnabla][INFO]: iter=6419 {Training loss}=4.825166115551838e-07\n",
            "2023-06-04 15:40:26,072 [nnabla][INFO]: iter=6419 {Training error}=0.0\n",
            "2023-06-04 15:40:26,108 [nnabla][INFO]: iter=6429 {Training loss}=4.7655612434027717e-07\n",
            "2023-06-04 15:40:26,108 [nnabla][INFO]: iter=6429 {Training error}=0.0\n",
            "2023-06-04 15:40:26,142 [nnabla][INFO]: iter=6439 {Training loss}=4.675223408412421e-07\n",
            "2023-06-04 15:40:26,143 [nnabla][INFO]: iter=6439 {Training error}=0.0\n",
            "2023-06-04 15:40:26,177 [nnabla][INFO]: iter=6449 {Training loss}=4.7199267783071264e-07\n",
            "2023-06-04 15:40:26,177 [nnabla][INFO]: iter=6449 {Training error}=0.0\n",
            "2023-06-04 15:40:26,211 [nnabla][INFO]: iter=6459 {Training loss}=4.5932679881843796e-07\n",
            "2023-06-04 15:40:26,211 [nnabla][INFO]: iter=6459 {Training error}=0.0\n",
            "2023-06-04 15:40:26,245 [nnabla][INFO]: iter=6469 {Training loss}=4.881975428361329e-07\n",
            "2023-06-04 15:40:26,245 [nnabla][INFO]: iter=6469 {Training error}=0.0\n",
            "2023-06-04 15:40:26,279 [nnabla][INFO]: iter=6479 {Training loss}=4.509449240686081e-07\n",
            "2023-06-04 15:40:26,279 [nnabla][INFO]: iter=6479 {Training error}=0.0\n",
            "2023-06-04 15:40:26,314 [nnabla][INFO]: iter=6489 {Training loss}=4.571847114220873e-07\n",
            "2023-06-04 15:40:26,315 [nnabla][INFO]: iter=6489 {Training error}=0.0\n",
            "2023-06-04 15:40:26,349 [nnabla][INFO]: iter=6499 {Training loss}=4.7450731699427706e-07\n",
            "2023-06-04 15:40:26,349 [nnabla][INFO]: iter=6499 {Training error}=0.0\n",
            "2023-06-04 15:40:26,349 [nnabla][INFO]: iter=6499 {Training time}=0.37209010124206543[sec/100iter] 29.565438747406006[sec]\n",
            "2023-06-04 15:40:26,369 [nnabla][INFO]: iter=6500 {Test error}=0.0140625\n",
            "2023-06-04 15:40:26,404 [nnabla][INFO]: iter=6509 {Training loss}=4.515968612395227e-07\n",
            "2023-06-04 15:40:26,404 [nnabla][INFO]: iter=6509 {Training error}=0.0\n",
            "2023-06-04 15:40:26,441 [nnabla][INFO]: iter=6519 {Training loss}=4.6407643594648107e-07\n",
            "2023-06-04 15:40:26,441 [nnabla][INFO]: iter=6519 {Training error}=0.0\n",
            "2023-06-04 15:40:26,475 [nnabla][INFO]: iter=6529 {Training loss}=4.456364024463255e-07\n",
            "2023-06-04 15:40:26,475 [nnabla][INFO]: iter=6529 {Training error}=0.0\n",
            "2023-06-04 15:40:26,509 [nnabla][INFO]: iter=6539 {Training loss}=4.5643974999620696e-07\n",
            "2023-06-04 15:40:26,509 [nnabla][INFO]: iter=6539 {Training error}=0.0\n",
            "2023-06-04 15:40:26,542 [nnabla][INFO]: iter=6549 {Training loss}=4.407003757478378e-07\n",
            "2023-06-04 15:40:26,543 [nnabla][INFO]: iter=6549 {Training error}=0.0\n",
            "2023-06-04 15:40:26,576 [nnabla][INFO]: iter=6559 {Training loss}=4.403279092457524e-07\n",
            "2023-06-04 15:40:26,576 [nnabla][INFO]: iter=6559 {Training error}=0.0\n",
            "2023-06-04 15:40:26,612 [nnabla][INFO]: iter=6569 {Training loss}=4.38372126154718e-07\n",
            "2023-06-04 15:40:26,613 [nnabla][INFO]: iter=6569 {Training error}=0.0\n",
            "2023-06-04 15:40:26,646 [nnabla][INFO]: iter=6579 {Training loss}=4.302696652302984e-07\n",
            "2023-06-04 15:40:26,646 [nnabla][INFO]: iter=6579 {Training error}=0.0\n",
            "2023-06-04 15:40:26,684 [nnabla][INFO]: iter=6589 {Training loss}=4.339018175869569e-07\n",
            "2023-06-04 15:40:26,685 [nnabla][INFO]: iter=6589 {Training error}=0.0\n",
            "2023-06-04 15:40:26,720 [nnabla][INFO]: iter=6599 {Training loss}=4.411660938785644e-07\n",
            "2023-06-04 15:40:26,721 [nnabla][INFO]: iter=6599 {Training error}=0.0\n",
            "2023-06-04 15:40:26,721 [nnabla][INFO]: iter=6599 {Training time}=0.37197399139404297[sec/100iter] 29.93741273880005[sec]\n",
            "2023-06-04 15:40:26,743 [nnabla][INFO]: iter=6600 {Test error}=0.01484375\n",
            "2023-06-04 15:40:26,776 [nnabla][INFO]: iter=6609 {Training loss}=4.045652985951165e-07\n",
            "2023-06-04 15:40:26,776 [nnabla][INFO]: iter=6609 {Training error}=0.0\n",
            "2023-06-04 15:40:26,809 [nnabla][INFO]: iter=6619 {Training loss}=4.0689366187507403e-07\n",
            "2023-06-04 15:40:26,809 [nnabla][INFO]: iter=6619 {Training error}=0.0\n",
            "2023-06-04 15:40:26,842 [nnabla][INFO]: iter=6629 {Training loss}=4.377201889838034e-07\n",
            "2023-06-04 15:40:26,843 [nnabla][INFO]: iter=6629 {Training error}=0.0\n",
            "2023-06-04 15:40:26,881 [nnabla][INFO]: iter=6639 {Training loss}=4.2896584773188806e-07\n",
            "2023-06-04 15:40:26,882 [nnabla][INFO]: iter=6639 {Training error}=0.0\n",
            "2023-06-04 15:40:26,918 [nnabla][INFO]: iter=6649 {Training loss}=4.013057548490906e-07\n",
            "2023-06-04 15:40:26,918 [nnabla][INFO]: iter=6649 {Training error}=0.0\n",
            "2023-06-04 15:40:26,952 [nnabla][INFO]: iter=6659 {Training loss}=4.2272603195669944e-07\n",
            "2023-06-04 15:40:26,952 [nnabla][INFO]: iter=6659 {Training error}=0.0\n",
            "2023-06-04 15:40:26,985 [nnabla][INFO]: iter=6669 {Training loss}=4.0391336142420187e-07\n",
            "2023-06-04 15:40:26,985 [nnabla][INFO]: iter=6669 {Training error}=0.0\n",
            "2023-06-04 15:40:27,018 [nnabla][INFO]: iter=6679 {Training loss}=4.155549504503142e-07\n",
            "2023-06-04 15:40:27,019 [nnabla][INFO]: iter=6679 {Training error}=0.0\n",
            "2023-06-04 15:40:27,053 [nnabla][INFO]: iter=6689 {Training loss}=4.0391341826762073e-07\n",
            "2023-06-04 15:40:27,053 [nnabla][INFO]: iter=6689 {Training error}=0.0\n",
            "2023-06-04 15:40:27,088 [nnabla][INFO]: iter=6699 {Training loss}=3.8416948200392653e-07\n",
            "2023-06-04 15:40:27,088 [nnabla][INFO]: iter=6699 {Training error}=0.0\n",
            "2023-06-04 15:40:27,088 [nnabla][INFO]: iter=6699 {Training time}=0.367480993270874[sec/100iter] 30.304893732070923[sec]\n",
            "2023-06-04 15:40:27,109 [nnabla][INFO]: iter=6700 {Test error}=0.0140625\n",
            "2023-06-04 15:40:27,142 [nnabla][INFO]: iter=6709 {Training loss}=4.1173643694492057e-07\n",
            "2023-06-04 15:40:27,142 [nnabla][INFO]: iter=6709 {Training error}=0.0\n",
            "2023-06-04 15:40:27,175 [nnabla][INFO]: iter=6719 {Training loss}=3.960903995903209e-07\n",
            "2023-06-04 15:40:27,176 [nnabla][INFO]: iter=6719 {Training error}=0.0\n",
            "2023-06-04 15:40:27,219 [nnabla][INFO]: iter=6729 {Training loss}=4.018644972347829e-07\n",
            "2023-06-04 15:40:27,219 [nnabla][INFO]: iter=6729 {Training error}=0.0\n",
            "2023-06-04 15:40:27,252 [nnabla][INFO]: iter=6739 {Training loss}=3.82306865276405e-07\n",
            "2023-06-04 15:40:27,252 [nnabla][INFO]: iter=6739 {Training error}=0.0\n",
            "2023-06-04 15:40:27,285 [nnabla][INFO]: iter=6749 {Training loss}=3.7979225453455e-07\n",
            "2023-06-04 15:40:27,285 [nnabla][INFO]: iter=6749 {Training error}=0.0\n",
            "2023-06-04 15:40:27,318 [nnabla][INFO]: iter=6759 {Training loss}=3.8761535847697814e-07\n",
            "2023-06-04 15:40:27,319 [nnabla][INFO]: iter=6759 {Training error}=0.0\n",
            "2023-06-04 15:40:27,352 [nnabla][INFO]: iter=6769 {Training loss}=3.9515902017228655e-07\n",
            "2023-06-04 15:40:27,352 [nnabla][INFO]: iter=6769 {Training error}=0.0\n",
            "2023-06-04 15:40:27,392 [nnabla][INFO]: iter=6779 {Training loss}=3.7681212461393443e-07\n",
            "2023-06-04 15:40:27,392 [nnabla][INFO]: iter=6779 {Training error}=0.0\n",
            "2023-06-04 15:40:27,426 [nnabla][INFO]: iter=6789 {Training loss}=3.800716683599603e-07\n",
            "2023-06-04 15:40:27,427 [nnabla][INFO]: iter=6789 {Training error}=0.0\n",
            "2023-06-04 15:40:27,460 [nnabla][INFO]: iter=6799 {Training loss}=3.8938486568440567e-07\n",
            "2023-06-04 15:40:27,460 [nnabla][INFO]: iter=6799 {Training error}=0.0\n",
            "2023-06-04 15:40:27,461 [nnabla][INFO]: iter=6799 {Training time}=0.37228918075561523[sec/100iter] 30.677182912826538[sec]\n",
            "2023-06-04 15:40:27,481 [nnabla][INFO]: iter=6800 {Test error}=0.0140625\n",
            "2023-06-04 15:40:27,519 [nnabla][INFO]: iter=6809 {Training loss}=3.837970439235505e-07\n",
            "2023-06-04 15:40:27,519 [nnabla][INFO]: iter=6809 {Training error}=0.0\n",
            "2023-06-04 15:40:27,552 [nnabla][INFO]: iter=6819 {Training loss}=3.6861644048258313e-07\n",
            "2023-06-04 15:40:27,553 [nnabla][INFO]: iter=6819 {Training error}=0.0\n",
            "2023-06-04 15:40:27,586 [nnabla][INFO]: iter=6829 {Training loss}=3.686165257477114e-07\n",
            "2023-06-04 15:40:27,586 [nnabla][INFO]: iter=6829 {Training error}=0.0\n",
            "2023-06-04 15:40:27,620 [nnabla][INFO]: iter=6839 {Training loss}=3.7373871464296826e-07\n",
            "2023-06-04 15:40:27,620 [nnabla][INFO]: iter=6839 {Training error}=0.0\n",
            "2023-06-04 15:40:27,653 [nnabla][INFO]: iter=6849 {Training loss}=3.594895758851635e-07\n",
            "2023-06-04 15:40:27,653 [nnabla][INFO]: iter=6849 {Training error}=0.0\n",
            "2023-06-04 15:40:27,695 [nnabla][INFO]: iter=6859 {Training loss}=3.7718467638114817e-07\n",
            "2023-06-04 15:40:27,695 [nnabla][INFO]: iter=6859 {Training error}=0.0\n",
            "2023-06-04 15:40:27,732 [nnabla][INFO]: iter=6869 {Training loss}=3.6507748291114694e-07\n",
            "2023-06-04 15:40:27,732 [nnabla][INFO]: iter=6869 {Training error}=0.0\n",
            "2023-06-04 15:40:27,766 [nnabla][INFO]: iter=6879 {Training loss}=3.6163160643809533e-07\n",
            "2023-06-04 15:40:27,767 [nnabla][INFO]: iter=6879 {Training error}=0.0\n",
            "2023-06-04 15:40:27,803 [nnabla][INFO]: iter=6889 {Training loss}=3.57626959157642e-07\n",
            "2023-06-04 15:40:27,803 [nnabla][INFO]: iter=6889 {Training error}=0.0\n",
            "2023-06-04 15:40:27,839 [nnabla][INFO]: iter=6899 {Training loss}=3.595827706703858e-07\n",
            "2023-06-04 15:40:27,839 [nnabla][INFO]: iter=6899 {Training error}=0.0\n",
            "2023-06-04 15:40:27,839 [nnabla][INFO]: iter=6899 {Training time}=0.37852025032043457[sec/100iter] 31.055703163146973[sec]\n",
            "2023-06-04 15:40:27,860 [nnabla][INFO]: iter=6900 {Test error}=0.01484375\n",
            "2023-06-04 15:40:27,893 [nnabla][INFO]: iter=6909 {Training loss}=3.400250534468796e-07\n",
            "2023-06-04 15:40:27,893 [nnabla][INFO]: iter=6909 {Training error}=0.0\n",
            "2023-06-04 15:40:27,927 [nnabla][INFO]: iter=6919 {Training loss}=3.674989272894891e-07\n",
            "2023-06-04 15:40:27,928 [nnabla][INFO]: iter=6919 {Training error}=0.0\n",
            "2023-06-04 15:40:27,964 [nnabla][INFO]: iter=6929 {Training loss}=3.534360075718723e-07\n",
            "2023-06-04 15:40:27,964 [nnabla][INFO]: iter=6929 {Training error}=0.0\n",
            "2023-06-04 15:40:27,998 [nnabla][INFO]: iter=6939 {Training loss}=3.3723105730132374e-07\n",
            "2023-06-04 15:40:27,998 [nnabla][INFO]: iter=6939 {Training error}=0.0\n",
            "2023-06-04 15:40:28,032 [nnabla][INFO]: iter=6949 {Training loss}=3.596758801904798e-07\n",
            "2023-06-04 15:40:28,032 [nnabla][INFO]: iter=6949 {Training error}=0.0\n",
            "2023-06-04 15:40:28,066 [nnabla][INFO]: iter=6959 {Training loss}=3.417014227125037e-07\n",
            "2023-06-04 15:40:28,066 [nnabla][INFO]: iter=6959 {Training error}=0.0\n",
            "2023-06-04 15:40:28,099 [nnabla][INFO]: iter=6969 {Training loss}=3.482206523131026e-07\n",
            "2023-06-04 15:40:28,100 [nnabla][INFO]: iter=6969 {Training error}=0.0\n",
            "2023-06-04 15:40:28,137 [nnabla][INFO]: iter=6979 {Training loss}=3.2251625725621125e-07\n",
            "2023-06-04 15:40:28,137 [nnabla][INFO]: iter=6979 {Training error}=0.0\n",
            "2023-06-04 15:40:28,171 [nnabla][INFO]: iter=6989 {Training loss}=3.650775113328564e-07\n",
            "2023-06-04 15:40:28,171 [nnabla][INFO]: iter=6989 {Training error}=0.0\n",
            "2023-06-04 15:40:28,205 [nnabla][INFO]: iter=6999 {Training loss}=3.239132695398439e-07\n",
            "2023-06-04 15:40:28,205 [nnabla][INFO]: iter=6999 {Training error}=0.0\n",
            "2023-06-04 15:40:28,205 [nnabla][INFO]: iter=6999 {Training time}=0.36618661880493164[sec/100iter] 31.421889781951904[sec]\n",
            "2023-06-04 15:40:28,226 [nnabla][INFO]: iter=7000 {Test error}=0.0140625\n",
            "2023-06-04 15:40:28,240 [nnabla][INFO]: Solver state save (.h5): output/states_7000.h5\n",
            "2023-06-04 15:40:28,247 [nnabla][INFO]: Parameter save (.h5): output/params_7000.h5\n",
            "2023-06-04 15:40:28,247 [nnabla][INFO]: Checkpoint save (.json): output/checkpoint_7000.json\n",
            "2023-06-04 15:40:28,287 [nnabla][INFO]: iter=7009 {Training loss}=3.3387837561349443e-07\n",
            "2023-06-04 15:40:28,287 [nnabla][INFO]: iter=7009 {Training error}=0.0\n",
            "2023-06-04 15:40:28,324 [nnabla][INFO]: iter=7019 {Training loss}=3.276384745731775e-07\n",
            "2023-06-04 15:40:28,325 [nnabla][INFO]: iter=7019 {Training error}=0.0\n",
            "2023-06-04 15:40:28,358 [nnabla][INFO]: iter=7029 {Training loss}=3.199085938376811e-07\n",
            "2023-06-04 15:40:28,358 [nnabla][INFO]: iter=7029 {Training error}=0.0\n",
            "2023-06-04 15:40:28,391 [nnabla][INFO]: iter=7039 {Training loss}=3.2223684343080095e-07\n",
            "2023-06-04 15:40:28,392 [nnabla][INFO]: iter=7039 {Training error}=0.0\n",
            "2023-06-04 15:40:28,425 [nnabla][INFO]: iter=7049 {Training loss}=3.402113293304865e-07\n",
            "2023-06-04 15:40:28,425 [nnabla][INFO]: iter=7049 {Training error}=0.0\n",
            "2023-06-04 15:40:28,460 [nnabla][INFO]: iter=7059 {Training loss}=3.1469318173549254e-07\n",
            "2023-06-04 15:40:28,460 [nnabla][INFO]: iter=7059 {Training error}=0.0\n",
            "2023-06-04 15:40:28,493 [nnabla][INFO]: iter=7069 {Training loss}=3.13016784048159e-07\n",
            "2023-06-04 15:40:28,494 [nnabla][INFO]: iter=7069 {Training error}=0.0\n",
            "2023-06-04 15:40:28,533 [nnabla][INFO]: iter=7079 {Training loss}=3.311775458314514e-07\n",
            "2023-06-04 15:40:28,533 [nnabla][INFO]: iter=7079 {Training error}=0.0\n",
            "2023-06-04 15:40:28,569 [nnabla][INFO]: iter=7089 {Training loss}=3.188841333212622e-07\n",
            "2023-06-04 15:40:28,569 [nnabla][INFO]: iter=7089 {Training error}=0.0\n",
            "2023-06-04 15:40:28,603 [nnabla][INFO]: iter=7099 {Training loss}=2.969050001411233e-07\n",
            "2023-06-04 15:40:28,603 [nnabla][INFO]: iter=7099 {Training error}=0.0\n",
            "2023-06-04 15:40:28,603 [nnabla][INFO]: iter=7099 {Training time}=0.397719144821167[sec/100iter] 31.81960892677307[sec]\n",
            "2023-06-04 15:40:28,624 [nnabla][INFO]: iter=7100 {Test error}=0.0140625\n",
            "2023-06-04 15:40:28,658 [nnabla][INFO]: iter=7109 {Training loss}=3.080808141930902e-07\n",
            "2023-06-04 15:40:28,658 [nnabla][INFO]: iter=7109 {Training error}=0.0\n",
            "2023-06-04 15:40:28,692 [nnabla][INFO]: iter=7119 {Training loss}=3.0258601668720075e-07\n",
            "2023-06-04 15:40:28,692 [nnabla][INFO]: iter=7119 {Training error}=0.0\n",
            "2023-06-04 15:40:28,730 [nnabla][INFO]: iter=7129 {Training loss}=3.300599757949385e-07\n",
            "2023-06-04 15:40:28,730 [nnabla][INFO]: iter=7129 {Training error}=0.0\n",
            "2023-06-04 15:40:28,764 [nnabla][INFO]: iter=7139 {Training loss}=3.028654589343205e-07\n",
            "2023-06-04 15:40:28,765 [nnabla][INFO]: iter=7139 {Training error}=0.0\n",
            "2023-06-04 15:40:28,799 [nnabla][INFO]: iter=7149 {Training loss}=2.9988518690515775e-07\n",
            "2023-06-04 15:40:28,799 [nnabla][INFO]: iter=7149 {Training error}=0.0\n",
            "2023-06-04 15:40:28,838 [nnabla][INFO]: iter=7159 {Training loss}=3.0826709007669706e-07\n",
            "2023-06-04 15:40:28,838 [nnabla][INFO]: iter=7159 {Training error}=0.0\n",
            "2023-06-04 15:40:28,877 [nnabla][INFO]: iter=7169 {Training loss}=2.926209106135502e-07\n",
            "2023-06-04 15:40:28,877 [nnabla][INFO]: iter=7169 {Training error}=0.0\n",
            "2023-06-04 15:40:28,911 [nnabla][INFO]: iter=7179 {Training loss}=2.9280715807544766e-07\n",
            "2023-06-04 15:40:28,911 [nnabla][INFO]: iter=7179 {Training error}=0.0\n",
            "2023-06-04 15:40:28,945 [nnabla][INFO]: iter=7189 {Training loss}=3.076151813274919e-07\n",
            "2023-06-04 15:40:28,945 [nnabla][INFO]: iter=7189 {Training error}=0.0\n",
            "2023-06-04 15:40:28,979 [nnabla][INFO]: iter=7199 {Training loss}=2.9299343395905453e-07\n",
            "2023-06-04 15:40:28,980 [nnabla][INFO]: iter=7199 {Training error}=0.0\n",
            "2023-06-04 15:40:28,980 [nnabla][INFO]: iter=7199 {Training time}=0.3766465187072754[sec/100iter] 32.19625544548035[sec]\n",
            "2023-06-04 15:40:29,000 [nnabla][INFO]: iter=7200 {Test error}=0.0140625\n",
            "2023-06-04 15:40:29,039 [nnabla][INFO]: iter=7209 {Training loss}=2.915033405770373e-07\n",
            "2023-06-04 15:40:29,039 [nnabla][INFO]: iter=7209 {Training error}=0.0\n",
            "2023-06-04 15:40:29,075 [nnabla][INFO]: iter=7219 {Training loss}=2.890819246204046e-07\n",
            "2023-06-04 15:40:29,075 [nnabla][INFO]: iter=7219 {Training error}=0.0\n",
            "2023-06-04 15:40:29,108 [nnabla][INFO]: iter=7229 {Training loss}=2.8293521836531e-07\n",
            "2023-06-04 15:40:29,109 [nnabla][INFO]: iter=7229 {Training error}=0.0\n",
            "2023-06-04 15:40:29,150 [nnabla][INFO]: iter=7239 {Training loss}=2.91223926751627e-07\n",
            "2023-06-04 15:40:29,151 [nnabla][INFO]: iter=7239 {Training error}=0.0\n",
            "2023-06-04 15:40:29,184 [nnabla][INFO]: iter=7249 {Training loss}=2.735288830990612e-07\n",
            "2023-06-04 15:40:29,184 [nnabla][INFO]: iter=7249 {Training error}=0.0\n",
            "2023-06-04 15:40:29,218 [nnabla][INFO]: iter=7259 {Training loss}=2.952286024537898e-07\n",
            "2023-06-04 15:40:29,219 [nnabla][INFO]: iter=7259 {Training error}=0.0\n",
            "2023-06-04 15:40:29,252 [nnabla][INFO]: iter=7269 {Training loss}=2.818176483287971e-07\n",
            "2023-06-04 15:40:29,253 [nnabla][INFO]: iter=7269 {Training error}=0.0\n",
            "2023-06-04 15:40:29,286 [nnabla][INFO]: iter=7279 {Training loss}=2.7706789751391625e-07\n",
            "2023-06-04 15:40:29,286 [nnabla][INFO]: iter=7279 {Training error}=0.0\n",
            "2023-06-04 15:40:29,323 [nnabla][INFO]: iter=7289 {Training loss}=2.817244819652842e-07\n",
            "2023-06-04 15:40:29,323 [nnabla][INFO]: iter=7289 {Training error}=0.0\n",
            "2023-06-04 15:40:29,356 [nnabla][INFO]: iter=7299 {Training loss}=2.6998986868420616e-07\n",
            "2023-06-04 15:40:29,356 [nnabla][INFO]: iter=7299 {Training error}=0.0\n",
            "2023-06-04 15:40:29,356 [nnabla][INFO]: iter=7299 {Training time}=0.37674689292907715[sec/100iter] 32.573002338409424[sec]\n",
            "2023-06-04 15:40:29,377 [nnabla][INFO]: iter=7300 {Test error}=0.01484375\n",
            "2023-06-04 15:40:29,410 [nnabla][INFO]: iter=7309 {Training loss}=2.843322022272332e-07\n",
            "2023-06-04 15:40:29,411 [nnabla][INFO]: iter=7309 {Training error}=0.0\n",
            "2023-06-04 15:40:29,446 [nnabla][INFO]: iter=7319 {Training loss}=2.7036239202971046e-07\n",
            "2023-06-04 15:40:29,446 [nnabla][INFO]: iter=7319 {Training error}=0.0\n",
            "2023-06-04 15:40:29,486 [nnabla][INFO]: iter=7329 {Training loss}=2.62073655221684e-07\n",
            "2023-06-04 15:40:29,486 [nnabla][INFO]: iter=7329 {Training error}=0.0\n",
            "2023-06-04 15:40:29,522 [nnabla][INFO]: iter=7339 {Training loss}=2.622599311052909e-07\n",
            "2023-06-04 15:40:29,523 [nnabla][INFO]: iter=7339 {Training error}=0.0\n",
            "2023-06-04 15:40:29,558 [nnabla][INFO]: iter=7349 {Training loss}=2.765091267065145e-07\n",
            "2023-06-04 15:40:29,558 [nnabla][INFO]: iter=7349 {Training error}=0.0\n",
            "2023-06-04 15:40:29,594 [nnabla][INFO]: iter=7359 {Training loss}=2.6142174647247884e-07\n",
            "2023-06-04 15:40:29,594 [nnabla][INFO]: iter=7359 {Training error}=0.0\n",
            "2023-06-04 15:40:29,628 [nnabla][INFO]: iter=7369 {Training loss}=2.745533436154801e-07\n",
            "2023-06-04 15:40:29,628 [nnabla][INFO]: iter=7369 {Training error}=0.0\n",
            "2023-06-04 15:40:29,662 [nnabla][INFO]: iter=7379 {Training loss}=2.568582999629143e-07\n",
            "2023-06-04 15:40:29,662 [nnabla][INFO]: iter=7379 {Training error}=0.0\n",
            "2023-06-04 15:40:29,696 [nnabla][INFO]: iter=7389 {Training loss}=2.565788577157946e-07\n",
            "2023-06-04 15:40:29,696 [nnabla][INFO]: iter=7389 {Training error}=0.0\n",
            "2023-06-04 15:40:29,735 [nnabla][INFO]: iter=7399 {Training loss}=2.650538704074279e-07\n",
            "2023-06-04 15:40:29,735 [nnabla][INFO]: iter=7399 {Training error}=0.0\n",
            "2023-06-04 15:40:29,736 [nnabla][INFO]: iter=7399 {Training time}=0.37960052490234375[sec/100iter] 32.95260286331177[sec]\n",
            "2023-06-04 15:40:29,757 [nnabla][INFO]: iter=7400 {Test error}=0.0140625\n",
            "2023-06-04 15:40:29,792 [nnabla][INFO]: iter=7409 {Training loss}=2.571376853666152e-07\n",
            "2023-06-04 15:40:29,792 [nnabla][INFO]: iter=7409 {Training error}=0.0\n",
            "2023-06-04 15:40:29,828 [nnabla][INFO]: iter=7419 {Training loss}=2.496871331914008e-07\n",
            "2023-06-04 15:40:29,828 [nnabla][INFO]: iter=7419 {Training error}=0.0\n",
            "2023-06-04 15:40:29,862 [nnabla][INFO]: iter=7429 {Training loss}=2.553682065808971e-07\n",
            "2023-06-04 15:40:29,862 [nnabla][INFO]: iter=7429 {Training error}=0.0\n",
            "2023-06-04 15:40:29,896 [nnabla][INFO]: iter=7439 {Training loss}=2.5341236664644384e-07\n",
            "2023-06-04 15:40:29,896 [nnabla][INFO]: iter=7439 {Training error}=0.0\n",
            "2023-06-04 15:40:29,930 [nnabla][INFO]: iter=7449 {Training loss}=2.486627010966913e-07\n",
            "2023-06-04 15:40:29,930 [nnabla][INFO]: iter=7449 {Training error}=0.0\n",
            "2023-06-04 15:40:29,964 [nnabla][INFO]: iter=7459 {Training loss}=2.600247341888462e-07\n",
            "2023-06-04 15:40:29,964 [nnabla][INFO]: iter=7459 {Training error}=0.0\n",
            "2023-06-04 15:40:29,998 [nnabla][INFO]: iter=7469 {Training loss}=2.3953572281243396e-07\n",
            "2023-06-04 15:40:29,998 [nnabla][INFO]: iter=7469 {Training error}=0.0\n",
            "2023-06-04 15:40:30,037 [nnabla][INFO]: iter=7479 {Training loss}=2.446580253945285e-07\n",
            "2023-06-04 15:40:30,037 [nnabla][INFO]: iter=7479 {Training error}=0.0\n",
            "2023-06-04 15:40:30,072 [nnabla][INFO]: iter=7489 {Training loss}=2.5723085173012805e-07\n",
            "2023-06-04 15:40:30,072 [nnabla][INFO]: iter=7489 {Training error}=0.0\n",
            "2023-06-04 15:40:30,115 [nnabla][INFO]: iter=7499 {Training loss}=2.318057852335187e-07\n",
            "2023-06-04 15:40:30,116 [nnabla][INFO]: iter=7499 {Training error}=0.0\n",
            "2023-06-04 15:40:30,116 [nnabla][INFO]: iter=7499 {Training time}=0.3796195983886719[sec/100iter] 33.33222246170044[sec]\n",
            "2023-06-04 15:40:30,137 [nnabla][INFO]: iter=7500 {Test error}=0.0140625\n",
            "2023-06-04 15:40:30,171 [nnabla][INFO]: iter=7509 {Training loss}=2.4745196469666553e-07\n",
            "2023-06-04 15:40:30,172 [nnabla][INFO]: iter=7509 {Training error}=0.0\n",
            "2023-06-04 15:40:30,218 [nnabla][INFO]: iter=7519 {Training loss}=2.4316790359080187e-07\n",
            "2023-06-04 15:40:30,218 [nnabla][INFO]: iter=7519 {Training error}=0.0\n",
            "2023-06-04 15:40:30,254 [nnabla][INFO]: iter=7529 {Training loss}=2.3487912415021128e-07\n",
            "2023-06-04 15:40:30,254 [nnabla][INFO]: iter=7529 {Training error}=0.0\n",
            "2023-06-04 15:40:30,289 [nnabla][INFO]: iter=7539 {Training loss}=2.472657172347681e-07\n",
            "2023-06-04 15:40:30,289 [nnabla][INFO]: iter=7539 {Training error}=0.0\n",
            "2023-06-04 15:40:30,323 [nnabla][INFO]: iter=7549 {Training loss}=2.270560628403473e-07\n",
            "2023-06-04 15:40:30,323 [nnabla][INFO]: iter=7549 {Training error}=0.0\n",
            "2023-06-04 15:40:30,363 [nnabla][INFO]: iter=7559 {Training loss}=2.469863034093578e-07\n",
            "2023-06-04 15:40:30,363 [nnabla][INFO]: iter=7559 {Training error}=0.0\n",
            "2023-06-04 15:40:30,397 [nnabla][INFO]: iter=7569 {Training loss}=2.24075819232894e-07\n",
            "2023-06-04 15:40:30,397 [nnabla][INFO]: iter=7569 {Training error}=0.0\n",
            "2023-06-04 15:40:30,431 [nnabla][INFO]: iter=7579 {Training loss}=2.3283025996079232e-07\n",
            "2023-06-04 15:40:30,431 [nnabla][INFO]: iter=7579 {Training error}=0.0\n",
            "2023-06-04 15:40:30,465 [nnabla][INFO]: iter=7589 {Training loss}=2.3264396986633074e-07\n",
            "2023-06-04 15:40:30,465 [nnabla][INFO]: iter=7589 {Training error}=0.0\n",
            "2023-06-04 15:40:30,504 [nnabla][INFO]: iter=7599 {Training loss}=2.314332760988691e-07\n",
            "2023-06-04 15:40:30,505 [nnabla][INFO]: iter=7599 {Training error}=0.0\n",
            "2023-06-04 15:40:30,505 [nnabla][INFO]: iter=7599 {Training time}=0.38906049728393555[sec/100iter] 33.721282958984375[sec]\n",
            "2023-06-04 15:40:30,529 [nnabla][INFO]: iter=7600 {Test error}=0.01484375\n",
            "2023-06-04 15:40:30,567 [nnabla][INFO]: iter=7609 {Training loss}=2.3031569185150147e-07\n",
            "2023-06-04 15:40:30,568 [nnabla][INFO]: iter=7609 {Training error}=0.0\n",
            "2023-06-04 15:40:30,603 [nnabla][INFO]: iter=7619 {Training loss}=2.206299569706971e-07\n",
            "2023-06-04 15:40:30,603 [nnabla][INFO]: iter=7619 {Training error}=0.0\n",
            "2023-06-04 15:40:30,637 [nnabla][INFO]: iter=7629 {Training loss}=2.2919812181498855e-07\n",
            "2023-06-04 15:40:30,638 [nnabla][INFO]: iter=7629 {Training error}=0.0\n",
            "2023-06-04 15:40:30,673 [nnabla][INFO]: iter=7639 {Training loss}=2.180222509196028e-07\n",
            "2023-06-04 15:40:30,673 [nnabla][INFO]: iter=7639 {Training error}=0.0\n",
            "2023-06-04 15:40:30,708 [nnabla][INFO]: iter=7649 {Training loss}=2.2696292489854386e-07\n",
            "2023-06-04 15:40:30,708 [nnabla][INFO]: iter=7649 {Training error}=0.0\n",
            "2023-06-04 15:40:30,745 [nnabla][INFO]: iter=7659 {Training loss}=2.2435521884744958e-07\n",
            "2023-06-04 15:40:30,745 [nnabla][INFO]: iter=7659 {Training error}=0.0\n",
            "2023-06-04 15:40:30,781 [nnabla][INFO]: iter=7669 {Training loss}=2.1615963419208128e-07\n",
            "2023-06-04 15:40:30,781 [nnabla][INFO]: iter=7669 {Training error}=0.0\n",
            "2023-06-04 15:40:30,823 [nnabla][INFO]: iter=7679 {Training loss}=2.0964039038062765e-07\n",
            "2023-06-04 15:40:30,823 [nnabla][INFO]: iter=7679 {Training error}=0.0\n",
            "2023-06-04 15:40:30,860 [nnabla][INFO]: iter=7689 {Training loss}=2.247277564038086e-07\n",
            "2023-06-04 15:40:30,860 [nnabla][INFO]: iter=7689 {Training error}=0.0\n",
            "2023-06-04 15:40:30,895 [nnabla][INFO]: iter=7699 {Training loss}=2.2854619885492866e-07\n",
            "2023-06-04 15:40:30,896 [nnabla][INFO]: iter=7699 {Training error}=0.0\n",
            "2023-06-04 15:40:30,896 [nnabla][INFO]: iter=7699 {Training time}=0.3910377025604248[sec/100iter] 34.1123206615448[sec]\n",
            "2023-06-04 15:40:30,917 [nnabla][INFO]: iter=7700 {Test error}=0.0140625\n",
            "2023-06-04 15:40:30,952 [nnabla][INFO]: iter=7709 {Training loss}=2.0265547107101156e-07\n",
            "2023-06-04 15:40:30,952 [nnabla][INFO]: iter=7709 {Training error}=0.0\n",
            "2023-06-04 15:40:30,987 [nnabla][INFO]: iter=7719 {Training loss}=2.1429698904285033e-07\n",
            "2023-06-04 15:40:30,988 [nnabla][INFO]: iter=7719 {Training error}=0.0\n",
            "2023-06-04 15:40:31,021 [nnabla][INFO]: iter=7729 {Training loss}=2.1681155715214118e-07\n",
            "2023-06-04 15:40:31,021 [nnabla][INFO]: iter=7729 {Training error}=0.0\n",
            "2023-06-04 15:40:31,057 [nnabla][INFO]: iter=7739 {Training loss}=2.067532989258325e-07\n",
            "2023-06-04 15:40:31,057 [nnabla][INFO]: iter=7739 {Training error}=0.0\n",
            "2023-06-04 15:40:31,102 [nnabla][INFO]: iter=7749 {Training loss}=2.0563570046761015e-07\n",
            "2023-06-04 15:40:31,102 [nnabla][INFO]: iter=7749 {Training error}=0.0\n",
            "2023-06-04 15:40:31,143 [nnabla][INFO]: iter=7759 {Training loss}=2.1224809643172193e-07\n",
            "2023-06-04 15:40:31,143 [nnabla][INFO]: iter=7759 {Training error}=0.0\n",
            "2023-06-04 15:40:31,177 [nnabla][INFO]: iter=7769 {Training loss}=2.1113048376264487e-07\n",
            "2023-06-04 15:40:31,177 [nnabla][INFO]: iter=7769 {Training error}=0.0\n",
            "2023-06-04 15:40:31,213 [nnabla][INFO]: iter=7779 {Training loss}=2.091747148824652e-07\n",
            "2023-06-04 15:40:31,213 [nnabla][INFO]: iter=7779 {Training error}=0.0\n",
            "2023-06-04 15:40:31,247 [nnabla][INFO]: iter=7789 {Training loss}=2.0824340651870443e-07\n",
            "2023-06-04 15:40:31,247 [nnabla][INFO]: iter=7789 {Training error}=0.0\n",
            "2023-06-04 15:40:31,281 [nnabla][INFO]: iter=7799 {Training loss}=2.0386622168189206e-07\n",
            "2023-06-04 15:40:31,281 [nnabla][INFO]: iter=7799 {Training error}=0.0\n",
            "2023-06-04 15:40:31,282 [nnabla][INFO]: iter=7799 {Training time}=0.3858792781829834[sec/100iter] 34.49819993972778[sec]\n",
            "2023-06-04 15:40:31,302 [nnabla][INFO]: iter=7800 {Test error}=0.0140625\n",
            "2023-06-04 15:40:31,343 [nnabla][INFO]: iter=7809 {Training loss}=2.1001289951527724e-07\n",
            "2023-06-04 15:40:31,343 [nnabla][INFO]: iter=7809 {Training error}=0.0\n",
            "2023-06-04 15:40:31,379 [nnabla][INFO]: iter=7819 {Training loss}=1.9203844203730114e-07\n",
            "2023-06-04 15:40:31,379 [nnabla][INFO]: iter=7819 {Training error}=0.0\n",
            "2023-06-04 15:40:31,413 [nnabla][INFO]: iter=7829 {Training loss}=2.0852280613326002e-07\n",
            "2023-06-04 15:40:31,414 [nnabla][INFO]: iter=7829 {Training error}=0.0\n",
            "2023-06-04 15:40:31,449 [nnabla][INFO]: iter=7839 {Training loss}=1.9380794924472866e-07\n",
            "2023-06-04 15:40:31,449 [nnabla][INFO]: iter=7839 {Training error}=0.0\n",
            "2023-06-04 15:40:31,483 [nnabla][INFO]: iter=7849 {Training loss}=2.0386619326018263e-07\n",
            "2023-06-04 15:40:31,483 [nnabla][INFO]: iter=7849 {Training error}=0.0\n",
            "2023-06-04 15:40:31,517 [nnabla][INFO]: iter=7859 {Training loss}=1.9203847045901057e-07\n",
            "2023-06-04 15:40:31,517 [nnabla][INFO]: iter=7859 {Training error}=0.0\n",
            "2023-06-04 15:40:31,552 [nnabla][INFO]: iter=7869 {Training loss}=2.000477934416267e-07\n",
            "2023-06-04 15:40:31,552 [nnabla][INFO]: iter=7869 {Training error}=0.0\n",
            "2023-06-04 15:40:31,585 [nnabla][INFO]: iter=7879 {Training loss}=1.922247037100533e-07\n",
            "2023-06-04 15:40:31,586 [nnabla][INFO]: iter=7879 {Training error}=0.0\n",
            "2023-06-04 15:40:31,625 [nnabla][INFO]: iter=7889 {Training loss}=1.97626377484994e-07\n",
            "2023-06-04 15:40:31,625 [nnabla][INFO]: iter=7889 {Training error}=0.0\n",
            "2023-06-04 15:40:31,666 [nnabla][INFO]: iter=7899 {Training loss}=1.936216733611218e-07\n",
            "2023-06-04 15:40:31,666 [nnabla][INFO]: iter=7899 {Training error}=0.0\n",
            "2023-06-04 15:40:31,666 [nnabla][INFO]: iter=7899 {Training time}=0.3842642307281494[sec/100iter] 34.88246417045593[sec]\n",
            "2023-06-04 15:40:31,687 [nnabla][INFO]: iter=7900 {Test error}=0.0140625\n",
            "2023-06-04 15:40:31,721 [nnabla][INFO]: iter=7909 {Training loss}=1.8868568929519824e-07\n",
            "2023-06-04 15:40:31,721 [nnabla][INFO]: iter=7909 {Training error}=0.0\n",
            "2023-06-04 15:40:31,758 [nnabla][INFO]: iter=7919 {Training loss}=1.909208577899335e-07\n",
            "2023-06-04 15:40:31,758 [nnabla][INFO]: iter=7919 {Training error}=0.0\n",
            "2023-06-04 15:40:31,795 [nnabla][INFO]: iter=7929 {Training loss}=1.9287662667011318e-07\n",
            "2023-06-04 15:40:31,795 [nnabla][INFO]: iter=7929 {Training error}=0.0\n",
            "2023-06-04 15:40:31,831 [nnabla][INFO]: iter=7939 {Training loss}=1.8393595269117213e-07\n",
            "2023-06-04 15:40:31,831 [nnabla][INFO]: iter=7939 {Training error}=0.0\n",
            "2023-06-04 15:40:31,868 [nnabla][INFO]: iter=7949 {Training loss}=1.8402909063297557e-07\n",
            "2023-06-04 15:40:31,868 [nnabla][INFO]: iter=7949 {Training error}=0.0\n",
            "2023-06-04 15:40:31,903 [nnabla][INFO]: iter=7959 {Training loss}=1.8915135058250598e-07\n",
            "2023-06-04 15:40:31,903 [nnabla][INFO]: iter=7959 {Training error}=0.0\n",
            "2023-06-04 15:40:31,942 [nnabla][INFO]: iter=7969 {Training loss}=1.8235270715649676e-07\n",
            "2023-06-04 15:40:31,943 [nnabla][INFO]: iter=7969 {Training error}=0.0\n",
            "2023-06-04 15:40:31,978 [nnabla][INFO]: iter=7979 {Training loss}=1.790930781453426e-07\n",
            "2023-06-04 15:40:31,978 [nnabla][INFO]: iter=7979 {Training error}=0.0\n",
            "2023-06-04 15:40:32,013 [nnabla][INFO]: iter=7989 {Training loss}=1.8086259956362483e-07\n",
            "2023-06-04 15:40:32,013 [nnabla][INFO]: iter=7989 {Training error}=0.0\n",
            "2023-06-04 15:40:32,047 [nnabla][INFO]: iter=7999 {Training loss}=1.8402910484383028e-07\n",
            "2023-06-04 15:40:32,047 [nnabla][INFO]: iter=7999 {Training error}=0.0\n",
            "2023-06-04 15:40:32,048 [nnabla][INFO]: iter=7999 {Training time}=0.38167500495910645[sec/100iter] 35.26413917541504[sec]\n",
            "2023-06-04 15:40:32,072 [nnabla][INFO]: iter=8000 {Test error}=0.01484375\n",
            "2023-06-04 15:40:32,086 [nnabla][INFO]: Solver state save (.h5): output/states_8000.h5\n",
            "2023-06-04 15:40:32,092 [nnabla][INFO]: Parameter save (.h5): output/params_8000.h5\n",
            "2023-06-04 15:40:32,093 [nnabla][INFO]: Checkpoint save (.json): output/checkpoint_8000.json\n",
            "2023-06-04 15:40:32,127 [nnabla][INFO]: iter=8009 {Training loss}=1.7788238437788095e-07\n",
            "2023-06-04 15:40:32,127 [nnabla][INFO]: iter=8009 {Training error}=0.0\n",
            "2023-06-04 15:40:32,161 [nnabla][INFO]: iter=8019 {Training loss}=1.8011755287261622e-07\n",
            "2023-06-04 15:40:32,161 [nnabla][INFO]: iter=8019 {Training error}=0.0\n",
            "2023-06-04 15:40:32,198 [nnabla][INFO]: iter=8029 {Training loss}=1.7890683068344515e-07\n",
            "2023-06-04 15:40:32,198 [nnabla][INFO]: iter=8029 {Training error}=0.0\n",
            "2023-06-04 15:40:32,235 [nnabla][INFO]: iter=8039 {Training loss}=1.7257386275559838e-07\n",
            "2023-06-04 15:40:32,235 [nnabla][INFO]: iter=8039 {Training error}=0.0\n",
            "2023-06-04 15:40:32,270 [nnabla][INFO]: iter=8049 {Training loss}=1.7788235595617152e-07\n",
            "2023-06-04 15:40:32,270 [nnabla][INFO]: iter=8049 {Training error}=0.0\n",
            "2023-06-04 15:40:32,304 [nnabla][INFO]: iter=8059 {Training loss}=1.6512328215867456e-07\n",
            "2023-06-04 15:40:32,304 [nnabla][INFO]: iter=8059 {Training error}=0.0\n",
            "2023-06-04 15:40:32,342 [nnabla][INFO]: iter=8069 {Training loss}=1.8030382875622308e-07\n",
            "2023-06-04 15:40:32,344 [nnabla][INFO]: iter=8069 {Training error}=0.0\n",
            "2023-06-04 15:40:32,380 [nnabla][INFO]: iter=8079 {Training loss}=1.780686460506331e-07\n",
            "2023-06-04 15:40:32,380 [nnabla][INFO]: iter=8079 {Training error}=0.0\n",
            "2023-06-04 15:40:32,414 [nnabla][INFO]: iter=8089 {Training loss}=1.6856917284258088e-07\n",
            "2023-06-04 15:40:32,414 [nnabla][INFO]: iter=8089 {Training error}=0.0\n",
            "2023-06-04 15:40:32,449 [nnabla][INFO]: iter=8099 {Training loss}=1.706180938754187e-07\n",
            "2023-06-04 15:40:32,450 [nnabla][INFO]: iter=8099 {Training error}=0.0\n",
            "2023-06-04 15:40:32,450 [nnabla][INFO]: iter=8099 {Training time}=0.4020984172821045[sec/100iter] 35.666237592697144[sec]\n",
            "2023-06-04 15:40:32,470 [nnabla][INFO]: iter=8100 {Test error}=0.0140625\n",
            "2023-06-04 15:40:32,504 [nnabla][INFO]: iter=8109 {Training loss}=1.6083920684195618e-07\n",
            "2023-06-04 15:40:32,504 [nnabla][INFO]: iter=8109 {Training error}=0.0\n",
            "2023-06-04 15:40:32,539 [nnabla][INFO]: iter=8119 {Training loss}=1.7788238437788095e-07\n",
            "2023-06-04 15:40:32,539 [nnabla][INFO]: iter=8119 {Training error}=0.0\n",
            "2023-06-04 15:40:32,575 [nnabla][INFO]: iter=8129 {Training loss}=1.7071121760636743e-07\n",
            "2023-06-04 15:40:32,575 [nnabla][INFO]: iter=8129 {Training error}=0.0\n",
            "2023-06-04 15:40:32,615 [nnabla][INFO]: iter=8139 {Training loss}=1.642850975258625e-07\n",
            "2023-06-04 15:40:32,615 [nnabla][INFO]: iter=8139 {Training error}=0.0\n",
            "2023-06-04 15:40:32,649 [nnabla][INFO]: iter=8149 {Training loss}=1.6260872826023842e-07\n",
            "2023-06-04 15:40:32,650 [nnabla][INFO]: iter=8149 {Training error}=0.0\n",
            "2023-06-04 15:40:32,684 [nnabla][INFO]: iter=8159 {Training loss}=1.6354003662399919e-07\n",
            "2023-06-04 15:40:32,684 [nnabla][INFO]: iter=8159 {Training error}=0.0\n",
            "2023-06-04 15:40:32,719 [nnabla][INFO]: iter=8169 {Training loss}=1.6754471232616197e-07\n",
            "2023-06-04 15:40:32,719 [nnabla][INFO]: iter=8169 {Training error}=0.0\n",
            "2023-06-04 15:40:32,753 [nnabla][INFO]: iter=8179 {Training loss}=1.6065295938005875e-07\n",
            "2023-06-04 15:40:32,754 [nnabla][INFO]: iter=8179 {Training error}=0.0\n",
            "2023-06-04 15:40:32,792 [nnabla][INFO]: iter=8189 {Training loss}=1.560894844487848e-07\n",
            "2023-06-04 15:40:32,793 [nnabla][INFO]: iter=8189 {Training error}=0.0\n",
            "2023-06-04 15:40:32,826 [nnabla][INFO]: iter=8199 {Training loss}=1.5795212959801574e-07\n",
            "2023-06-04 15:40:32,826 [nnabla][INFO]: iter=8199 {Training error}=0.0\n",
            "2023-06-04 15:40:32,826 [nnabla][INFO]: iter=8199 {Training time}=0.3768141269683838[sec/100iter] 36.04305171966553[sec]\n",
            "2023-06-04 15:40:32,847 [nnabla][INFO]: iter=8200 {Test error}=0.0140625\n",
            "2023-06-04 15:40:32,884 [nnabla][INFO]: iter=8209 {Training loss}=1.6828978743888e-07\n",
            "2023-06-04 15:40:32,884 [nnabla][INFO]: iter=8209 {Training error}=0.0\n",
            "2023-06-04 15:40:32,918 [nnabla][INFO]: iter=8219 {Training loss}=1.5338865466674179e-07\n",
            "2023-06-04 15:40:32,918 [nnabla][INFO]: iter=8219 {Training error}=0.0\n",
            "2023-06-04 15:40:32,953 [nnabla][INFO]: iter=8229 {Training loss}=1.504084252701432e-07\n",
            "2023-06-04 15:40:32,953 [nnabla][INFO]: iter=8229 {Training error}=0.0\n",
            "2023-06-04 15:40:32,988 [nnabla][INFO]: iter=8239 {Training loss}=1.5757959204165672e-07\n",
            "2023-06-04 15:40:32,988 [nnabla][INFO]: iter=8239 {Training error}=0.0\n",
            "2023-06-04 15:40:33,027 [nnabla][INFO]: iter=8249 {Training loss}=1.6214305276207597e-07\n",
            "2023-06-04 15:40:33,027 [nnabla][INFO]: iter=8249 {Training error}=0.0\n",
            "2023-06-04 15:40:33,063 [nnabla][INFO]: iter=8259 {Training loss}=1.5469248637600685e-07\n",
            "2023-06-04 15:40:33,063 [nnabla][INFO]: iter=8259 {Training error}=0.0\n",
            "2023-06-04 15:40:33,096 [nnabla][INFO]: iter=8269 {Training loss}=1.5292299337943405e-07\n",
            "2023-06-04 15:40:33,097 [nnabla][INFO]: iter=8269 {Training error}=0.0\n",
            "2023-06-04 15:40:33,132 [nnabla][INFO]: iter=8279 {Training loss}=1.5255045582307503e-07\n",
            "2023-06-04 15:40:33,132 [nnabla][INFO]: iter=8279 {Training error}=0.0\n",
            "2023-06-04 15:40:33,169 [nnabla][INFO]: iter=8289 {Training loss}=1.4519302737880935e-07\n",
            "2023-06-04 15:40:33,169 [nnabla][INFO]: iter=8289 {Training error}=0.0\n",
            "2023-06-04 15:40:33,203 [nnabla][INFO]: iter=8299 {Training loss}=1.5133971942304925e-07\n",
            "2023-06-04 15:40:33,203 [nnabla][INFO]: iter=8299 {Training error}=0.0\n",
            "2023-06-04 15:40:33,204 [nnabla][INFO]: iter=8299 {Training time}=0.3771250247955322[sec/100iter] 36.42017674446106[sec]\n",
            "2023-06-04 15:40:33,224 [nnabla][INFO]: iter=8300 {Test error}=0.01484375\n",
            "2023-06-04 15:40:33,258 [nnabla][INFO]: iter=8309 {Training loss}=1.5357491633949394e-07\n",
            "2023-06-04 15:40:33,258 [nnabla][INFO]: iter=8309 {Training error}=0.0\n",
            "2023-06-04 15:40:33,299 [nnabla][INFO]: iter=8319 {Training loss}=1.4789385716085235e-07\n",
            "2023-06-04 15:40:33,299 [nnabla][INFO]: iter=8319 {Training error}=0.0\n",
            "2023-06-04 15:40:33,333 [nnabla][INFO]: iter=8329 {Training loss}=1.4239907386581763e-07\n",
            "2023-06-04 15:40:33,333 [nnabla][INFO]: iter=8329 {Training error}=0.0\n",
            "2023-06-04 15:40:33,372 [nnabla][INFO]: iter=8339 {Training loss}=1.5217794668842544e-07\n",
            "2023-06-04 15:40:33,372 [nnabla][INFO]: iter=8339 {Training error}=0.0\n",
            "2023-06-04 15:40:33,406 [nnabla][INFO]: iter=8349 {Training loss}=1.4351664390233054e-07\n",
            "2023-06-04 15:40:33,406 [nnabla][INFO]: iter=8349 {Training error}=0.0\n",
            "2023-06-04 15:40:33,439 [nnabla][INFO]: iter=8359 {Training loss}=1.5031528732833976e-07\n",
            "2023-06-04 15:40:33,439 [nnabla][INFO]: iter=8359 {Training error}=0.0\n",
            "2023-06-04 15:40:33,475 [nnabla][INFO]: iter=8369 {Training loss}=1.470556725280403e-07\n",
            "2023-06-04 15:40:33,475 [nnabla][INFO]: iter=8369 {Training error}=0.0\n",
            "2023-06-04 15:40:33,509 [nnabla][INFO]: iter=8379 {Training loss}=1.4696254879709159e-07\n",
            "2023-06-04 15:40:33,509 [nnabla][INFO]: iter=8379 {Training error}=0.0\n",
            "2023-06-04 15:40:33,543 [nnabla][INFO]: iter=8389 {Training loss}=1.389531831819113e-07\n",
            "2023-06-04 15:40:33,544 [nnabla][INFO]: iter=8389 {Training error}=0.0\n",
            "2023-06-04 15:40:33,578 [nnabla][INFO]: iter=8399 {Training loss}=1.5096722449925437e-07\n",
            "2023-06-04 15:40:33,578 [nnabla][INFO]: iter=8399 {Training error}=0.0\n",
            "2023-06-04 15:40:33,579 [nnabla][INFO]: iter=8399 {Training time}=0.3750147819519043[sec/100iter] 36.795191526412964[sec]\n",
            "2023-06-04 15:40:33,599 [nnabla][INFO]: iter=8400 {Test error}=0.0140625\n",
            "2023-06-04 15:40:33,634 [nnabla][INFO]: iter=8409 {Training loss}=1.344828604032955e-07\n",
            "2023-06-04 15:40:33,634 [nnabla][INFO]: iter=8409 {Training error}=0.0\n",
            "2023-06-04 15:40:33,678 [nnabla][INFO]: iter=8419 {Training loss}=1.4035016704383452e-07\n",
            "2023-06-04 15:40:33,678 [nnabla][INFO]: iter=8419 {Training error}=0.0\n",
            "2023-06-04 15:40:33,712 [nnabla][INFO]: iter=8429 {Training loss}=1.4165399875309959e-07\n",
            "2023-06-04 15:40:33,712 [nnabla][INFO]: iter=8429 {Training error}=0.0\n",
            "2023-06-04 15:40:33,746 [nnabla][INFO]: iter=8439 {Training loss}=1.4109522794569784e-07\n",
            "2023-06-04 15:40:33,747 [nnabla][INFO]: iter=8439 {Training error}=0.0\n",
            "2023-06-04 15:40:33,781 [nnabla][INFO]: iter=8449 {Training loss}=1.4044330498563795e-07\n",
            "2023-06-04 15:40:33,781 [nnabla][INFO]: iter=8449 {Training error}=0.0\n",
            "2023-06-04 15:40:33,820 [nnabla][INFO]: iter=8459 {Training loss}=1.3504164542155195e-07\n",
            "2023-06-04 15:40:33,820 [nnabla][INFO]: iter=8459 {Training error}=0.0\n",
            "2023-06-04 15:40:33,855 [nnabla][INFO]: iter=8469 {Training loss}=1.4007076742927893e-07\n",
            "2023-06-04 15:40:33,855 [nnabla][INFO]: iter=8469 {Training error}=0.0\n",
            "2023-06-04 15:40:33,890 [nnabla][INFO]: iter=8479 {Training loss}=1.3438972246149206e-07\n",
            "2023-06-04 15:40:33,890 [nnabla][INFO]: iter=8479 {Training error}=0.0\n",
            "2023-06-04 15:40:33,932 [nnabla][INFO]: iter=8489 {Training loss}=1.3746307558903936e-07\n",
            "2023-06-04 15:40:33,932 [nnabla][INFO]: iter=8489 {Training error}=0.0\n",
            "2023-06-04 15:40:33,984 [nnabla][INFO]: iter=8499 {Training loss}=1.356004162289537e-07\n",
            "2023-06-04 15:40:33,984 [nnabla][INFO]: iter=8499 {Training error}=0.0\n",
            "2023-06-04 15:40:33,985 [nnabla][INFO]: iter=8499 {Training time}=0.4059755802154541[sec/100iter] 37.20116710662842[sec]\n",
            "2023-06-04 15:40:34,010 [nnabla][INFO]: iter=8500 {Test error}=0.0140625\n",
            "2023-06-04 15:40:34,055 [nnabla][INFO]: iter=8509 {Training loss}=1.3271333898501325e-07\n",
            "2023-06-04 15:40:34,055 [nnabla][INFO]: iter=8509 {Training error}=0.0\n",
            "2023-06-04 15:40:34,093 [nnabla][INFO]: iter=8519 {Training loss}=1.3140947885403875e-07\n",
            "2023-06-04 15:40:34,094 [nnabla][INFO]: iter=8519 {Training error}=0.0\n",
            "2023-06-04 15:40:34,133 [nnabla][INFO]: iter=8529 {Training loss}=1.3243393937045767e-07\n",
            "2023-06-04 15:40:34,134 [nnabla][INFO]: iter=8529 {Training error}=0.0\n",
            "2023-06-04 15:40:34,180 [nnabla][INFO]: iter=8539 {Training loss}=1.2870864907199575e-07\n",
            "2023-06-04 15:40:34,180 [nnabla][INFO]: iter=8539 {Training error}=0.0\n",
            "2023-06-04 15:40:34,219 [nnabla][INFO]: iter=8549 {Training loss}=1.3010563293391897e-07\n",
            "2023-06-04 15:40:34,219 [nnabla][INFO]: iter=8549 {Training error}=0.0\n",
            "2023-06-04 15:40:34,257 [nnabla][INFO]: iter=8559 {Training loss}=1.2442457375527738e-07\n",
            "2023-06-04 15:40:34,258 [nnabla][INFO]: iter=8559 {Training error}=0.0\n",
            "2023-06-04 15:40:34,295 [nnabla][INFO]: iter=8569 {Training loss}=1.2768421697728627e-07\n",
            "2023-06-04 15:40:34,296 [nnabla][INFO]: iter=8569 {Training error}=0.0\n",
            "2023-06-04 15:40:34,338 [nnabla][INFO]: iter=8579 {Training loss}=1.2498337298438855e-07\n",
            "2023-06-04 15:40:34,338 [nnabla][INFO]: iter=8579 {Training error}=0.0\n",
            "2023-06-04 15:40:34,386 [nnabla][INFO]: iter=8589 {Training loss}=1.258215576172006e-07\n",
            "2023-06-04 15:40:34,386 [nnabla][INFO]: iter=8589 {Training error}=0.0\n",
            "2023-06-04 15:40:34,424 [nnabla][INFO]: iter=8599 {Training loss}=1.241451883515765e-07\n",
            "2023-06-04 15:40:34,424 [nnabla][INFO]: iter=8599 {Training error}=0.0\n",
            "2023-06-04 15:40:34,425 [nnabla][INFO]: iter=8599 {Training time}=0.4399580955505371[sec/100iter] 37.641125202178955[sec]\n",
            "2023-06-04 15:40:34,459 [nnabla][INFO]: iter=8600 {Test error}=0.0140625\n",
            "2023-06-04 15:40:34,500 [nnabla][INFO]: iter=8609 {Training loss}=1.2833612572649145e-07\n",
            "2023-06-04 15:40:34,500 [nnabla][INFO]: iter=8609 {Training error}=0.0\n",
            "2023-06-04 15:40:34,537 [nnabla][INFO]: iter=8619 {Training loss}=1.2479712552249111e-07\n",
            "2023-06-04 15:40:34,538 [nnabla][INFO]: iter=8619 {Training error}=0.0\n",
            "2023-06-04 15:40:34,577 [nnabla][INFO]: iter=8629 {Training loss}=1.220962815295934e-07\n",
            "2023-06-04 15:40:34,577 [nnabla][INFO]: iter=8629 {Training error}=0.0\n",
            "2023-06-04 15:40:34,614 [nnabla][INFO]: iter=8639 {Training loss}=1.207924213986189e-07\n",
            "2023-06-04 15:40:34,614 [nnabla][INFO]: iter=8639 {Training error}=0.0\n",
            "2023-06-04 15:40:34,656 [nnabla][INFO]: iter=8649 {Training loss}=1.2507649671533727e-07\n",
            "2023-06-04 15:40:34,656 [nnabla][INFO]: iter=8649 {Training error}=0.0\n",
            "2023-06-04 15:40:34,700 [nnabla][INFO]: iter=8659 {Training loss}=1.2014052686026844e-07\n",
            "2023-06-04 15:40:34,700 [nnabla][INFO]: iter=8659 {Training error}=0.0\n",
            "2023-06-04 15:40:34,741 [nnabla][INFO]: iter=8669 {Training loss}=1.2405205040977307e-07\n",
            "2023-06-04 15:40:34,741 [nnabla][INFO]: iter=8669 {Training error}=0.0\n",
            "2023-06-04 15:40:34,779 [nnabla][INFO]: iter=8679 {Training loss}=1.1604269190002015e-07\n",
            "2023-06-04 15:40:34,779 [nnabla][INFO]: iter=8679 {Training error}=0.0\n",
            "2023-06-04 15:40:34,819 [nnabla][INFO]: iter=8689 {Training loss}=1.1734654492556729e-07\n",
            "2023-06-04 15:40:34,819 [nnabla][INFO]: iter=8689 {Training error}=0.0\n",
            "2023-06-04 15:40:34,858 [nnabla][INFO]: iter=8699 {Training loss}=1.1753280659831944e-07\n",
            "2023-06-04 15:40:34,859 [nnabla][INFO]: iter=8699 {Training error}=0.0\n",
            "2023-06-04 15:40:34,860 [nnabla][INFO]: iter=8699 {Training time}=0.43502378463745117[sec/100iter] 38.076148986816406[sec]\n",
            "2023-06-04 15:40:34,884 [nnabla][INFO]: iter=8700 {Test error}=0.01484375\n",
            "2023-06-04 15:40:34,922 [nnabla][INFO]: iter=8709 {Training loss}=1.207924213986189e-07\n",
            "2023-06-04 15:40:34,922 [nnabla][INFO]: iter=8709 {Training error}=0.0\n",
            "2023-06-04 15:40:34,960 [nnabla][INFO]: iter=8719 {Training loss}=1.1324872417617371e-07\n",
            "2023-06-04 15:40:34,960 [nnabla][INFO]: iter=8719 {Training error}=0.0\n",
            "2023-06-04 15:40:35,006 [nnabla][INFO]: iter=8729 {Training loss}=1.1874351457663579e-07\n",
            "2023-06-04 15:40:35,007 [nnabla][INFO]: iter=8729 {Training error}=0.0\n",
            "2023-06-04 15:40:35,051 [nnabla][INFO]: iter=8739 {Training loss}=1.1771906827107159e-07\n",
            "2023-06-04 15:40:35,051 [nnabla][INFO]: iter=8739 {Training error}=0.0\n",
            "2023-06-04 15:40:35,090 [nnabla][INFO]: iter=8749 {Training loss}=1.1380751629985753e-07\n",
            "2023-06-04 15:40:35,090 [nnabla][INFO]: iter=8749 {Training error}=0.0\n",
            "2023-06-04 15:40:35,130 [nnabla][INFO]: iter=8759 {Training loss}=1.1129295529599403e-07\n",
            "2023-06-04 15:40:35,131 [nnabla][INFO]: iter=8759 {Training error}=0.0\n",
            "2023-06-04 15:40:35,169 [nnabla][INFO]: iter=8769 {Training loss}=1.162289535727723e-07\n",
            "2023-06-04 15:40:35,169 [nnabla][INFO]: iter=8769 {Training error}=0.0\n",
            "2023-06-04 15:40:35,208 [nnabla][INFO]: iter=8779 {Training loss}=1.101753710486264e-07\n",
            "2023-06-04 15:40:35,208 [nnabla][INFO]: iter=8779 {Training error}=0.0\n",
            "2023-06-04 15:40:35,246 [nnabla][INFO]: iter=8789 {Training loss}=1.1529763099815682e-07\n",
            "2023-06-04 15:40:35,247 [nnabla][INFO]: iter=8789 {Training error}=0.0\n",
            "2023-06-04 15:40:35,286 [nnabla][INFO]: iter=8799 {Training loss}=1.1492510054722516e-07\n",
            "2023-06-04 15:40:35,286 [nnabla][INFO]: iter=8799 {Training error}=0.0\n",
            "2023-06-04 15:40:35,287 [nnabla][INFO]: iter=8799 {Training time}=0.42694878578186035[sec/100iter] 38.50309777259827[sec]\n",
            "2023-06-04 15:40:35,313 [nnabla][INFO]: iter=8800 {Test error}=0.0140625\n",
            "2023-06-04 15:40:35,354 [nnabla][INFO]: iter=8809 {Training loss}=1.101753710486264e-07\n",
            "2023-06-04 15:40:35,354 [nnabla][INFO]: iter=8809 {Training error}=0.0\n",
            "2023-06-04 15:40:35,393 [nnabla][INFO]: iter=8819 {Training loss}=1.0738141043020732e-07\n",
            "2023-06-04 15:40:35,393 [nnabla][INFO]: iter=8819 {Training error}=0.0\n",
            "2023-06-04 15:40:35,433 [nnabla][INFO]: iter=8829 {Training loss}=1.1324872417617371e-07\n",
            "2023-06-04 15:40:35,433 [nnabla][INFO]: iter=8829 {Training error}=0.0\n",
            "2023-06-04 15:40:35,473 [nnabla][INFO]: iter=8839 {Training loss}=1.1613583694725094e-07\n",
            "2023-06-04 15:40:35,474 [nnabla][INFO]: iter=8839 {Training error}=0.0\n",
            "2023-06-04 15:40:35,519 [nnabla][INFO]: iter=8849 {Training loss}=1.1064103233593414e-07\n",
            "2023-06-04 15:40:35,519 [nnabla][INFO]: iter=8849 {Training error}=0.0\n",
            "2023-06-04 15:40:35,557 [nnabla][INFO]: iter=8859 {Training loss}=1.0682261120109615e-07\n",
            "2023-06-04 15:40:35,557 [nnabla][INFO]: iter=8859 {Training error}=0.0\n",
            "2023-06-04 15:40:35,597 [nnabla][INFO]: iter=8869 {Training loss}=1.1064103233593414e-07\n",
            "2023-06-04 15:40:35,597 [nnabla][INFO]: iter=8869 {Training error}=0.0\n",
            "2023-06-04 15:40:35,637 [nnabla][INFO]: iter=8879 {Training loss}=1.0468056643730961e-07\n",
            "2023-06-04 15:40:35,637 [nnabla][INFO]: iter=8879 {Training error}=0.0\n",
            "2023-06-04 15:40:35,677 [nnabla][INFO]: iter=8889 {Training loss}=1.0645008785559185e-07\n",
            "2023-06-04 15:40:35,677 [nnabla][INFO]: iter=8889 {Training error}=0.0\n",
            "2023-06-04 15:40:35,718 [nnabla][INFO]: iter=8899 {Training loss}=1.0551875817554901e-07\n",
            "2023-06-04 15:40:35,718 [nnabla][INFO]: iter=8899 {Training error}=0.0\n",
            "2023-06-04 15:40:35,718 [nnabla][INFO]: iter=8899 {Training time}=0.4318077564239502[sec/100iter] 38.93490552902222[sec]\n",
            "2023-06-04 15:40:35,743 [nnabla][INFO]: iter=8900 {Test error}=0.0140625\n",
            "2023-06-04 15:40:35,782 [nnabla][INFO]: iter=8909 {Training loss}=1.0812645712121594e-07\n",
            "2023-06-04 15:40:35,782 [nnabla][INFO]: iter=8909 {Training error}=0.0\n",
            "2023-06-04 15:40:35,827 [nnabla][INFO]: iter=8919 {Training loss}=1.0346985845899326e-07\n",
            "2023-06-04 15:40:35,828 [nnabla][INFO]: iter=8919 {Training error}=0.0\n",
            "2023-06-04 15:40:35,876 [nnabla][INFO]: iter=8929 {Training loss}=1.079402096593185e-07\n",
            "2023-06-04 15:40:35,876 [nnabla][INFO]: iter=8929 {Training error}=0.0\n",
            "2023-06-04 15:40:35,921 [nnabla][INFO]: iter=8939 {Training loss}=1.0784707171751506e-07\n",
            "2023-06-04 15:40:35,921 [nnabla][INFO]: iter=8939 {Training error}=0.0\n",
            "2023-06-04 15:40:35,962 [nnabla][INFO]: iter=8949 {Training loss}=9.788192301130039e-08\n",
            "2023-06-04 15:40:35,963 [nnabla][INFO]: iter=8949 {Training error}=0.0\n",
            "2023-06-04 15:40:36,007 [nnabla][INFO]: iter=8959 {Training loss}=1.0356298929536933e-07\n",
            "2023-06-04 15:40:36,007 [nnabla][INFO]: iter=8959 {Training error}=0.0\n",
            "2023-06-04 15:40:36,050 [nnabla][INFO]: iter=8969 {Training loss}=1.0263168093160857e-07\n",
            "2023-06-04 15:40:36,050 [nnabla][INFO]: iter=8969 {Training error}=0.0\n",
            "2023-06-04 15:40:36,095 [nnabla][INFO]: iter=8979 {Training loss}=1.03935519746301e-07\n",
            "2023-06-04 15:40:36,096 [nnabla][INFO]: iter=8979 {Training error}=0.0\n",
            "2023-06-04 15:40:36,139 [nnabla][INFO]: iter=8989 {Training loss}=9.862699101859107e-08\n",
            "2023-06-04 15:40:36,139 [nnabla][INFO]: iter=8989 {Training error}=0.0\n",
            "2023-06-04 15:40:36,178 [nnabla][INFO]: iter=8999 {Training loss}=9.769566844397559e-08\n",
            "2023-06-04 15:40:36,178 [nnabla][INFO]: iter=8999 {Training error}=0.0\n",
            "2023-06-04 15:40:36,180 [nnabla][INFO]: iter=8999 {Training time}=0.4612553119659424[sec/100iter] 39.39616084098816[sec]\n",
            "2023-06-04 15:40:36,205 [nnabla][INFO]: iter=9000 {Test error}=0.01484375\n",
            "2023-06-04 15:40:36,224 [nnabla][INFO]: Solver state save (.h5): output/states_9000.h5\n",
            "2023-06-04 15:40:36,234 [nnabla][INFO]: Parameter save (.h5): output/params_9000.h5\n",
            "2023-06-04 15:40:36,234 [nnabla][INFO]: Checkpoint save (.json): output/checkpoint_9000.json\n",
            "2023-06-04 15:40:36,277 [nnabla][INFO]: iter=9009 {Training loss}=1.0011710571689036e-07\n",
            "2023-06-04 15:40:36,277 [nnabla][INFO]: iter=9009 {Training error}=0.0\n",
            "2023-06-04 15:40:36,324 [nnabla][INFO]: iter=9019 {Training loss}=1.0104842829150584e-07\n",
            "2023-06-04 15:40:36,325 [nnabla][INFO]: iter=9019 {Training error}=0.0\n",
            "2023-06-04 15:40:36,366 [nnabla][INFO]: iter=9029 {Training loss}=9.322531724365035e-08\n",
            "2023-06-04 15:40:36,366 [nnabla][INFO]: iter=9029 {Training error}=0.0\n",
            "2023-06-04 15:40:36,406 [nnabla][INFO]: iter=9039 {Training loss}=9.573989245836856e-08\n",
            "2023-06-04 15:40:36,407 [nnabla][INFO]: iter=9039 {Training error}=0.0\n",
            "2023-06-04 15:40:36,448 [nnabla][INFO]: iter=9049 {Training loss}=9.294592473452212e-08\n",
            "2023-06-04 15:40:36,448 [nnabla][INFO]: iter=9049 {Training error}=0.0\n",
            "2023-06-04 15:40:36,488 [nnabla][INFO]: iter=9059 {Training loss}=1.0225913626982219e-07\n",
            "2023-06-04 15:40:36,489 [nnabla][INFO]: iter=9059 {Training error}=0.0\n",
            "2023-06-04 15:40:36,531 [nnabla][INFO]: iter=9069 {Training loss}=9.713687632029178e-08\n",
            "2023-06-04 15:40:36,531 [nnabla][INFO]: iter=9069 {Training error}=0.0\n",
            "2023-06-04 15:40:36,573 [nnabla][INFO]: iter=9079 {Training loss}=9.639180831300109e-08\n",
            "2023-06-04 15:40:36,573 [nnabla][INFO]: iter=9079 {Training error}=0.0\n",
            "2023-06-04 15:40:36,621 [nnabla][INFO]: iter=9089 {Training loss}=9.266652511996654e-08\n",
            "2023-06-04 15:40:36,622 [nnabla][INFO]: iter=9089 {Training error}=0.0\n",
            "2023-06-04 15:40:36,662 [nnabla][INFO]: iter=9099 {Training loss}=9.11764175270946e-08\n",
            "2023-06-04 15:40:36,663 [nnabla][INFO]: iter=9099 {Training error}=0.0\n",
            "2023-06-04 15:40:36,663 [nnabla][INFO]: iter=9099 {Training time}=0.48308515548706055[sec/100iter] 39.87924599647522[sec]\n",
            "2023-06-04 15:40:36,688 [nnabla][INFO]: iter=9100 {Test error}=0.0140625\n",
            "2023-06-04 15:40:36,728 [nnabla][INFO]: iter=9109 {Training loss}=9.769566844397559e-08\n",
            "2023-06-04 15:40:36,728 [nnabla][INFO]: iter=9109 {Training error}=0.0\n",
            "2023-06-04 15:40:36,768 [nnabla][INFO]: iter=9119 {Training loss}=8.875498735960718e-08\n",
            "2023-06-04 15:40:36,768 [nnabla][INFO]: iter=9119 {Training error}=0.0\n",
            "2023-06-04 15:40:36,813 [nnabla][INFO]: iter=9129 {Training loss}=9.657808419660796e-08\n",
            "2023-06-04 15:40:36,813 [nnabla][INFO]: iter=9129 {Training error}=0.0\n",
            "2023-06-04 15:40:36,854 [nnabla][INFO]: iter=9139 {Training loss}=9.257339428359046e-08\n",
            "2023-06-04 15:40:36,855 [nnabla][INFO]: iter=9139 {Training error}=0.0\n",
            "2023-06-04 15:40:36,897 [nnabla][INFO]: iter=9149 {Training loss}=8.977944077059874e-08\n",
            "2023-06-04 15:40:36,897 [nnabla][INFO]: iter=9149 {Training error}=0.0\n",
            "2023-06-04 15:40:36,941 [nnabla][INFO]: iter=9159 {Training loss}=8.922064864691492e-08\n",
            "2023-06-04 15:40:36,941 [nnabla][INFO]: iter=9159 {Training error}=0.0\n",
            "2023-06-04 15:40:36,982 [nnabla][INFO]: iter=9169 {Training loss}=9.089701791253901e-08\n",
            "2023-06-04 15:40:36,982 [nnabla][INFO]: iter=9169 {Training error}=0.0\n",
            "2023-06-04 15:40:37,030 [nnabla][INFO]: iter=9179 {Training loss}=8.605415047213683e-08\n",
            "2023-06-04 15:40:37,031 [nnabla][INFO]: iter=9179 {Training error}=0.0\n",
            "2023-06-04 15:40:37,073 [nnabla][INFO]: iter=9189 {Training loss}=9.192147132353057e-08\n",
            "2023-06-04 15:40:37,073 [nnabla][INFO]: iter=9189 {Training error}=0.0\n",
            "2023-06-04 15:40:37,121 [nnabla][INFO]: iter=9199 {Training loss}=9.117641042166724e-08\n",
            "2023-06-04 15:40:37,122 [nnabla][INFO]: iter=9199 {Training error}=0.0\n",
            "2023-06-04 15:40:37,122 [nnabla][INFO]: iter=9199 {Training time}=0.45923709869384766[sec/100iter] 40.33848309516907[sec]\n",
            "2023-06-04 15:40:37,147 [nnabla][INFO]: iter=9200 {Test error}=0.0140625\n",
            "2023-06-04 15:40:37,187 [nnabla][INFO]: iter=9209 {Training loss}=8.44708978320341e-08\n",
            "2023-06-04 15:40:37,187 [nnabla][INFO]: iter=9209 {Training error}=0.0\n",
            "2023-06-04 15:40:37,228 [nnabla][INFO]: iter=9219 {Training loss}=8.698546594132495e-08\n",
            "2023-06-04 15:40:37,229 [nnabla][INFO]: iter=9219 {Training error}=0.0\n",
            "2023-06-04 15:40:37,269 [nnabla][INFO]: iter=9229 {Training loss}=8.493655911934184e-08\n",
            "2023-06-04 15:40:37,269 [nnabla][INFO]: iter=9229 {Training error}=0.0\n",
            "2023-06-04 15:40:37,317 [nnabla][INFO]: iter=9239 {Training loss}=8.74511414394874e-08\n",
            "2023-06-04 15:40:37,317 [nnabla][INFO]: iter=9239 {Training error}=0.0\n",
            "2023-06-04 15:40:37,360 [nnabla][INFO]: iter=9249 {Training loss}=8.922064864691492e-08\n",
            "2023-06-04 15:40:37,361 [nnabla][INFO]: iter=9249 {Training error}=0.0\n",
            "2023-06-04 15:40:37,406 [nnabla][INFO]: iter=9259 {Training loss}=8.428462905385459e-08\n",
            "2023-06-04 15:40:37,406 [nnabla][INFO]: iter=9259 {Training error}=0.0\n",
            "2023-06-04 15:40:37,448 [nnabla][INFO]: iter=9269 {Training loss}=8.670607343219672e-08\n",
            "2023-06-04 15:40:37,448 [nnabla][INFO]: iter=9269 {Training error}=0.0\n",
            "2023-06-04 15:40:37,488 [nnabla][INFO]: iter=9279 {Training loss}=8.605414336670947e-08\n",
            "2023-06-04 15:40:37,489 [nnabla][INFO]: iter=9279 {Training error}=0.0\n",
            "2023-06-04 15:40:37,531 [nnabla][INFO]: iter=9289 {Training loss}=8.027996045711916e-08\n",
            "2023-06-04 15:40:37,531 [nnabla][INFO]: iter=9289 {Training error}=0.0\n",
            "2023-06-04 15:40:37,572 [nnabla][INFO]: iter=9299 {Training loss}=8.29807831337348e-08\n",
            "2023-06-04 15:40:37,572 [nnabla][INFO]: iter=9299 {Training error}=0.0\n",
            "2023-06-04 15:40:37,572 [nnabla][INFO]: iter=9299 {Training time}=0.4505496025085449[sec/100iter] 40.78903269767761[sec]\n",
            "2023-06-04 15:40:37,598 [nnabla][INFO]: iter=9300 {Test error}=0.0140625\n",
            "2023-06-04 15:40:37,640 [nnabla][INFO]: iter=9309 {Training loss}=8.596101963576075e-08\n",
            "2023-06-04 15:40:37,641 [nnabla][INFO]: iter=9309 {Training error}=0.0\n",
            "2023-06-04 15:40:37,686 [nnabla][INFO]: iter=9319 {Training loss}=7.813791569333262e-08\n",
            "2023-06-04 15:40:37,686 [nnabla][INFO]: iter=9319 {Training error}=0.0\n",
            "2023-06-04 15:40:37,732 [nnabla][INFO]: iter=9329 {Training loss}=8.456403577383753e-08\n",
            "2023-06-04 15:40:37,732 [nnabla][INFO]: iter=9329 {Training error}=0.0\n",
            "2023-06-04 15:40:37,773 [nnabla][INFO]: iter=9339 {Training loss}=8.307392107553824e-08\n",
            "2023-06-04 15:40:37,774 [nnabla][INFO]: iter=9339 {Training error}=0.0\n",
            "2023-06-04 15:40:37,815 [nnabla][INFO]: iter=9349 {Training loss}=7.823105363513605e-08\n",
            "2023-06-04 15:40:37,815 [nnabla][INFO]: iter=9349 {Training error}=0.0\n",
            "2023-06-04 15:40:37,856 [nnabla][INFO]: iter=9359 {Training loss}=8.633354298126505e-08\n",
            "2023-06-04 15:40:37,856 [nnabla][INFO]: iter=9359 {Training error}=0.0\n",
            "2023-06-04 15:40:37,897 [nnabla][INFO]: iter=9369 {Training loss}=7.506455546035795e-08\n",
            "2023-06-04 15:40:37,897 [nnabla][INFO]: iter=9369 {Training error}=0.0\n",
            "2023-06-04 15:40:37,949 [nnabla][INFO]: iter=9379 {Training loss}=8.065247669719611e-08\n",
            "2023-06-04 15:40:37,949 [nnabla][INFO]: iter=9379 {Training error}=0.0\n",
            "2023-06-04 15:40:37,990 [nnabla][INFO]: iter=9389 {Training loss}=7.823105363513605e-08\n",
            "2023-06-04 15:40:37,991 [nnabla][INFO]: iter=9389 {Training error}=0.0\n",
            "2023-06-04 15:40:38,035 [nnabla][INFO]: iter=9399 {Training loss}=8.083875258080297e-08\n",
            "2023-06-04 15:40:38,035 [nnabla][INFO]: iter=9399 {Training error}=0.0\n",
            "2023-06-04 15:40:38,035 [nnabla][INFO]: iter=9399 {Training time}=0.4625856876373291[sec/100iter] 41.25161838531494[sec]\n",
            "2023-06-04 15:40:38,061 [nnabla][INFO]: iter=9400 {Test error}=0.01484375\n",
            "2023-06-04 15:40:38,103 [nnabla][INFO]: iter=9409 {Training loss}=8.232886727910227e-08\n",
            "2023-06-04 15:40:38,103 [nnabla][INFO]: iter=9409 {Training error}=0.0\n",
            "2023-06-04 15:40:38,145 [nnabla][INFO]: iter=9419 {Training loss}=7.459888706762285e-08\n",
            "2023-06-04 15:40:38,147 [nnabla][INFO]: iter=9419 {Training error}=0.0\n",
            "2023-06-04 15:40:38,187 [nnabla][INFO]: iter=9429 {Training loss}=7.99074300061875e-08\n",
            "2023-06-04 15:40:38,187 [nnabla][INFO]: iter=9429 {Training error}=0.0\n",
            "2023-06-04 15:40:38,223 [nnabla][INFO]: iter=9439 {Training loss}=7.431949455849463e-08\n",
            "2023-06-04 15:40:38,223 [nnabla][INFO]: iter=9439 {Training error}=0.0\n",
            "2023-06-04 15:40:38,263 [nnabla][INFO]: iter=9449 {Training loss}=7.692720771501627e-08\n",
            "2023-06-04 15:40:38,264 [nnabla][INFO]: iter=9449 {Training error}=0.0\n",
            "2023-06-04 15:40:38,300 [nnabla][INFO]: iter=9459 {Training loss}=6.966288879084459e-08\n",
            "2023-06-04 15:40:38,300 [nnabla][INFO]: iter=9459 {Training error}=0.0\n",
            "2023-06-04 15:40:38,340 [nnabla][INFO]: iter=9469 {Training loss}=7.580960925679392e-08\n",
            "2023-06-04 15:40:38,340 [nnabla][INFO]: iter=9469 {Training error}=0.0\n",
            "2023-06-04 15:40:38,376 [nnabla][INFO]: iter=9479 {Training loss}=7.199118812195593e-08\n",
            "2023-06-04 15:40:38,376 [nnabla][INFO]: iter=9479 {Training error}=0.0\n",
            "2023-06-04 15:40:38,412 [nnabla][INFO]: iter=9489 {Training loss}=7.078047303821222e-08\n",
            "2023-06-04 15:40:38,412 [nnabla][INFO]: iter=9489 {Training error}=0.0\n",
            "2023-06-04 15:40:38,448 [nnabla][INFO]: iter=9499 {Training loss}=7.022168801995576e-08\n",
            "2023-06-04 15:40:38,448 [nnabla][INFO]: iter=9499 {Training error}=0.0\n",
            "2023-06-04 15:40:38,448 [nnabla][INFO]: iter=9499 {Training time}=0.4134557247161865[sec/100iter] 41.66507411003113[sec]\n",
            "2023-06-04 15:40:38,471 [nnabla][INFO]: iter=9500 {Test error}=0.0140625\n",
            "2023-06-04 15:40:38,508 [nnabla][INFO]: iter=9509 {Training loss}=7.385383327118689e-08\n",
            "2023-06-04 15:40:38,508 [nnabla][INFO]: iter=9509 {Training error}=0.0\n",
            "2023-06-04 15:40:38,548 [nnabla][INFO]: iter=9519 {Training loss}=7.133926516189604e-08\n",
            "2023-06-04 15:40:38,548 [nnabla][INFO]: iter=9519 {Training error}=0.0\n",
            "2023-06-04 15:40:38,583 [nnabla][INFO]: iter=9529 {Training loss}=6.98491575690241e-08\n",
            "2023-06-04 15:40:38,584 [nnabla][INFO]: iter=9529 {Training error}=0.0\n",
            "2023-06-04 15:40:38,619 [nnabla][INFO]: iter=9539 {Training loss}=7.059421847088743e-08\n",
            "2023-06-04 15:40:38,619 [nnabla][INFO]: iter=9539 {Training error}=0.0\n",
            "2023-06-04 15:40:38,654 [nnabla][INFO]: iter=9549 {Training loss}=6.929035123448557e-08\n",
            "2023-06-04 15:40:38,654 [nnabla][INFO]: iter=9549 {Training error}=0.0\n",
            "2023-06-04 15:40:38,689 [nnabla][INFO]: iter=9559 {Training loss}=7.273624902381925e-08\n",
            "2023-06-04 15:40:38,690 [nnabla][INFO]: iter=9559 {Training error}=0.0\n",
            "2023-06-04 15:40:38,726 [nnabla][INFO]: iter=9569 {Training loss}=6.956976505989587e-08\n",
            "2023-06-04 15:40:38,726 [nnabla][INFO]: iter=9569 {Training error}=0.0\n",
            "2023-06-04 15:40:38,763 [nnabla][INFO]: iter=9579 {Training loss}=6.845216660167353e-08\n",
            "2023-06-04 15:40:38,763 [nnabla][INFO]: iter=9579 {Training error}=0.0\n",
            "2023-06-04 15:40:38,802 [nnabla][INFO]: iter=9589 {Training loss}=6.649640482692121e-08\n",
            "2023-06-04 15:40:38,802 [nnabla][INFO]: iter=9589 {Training error}=0.0\n",
            "2023-06-04 15:40:38,838 [nnabla][INFO]: iter=9599 {Training loss}=7.254998024563974e-08\n",
            "2023-06-04 15:40:38,838 [nnabla][INFO]: iter=9599 {Training error}=0.0\n",
            "2023-06-04 15:40:38,838 [nnabla][INFO]: iter=9599 {Training time}=0.3896002769470215[sec/100iter] 42.05467438697815[sec]\n",
            "2023-06-04 15:40:38,859 [nnabla][INFO]: iter=9600 {Test error}=0.0140625\n",
            "2023-06-04 15:40:38,893 [nnabla][INFO]: iter=9609 {Training loss}=7.17117956128277e-08\n",
            "2023-06-04 15:40:38,893 [nnabla][INFO]: iter=9609 {Training error}=0.0\n",
            "2023-06-04 15:40:38,928 [nnabla][INFO]: iter=9619 {Training loss}=6.798651241979314e-08\n",
            "2023-06-04 15:40:38,929 [nnabla][INFO]: iter=9619 {Training error}=0.0\n",
            "2023-06-04 15:40:38,967 [nnabla][INFO]: iter=9629 {Training loss}=6.463375967769025e-08\n",
            "2023-06-04 15:40:38,967 [nnabla][INFO]: iter=9629 {Training error}=0.0\n",
            "2023-06-04 15:40:39,001 [nnabla][INFO]: iter=9639 {Training loss}=6.752084402705805e-08\n",
            "2023-06-04 15:40:39,001 [nnabla][INFO]: iter=9639 {Training error}=0.0\n",
            "2023-06-04 15:40:39,039 [nnabla][INFO]: iter=9649 {Training loss}=6.780024364161363e-08\n",
            "2023-06-04 15:40:39,040 [nnabla][INFO]: iter=9649 {Training error}=0.0\n",
            "2023-06-04 15:40:39,074 [nnabla][INFO]: iter=9659 {Training loss}=6.966289589627195e-08\n",
            "2023-06-04 15:40:39,075 [nnabla][INFO]: iter=9659 {Training error}=0.0\n",
            "2023-06-04 15:40:39,111 [nnabla][INFO]: iter=9669 {Training loss}=6.537882057955358e-08\n",
            "2023-06-04 15:40:39,111 [nnabla][INFO]: iter=9669 {Training error}=0.0\n",
            "2023-06-04 15:40:39,146 [nnabla][INFO]: iter=9679 {Training loss}=6.519255180137407e-08\n",
            "2023-06-04 15:40:39,146 [nnabla][INFO]: iter=9679 {Training error}=0.0\n",
            "2023-06-04 15:40:39,182 [nnabla][INFO]: iter=9689 {Training loss}=6.863844248528039e-08\n",
            "2023-06-04 15:40:39,182 [nnabla][INFO]: iter=9689 {Training error}=0.0\n",
            "2023-06-04 15:40:39,216 [nnabla][INFO]: iter=9699 {Training loss}=6.323677581576703e-08\n",
            "2023-06-04 15:40:39,216 [nnabla][INFO]: iter=9699 {Training error}=0.0\n",
            "2023-06-04 15:40:39,216 [nnabla][INFO]: iter=9699 {Training time}=0.3777496814727783[sec/100iter] 42.43242406845093[sec]\n",
            "2023-06-04 15:40:39,241 [nnabla][INFO]: iter=9700 {Test error}=0.01484375\n",
            "2023-06-04 15:40:39,275 [nnabla][INFO]: iter=9709 {Training loss}=6.658954276872464e-08\n",
            "2023-06-04 15:40:39,275 [nnabla][INFO]: iter=9709 {Training error}=0.0\n",
            "2023-06-04 15:40:39,310 [nnabla][INFO]: iter=9719 {Training loss}=6.677579733604944e-08\n",
            "2023-06-04 15:40:39,310 [nnabla][INFO]: iter=9719 {Training error}=0.0\n",
            "2023-06-04 15:40:39,344 [nnabla][INFO]: iter=9729 {Training loss}=6.118786899378392e-08\n",
            "2023-06-04 15:40:39,344 [nnabla][INFO]: iter=9729 {Training error}=0.0\n",
            "2023-06-04 15:40:39,378 [nnabla][INFO]: iter=9739 {Training loss}=6.593761270323739e-08\n",
            "2023-06-04 15:40:39,379 [nnabla][INFO]: iter=9739 {Training error}=0.0\n",
            "2023-06-04 15:40:39,416 [nnabla][INFO]: iter=9749 {Training loss}=6.426122212133123e-08\n",
            "2023-06-04 15:40:39,416 [nnabla][INFO]: iter=9749 {Training error}=0.0\n",
            "2023-06-04 15:40:39,454 [nnabla][INFO]: iter=9759 {Training loss}=6.211918446297204e-08\n",
            "2023-06-04 15:40:39,454 [nnabla][INFO]: iter=9759 {Training error}=0.0\n",
            "2023-06-04 15:40:39,489 [nnabla][INFO]: iter=9769 {Training loss}=6.286424536483537e-08\n",
            "2023-06-04 15:40:39,489 [nnabla][INFO]: iter=9769 {Training error}=0.0\n",
            "2023-06-04 15:40:39,524 [nnabla][INFO]: iter=9779 {Training loss}=6.565820598325445e-08\n",
            "2023-06-04 15:40:39,524 [nnabla][INFO]: iter=9779 {Training error}=0.0\n",
            "2023-06-04 15:40:39,558 [nnabla][INFO]: iter=9789 {Training loss}=6.500629012862191e-08\n",
            "2023-06-04 15:40:39,558 [nnabla][INFO]: iter=9789 {Training error}=0.0\n",
            "2023-06-04 15:40:39,593 [nnabla][INFO]: iter=9799 {Training loss}=6.118786188835657e-08\n",
            "2023-06-04 15:40:39,593 [nnabla][INFO]: iter=9799 {Training error}=0.0\n",
            "2023-06-04 15:40:39,593 [nnabla][INFO]: iter=9799 {Training time}=0.37721705436706543[sec/100iter] 42.80964112281799[sec]\n",
            "2023-06-04 15:40:39,615 [nnabla][INFO]: iter=9800 {Test error}=0.0140625\n",
            "2023-06-04 15:40:39,650 [nnabla][INFO]: iter=9809 {Training loss}=6.519255180137407e-08\n",
            "2023-06-04 15:40:39,650 [nnabla][INFO]: iter=9809 {Training error}=0.0\n",
            "2023-06-04 15:40:39,684 [nnabla][INFO]: iter=9819 {Training loss}=5.811450520809558e-08\n",
            "2023-06-04 15:40:39,684 [nnabla][INFO]: iter=9819 {Training error}=0.0\n",
            "2023-06-04 15:40:39,722 [nnabla][INFO]: iter=9829 {Training loss}=6.575134392505788e-08\n",
            "2023-06-04 15:40:39,723 [nnabla][INFO]: iter=9829 {Training error}=0.0\n",
            "2023-06-04 15:40:39,762 [nnabla][INFO]: iter=9839 {Training loss}=6.100160021560441e-08\n",
            "2023-06-04 15:40:39,762 [nnabla][INFO]: iter=9839 {Training error}=0.0\n",
            "2023-06-04 15:40:39,796 [nnabla][INFO]: iter=9849 {Training loss}=6.156038523386087e-08\n",
            "2023-06-04 15:40:39,797 [nnabla][INFO]: iter=9849 {Training error}=0.0\n",
            "2023-06-04 15:40:39,830 [nnabla][INFO]: iter=9859 {Training loss}=5.9138955066373455e-08\n",
            "2023-06-04 15:40:39,830 [nnabla][INFO]: iter=9859 {Training error}=0.0\n",
            "2023-06-04 15:40:39,864 [nnabla][INFO]: iter=9869 {Training loss}=6.128099983016e-08\n",
            "2023-06-04 15:40:39,864 [nnabla][INFO]: iter=9869 {Training error}=0.0\n",
            "2023-06-04 15:40:39,898 [nnabla][INFO]: iter=9879 {Training loss}=6.109473105198049e-08\n",
            "2023-06-04 15:40:39,898 [nnabla][INFO]: iter=9879 {Training error}=0.0\n",
            "2023-06-04 15:40:39,935 [nnabla][INFO]: iter=9889 {Training loss}=5.8207639597185334e-08\n",
            "2023-06-04 15:40:39,936 [nnabla][INFO]: iter=9889 {Training error}=0.0\n",
            "2023-06-04 15:40:39,974 [nnabla][INFO]: iter=9899 {Training loss}=5.923209300817689e-08\n",
            "2023-06-04 15:40:39,974 [nnabla][INFO]: iter=9899 {Training error}=0.0\n",
            "2023-06-04 15:40:39,974 [nnabla][INFO]: iter=9899 {Training time}=0.3811056613922119[sec/100iter] 43.190746784210205[sec]\n",
            "2023-06-04 15:40:39,999 [nnabla][INFO]: iter=9900 {Test error}=0.0140625\n",
            "2023-06-04 15:40:40,033 [nnabla][INFO]: iter=9909 {Training loss}=6.081533854285226e-08\n",
            "2023-06-04 15:40:40,033 [nnabla][INFO]: iter=9909 {Training error}=0.0\n",
            "2023-06-04 15:40:40,068 [nnabla][INFO]: iter=9919 {Training loss}=5.904582422999738e-08\n",
            "2023-06-04 15:40:40,068 [nnabla][INFO]: iter=9919 {Training error}=0.0\n",
            "2023-06-04 15:40:40,102 [nnabla][INFO]: iter=9929 {Training loss}=5.736945496437329e-08\n",
            "2023-06-04 15:40:40,102 [nnabla][INFO]: iter=9929 {Training error}=0.0\n",
            "2023-06-04 15:40:40,137 [nnabla][INFO]: iter=9939 {Training loss}=5.95114890700188e-08\n",
            "2023-06-04 15:40:40,137 [nnabla][INFO]: iter=9939 {Training error}=0.0\n",
            "2023-06-04 15:40:40,170 [nnabla][INFO]: iter=9949 {Training loss}=5.7369440753518575e-08\n",
            "2023-06-04 15:40:40,170 [nnabla][INFO]: iter=9949 {Training error}=0.0\n",
            "2023-06-04 15:40:40,204 [nnabla][INFO]: iter=9959 {Training loss}=5.634499444795438e-08\n",
            "2023-06-04 15:40:40,204 [nnabla][INFO]: iter=9959 {Training error}=0.0\n",
            "2023-06-04 15:40:40,243 [nnabla][INFO]: iter=9969 {Training loss}=5.71831826334801e-08\n",
            "2023-06-04 15:40:40,243 [nnabla][INFO]: iter=9969 {Training error}=0.0\n",
            "2023-06-04 15:40:40,281 [nnabla][INFO]: iter=9979 {Training loss}=5.3271627109552355e-08\n",
            "2023-06-04 15:40:40,281 [nnabla][INFO]: iter=9979 {Training error}=0.0\n",
            "2023-06-04 15:40:40,315 [nnabla][INFO]: iter=9989 {Training loss}=5.466861097147557e-08\n",
            "2023-06-04 15:40:40,315 [nnabla][INFO]: iter=9989 {Training error}=0.0\n",
            "2023-06-04 15:40:40,350 [nnabla][INFO]: iter=9999 {Training loss}=5.494801058603116e-08\n",
            "2023-06-04 15:40:40,350 [nnabla][INFO]: iter=9999 {Training error}=0.0\n",
            "2023-06-04 15:40:40,350 [nnabla][INFO]: iter=9999 {Training time}=0.3762238025665283[sec/100iter] 43.56697058677673[sec]\n",
            "2023-06-04 15:40:40,371 [nnabla][INFO]: iter=9999 {Test error}=0.0140625\n",
            "2023-06-04 15:40:40,378 [nnabla][INFO]: Parameter save (.h5): output/lenet_params_010000.h5\n",
            "2023-06-04 15:40:40,381 [nnabla][INFO]: Saving output/lenet_result.nnp as nnp\n",
            "2023-06-04 15:40:40,381 [nnabla][INFO]: Saving <_io.StringIO object at 0x7fe68bf35480> as prototxt\n",
            "2023-06-04 15:40:40,388 [nnabla][INFO]: Parameter save (.h5): <_io.BytesIO object at 0x7fe68b957560>\n",
            "2023-06-04 15:40:40,388 [nnabla][INFO]: Model file is saved as (.nnp): output/lenet_result.nnp\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "outputフォルダが作られていることを確認。"
      ],
      "metadata": {
        "id": "oaaEWUuI2mq7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!ls"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x6yXUm1t2rVN",
        "outputId": "1567e3ff-d7e8-4beb-a98c-e02f7c5f1d7a"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "args.py\t\t\t  converted_datasets  README.md\t\ttrain.csv\n",
            "_checkpoint_nnp_util.py   dcgan.py\t      requirements.txt\tvae.py\n",
            "classification_bnn.py\t  mnist_data.py       siamese.py\tvat.py\n",
            "classification_mydata.py  output\t      test.csv\n",
            "classification.py\t  __pycache__\t      tmp.monitor\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Cソースコードの出力先フォルダを作成。"
      ],
      "metadata": {
        "id": "iUUn93sd2suQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!mkdir ./output_csrc"
      ],
      "metadata": {
        "id": "93786eh_McKj"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "学習済みモデルファイル(.nnp)をCソースコードに変換。"
      ],
      "metadata": {
        "id": "8IlLx0RZ210H"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!nnabla_cli convert -O CSRC -b 1 ./output/lenet_result.nnp ./output_csrc"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-THulbzMMwhk",
        "outputId": "e71110b0-b411-4c41-9948-f8fad7d9a11e"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2023-06-04 15:41:10,990 [nnabla][INFO]: Initializing CPU extension...\n",
            "NNabla command line interface (Version:1.33.1, Build:230206062927)\n",
            "2023-06-04 15:41:11,498 [nnabla][WARNING]: The export file format is 'CSRC' or 'SAVED_MODEL' that argument '--export-format' will have to be set!!!\n",
            "Importing ./output/lenet_result.nnp\n",
            " Expanding Validation.\n",
            "Using network [Validation].\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "変換の結果、次の４つのファイルが出来ていることを確認（その他のファイルは使わない）。\n",
        "Validation_inference.c\n",
        "Validation_inference.h\n",
        "Validation_parameters.c\n",
        "Validation_parameters.h"
      ],
      "metadata": {
        "id": "cJXpB1Ip3JqI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!ls -all output_csrc"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-DlV0KqDM1Y3",
        "outputId": "124eedba-1cec-4e85-8b69-71a655800b0e"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "total 568\n",
            "drwxr-xr-x 2 root root   4096 Jun  4 15:41 .\n",
            "drwxr-xr-x 7 root root   4096 Jun  4 15:40 ..\n",
            "-rw-r--r-- 1 root root    902 Jun  4 15:41 GNUmakefile\n",
            "-rw-r--r-- 1 root root   2124 Jun  4 15:41 Validation_example.c\n",
            "-rw-r--r-- 1 root root  23761 Jun  4 15:41 Validation_inference.c\n",
            "-rw-r--r-- 1 root root   2417 Jun  4 15:41 Validation_inference.h\n",
            "-rw-r--r-- 1 root root 529730 Jun  4 15:41 Validation_parameters.c\n",
            "-rw-r--r-- 1 root root    983 Jun  4 15:41 Validation_parameters.h\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "変換後のファイルをPCにダウンロードするために、Google Driveに一旦コピーする。"
      ],
      "metadata": {
        "id": "dFab70nk3Ovz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qgY1Ml1SHEp8",
        "outputId": "79160907-8d4c-4653-9512-9774eb847272"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!cp -r output_csrc /content/drive/MyDrive/"
      ],
      "metadata": {
        "id": "XDGf0RyDM-Fo"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}