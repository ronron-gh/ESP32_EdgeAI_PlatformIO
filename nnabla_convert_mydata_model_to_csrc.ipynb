{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMJKkBrP046V+XTw7zCiXZQ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ronron-gh/ESP32_EdgeAI/blob/main/nnabla_convert_mydata_model_to_csrc.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "nnablaをインストールする。"
      ],
      "metadata": {
        "id": "rXJs-ZgSD3lK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install nnabla-ext-cuda114"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ef69qbZjGiG5",
        "outputId": "f1e0f469-4915-4cbd-bf1d-6c0225b692cd"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting nnabla-ext-cuda114\n",
            "  Downloading nnabla_ext_cuda114-1.33.1-cp38-cp38-manylinux_2_17_x86_64.whl (121.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m121.2/121.2 MB\u001b[0m \u001b[31m6.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nnabla==1.33.1\n",
            "  Downloading nnabla-1.33.1-cp38-cp38-manylinux_2_17_x86_64.whl (167.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m167.2/167.2 MB\u001b[0m \u001b[31m7.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: setuptools in /usr/local/lib/python3.8/dist-packages (from nnabla-ext-cuda114) (57.4.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.8/dist-packages (from nnabla==1.33.1->nnabla-ext-cuda114) (4.64.1)\n",
            "Collecting ply\n",
            "  Downloading ply-3.11-py2.py3-none-any.whl (49 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.6/49.6 KB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pyyaml in /usr/local/lib/python3.8/dist-packages (from nnabla==1.33.1->nnabla-ext-cuda114) (6.0)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.8/dist-packages (from nnabla==1.33.1->nnabla-ext-cuda114) (1.7.3)\n",
            "Requirement already satisfied: imageio in /usr/local/lib/python3.8/dist-packages (from nnabla==1.33.1->nnabla-ext-cuda114) (2.9.0)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.8/dist-packages (from nnabla==1.33.1->nnabla-ext-cuda114) (3.1.0)\n",
            "Collecting protobuf<=3.19.4\n",
            "  Downloading protobuf-3.19.4-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m28.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pillow in /usr/local/lib/python3.8/dist-packages (from nnabla==1.33.1->nnabla-ext-cuda114) (7.1.2)\n",
            "Requirement already satisfied: contextlib2 in /usr/local/lib/python3.8/dist-packages (from nnabla==1.33.1->nnabla-ext-cuda114) (0.5.5)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.8/dist-packages (from nnabla==1.33.1->nnabla-ext-cuda114) (1.15.0)\n",
            "Requirement already satisfied: Cython in /usr/local/lib/python3.8/dist-packages (from nnabla==1.33.1->nnabla-ext-cuda114) (0.29.33)\n",
            "Collecting numpy~=1.23.0\n",
            "  Downloading numpy-1.23.5-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (17.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m17.1/17.1 MB\u001b[0m \u001b[31m62.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting configparser\n",
            "  Downloading configparser-5.3.0-py3-none-any.whl (19 kB)\n",
            "Collecting boto3\n",
            "  Downloading boto3-1.26.69-py3-none-any.whl (132 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m132.7/132.7 KB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting botocore<1.30.0,>=1.29.69\n",
            "  Downloading botocore-1.29.69-py3-none-any.whl (10.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.4/10.4 MB\u001b[0m \u001b[31m51.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting jmespath<2.0.0,>=0.7.1\n",
            "  Downloading jmespath-1.0.1-py3-none-any.whl (20 kB)\n",
            "Collecting s3transfer<0.7.0,>=0.6.0\n",
            "  Downloading s3transfer-0.6.0-py3-none-any.whl (79 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m79.6/79.6 KB\u001b[0m \u001b[31m8.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting scipy\n",
            "  Downloading scipy-1.10.0-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (34.5 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m34.5/34.5 MB\u001b[0m \u001b[31m16.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting urllib3<1.27,>=1.25.4\n",
            "  Downloading urllib3-1.26.14-py2.py3-none-any.whl (140 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m140.6/140.6 KB\u001b[0m \u001b[31m8.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: python-dateutil<3.0.0,>=2.1 in /usr/local/lib/python3.8/dist-packages (from botocore<1.30.0,>=1.29.69->boto3->nnabla==1.33.1->nnabla-ext-cuda114) (2.8.2)\n",
            "Installing collected packages: ply, urllib3, protobuf, numpy, jmespath, configparser, scipy, botocore, s3transfer, boto3, nnabla, nnabla-ext-cuda114\n",
            "  Attempting uninstall: urllib3\n",
            "    Found existing installation: urllib3 1.24.3\n",
            "    Uninstalling urllib3-1.24.3:\n",
            "      Successfully uninstalled urllib3-1.24.3\n",
            "  Attempting uninstall: protobuf\n",
            "    Found existing installation: protobuf 3.19.6\n",
            "    Uninstalling protobuf-3.19.6:\n",
            "      Successfully uninstalled protobuf-3.19.6\n",
            "  Attempting uninstall: numpy\n",
            "    Found existing installation: numpy 1.21.6\n",
            "    Uninstalling numpy-1.21.6:\n",
            "      Successfully uninstalled numpy-1.21.6\n",
            "  Attempting uninstall: scipy\n",
            "    Found existing installation: scipy 1.7.3\n",
            "    Uninstalling scipy-1.7.3:\n",
            "      Successfully uninstalled scipy-1.7.3\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "googleapis-common-protos 1.58.0 requires protobuf!=3.20.0,!=3.20.1,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5, but you have protobuf 3.19.4 which is incompatible.\n",
            "google-cloud-translate 3.8.4 requires protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5, but you have protobuf 3.19.4 which is incompatible.\n",
            "google-cloud-language 2.6.1 requires protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5, but you have protobuf 3.19.4 which is incompatible.\n",
            "google-cloud-firestore 2.7.3 requires protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5, but you have protobuf 3.19.4 which is incompatible.\n",
            "google-cloud-datastore 2.11.1 requires protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5, but you have protobuf 3.19.4 which is incompatible.\n",
            "google-cloud-bigquery 3.4.2 requires protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5, but you have protobuf 3.19.4 which is incompatible.\n",
            "google-cloud-bigquery-storage 2.18.1 requires protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5, but you have protobuf 3.19.4 which is incompatible.\n",
            "google-api-core 2.11.0 requires protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5, but you have protobuf 3.19.4 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed boto3-1.26.69 botocore-1.29.69 configparser-5.3.0 jmespath-1.0.1 nnabla-1.33.1 nnabla-ext-cuda114-1.33.1 numpy-1.23.5 ply-3.11 protobuf-3.19.4 s3transfer-0.6.0 scipy-1.10.0 urllib3-1.26.14\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "numpyを1.23.0に下げて、pillowを9.1.0に上げる（そうしないと、この後のclassification.pyの実行でエラーになった。数カ月前まではnumpyの変更だけで動いていたので、今後もこのようなバージョンの不整合は発生するかもしれない）。"
      ],
      "metadata": {
        "id": "E45JSZ8pECXk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install numpy==1.23.0"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 292
        },
        "id": "GFw9wEI5G1-8",
        "outputId": "10127ec1-20fe-4040-d1c5-a415f919982d"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting numpy==1.23.0\n",
            "  Downloading numpy-1.23.0-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (17.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m17.1/17.1 MB\u001b[0m \u001b[31m47.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: numpy\n",
            "  Attempting uninstall: numpy\n",
            "    Found existing installation: numpy 1.23.5\n",
            "    Uninstalling numpy-1.23.5:\n",
            "      Successfully uninstalled numpy-1.23.5\n",
            "Successfully installed numpy-1.23.0\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "numpy"
                ]
              }
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pillow==9.1.0"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 292
        },
        "id": "cstzfbIsEGOO",
        "outputId": "618c3a16-1e2c-4134-b2de-5660c789d710"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting pillow==9.1.0\n",
            "  Downloading Pillow-9.1.0-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.3/4.3 MB\u001b[0m \u001b[31m38.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: pillow\n",
            "  Attempting uninstall: pillow\n",
            "    Found existing installation: Pillow 7.1.2\n",
            "    Uninstalling Pillow-7.1.2:\n",
            "      Successfully uninstalled Pillow-7.1.2\n",
            "Successfully installed pillow-9.1.0\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "PIL"
                ]
              }
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "nnablaのサンプル群をGitHubから取得。"
      ],
      "metadata": {
        "id": "nCYTRdT5D6Aj"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yBnv_Ye6GTYm",
        "outputId": "c1f79adf-4630-42d1-883b-12de4f2ac676"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'nnabla-examples'...\n",
            "remote: Enumerating objects: 9292, done.\u001b[K\n",
            "remote: Counting objects: 100% (1850/1850), done.\u001b[K\n",
            "remote: Compressing objects: 100% (722/722), done.\u001b[K\n",
            "remote: Total 9292 (delta 1090), reused 1769 (delta 1077), pack-reused 7442\u001b[K\n",
            "Receiving objects: 100% (9292/9292), 296.48 MiB | 35.13 MiB/s, done.\n",
            "Resolving deltas: 100% (4992/4992), done.\n",
            "Updating files: 100% (1676/1676), done.\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/sony/nnabla-examples.git"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "本ブログで公開しているデータセットやプログラムをGitHubから取得。"
      ],
      "metadata": {
        "id": "UgWFw-HwT2o0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/ronron-gh/ESP32_EdgeAI.git"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ecF5PgfsUl0b",
        "outputId": "78dd031a-36ff-4a44-b2bf-7d1cbb7e1bbf"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'ESP32_EdgeAI'...\n",
            "remote: Enumerating objects: 591, done.\u001b[K\n",
            "remote: Counting objects:   0% (1/591)\u001b[K\rremote: Counting objects:   1% (6/591)\u001b[K\rremote: Counting objects:   2% (12/591)\u001b[K\rremote: Counting objects:   3% (18/591)\u001b[K\rremote: Counting objects:   4% (24/591)\u001b[K\rremote: Counting objects:   5% (30/591)\u001b[K\rremote: Counting objects:   6% (36/591)\u001b[K\rremote: Counting objects:   7% (42/591)\u001b[K\rremote: Counting objects:   8% (48/591)\u001b[K\rremote: Counting objects:   9% (54/591)\u001b[K\rremote: Counting objects:  10% (60/591)\u001b[K\rremote: Counting objects:  11% (66/591)\u001b[K\rremote: Counting objects:  12% (71/591)\u001b[K\rremote: Counting objects:  13% (77/591)\u001b[K\rremote: Counting objects:  14% (83/591)\u001b[K\rremote: Counting objects:  15% (89/591)\u001b[K\rremote: Counting objects:  16% (95/591)\u001b[K\rremote: Counting objects:  17% (101/591)\u001b[K\rremote: Counting objects:  18% (107/591)\u001b[K\rremote: Counting objects:  19% (113/591)\u001b[K\rremote: Counting objects:  20% (119/591)\u001b[K\rremote: Counting objects:  21% (125/591)\u001b[K\rremote: Counting objects:  22% (131/591)\u001b[K\rremote: Counting objects:  23% (136/591)\u001b[K\rremote: Counting objects:  24% (142/591)\u001b[K\rremote: Counting objects:  25% (148/591)\u001b[K\rremote: Counting objects:  26% (154/591)\u001b[K\rremote: Counting objects:  27% (160/591)\u001b[K\rremote: Counting objects:  28% (166/591)\u001b[K\rremote: Counting objects:  29% (172/591)\u001b[K\rremote: Counting objects:  30% (178/591)\u001b[K\rremote: Counting objects:  31% (184/591)\u001b[K\rremote: Counting objects:  32% (190/591)\u001b[K\rremote: Counting objects:  33% (196/591)\u001b[K\rremote: Counting objects:  34% (201/591)\u001b[K\rremote: Counting objects:  35% (207/591)\u001b[K\rremote: Counting objects:  36% (213/591)\u001b[K\rremote: Counting objects:  37% (219/591)\u001b[K\rremote: Counting objects:  38% (225/591)\u001b[K\rremote: Counting objects:  39% (231/591)\u001b[K\rremote: Counting objects:  40% (237/591)\u001b[K\rremote: Counting objects:  41% (243/591)\u001b[K\rremote: Counting objects:  42% (249/591)\u001b[K\rremote: Counting objects:  43% (255/591)\u001b[K\rremote: Counting objects:  44% (261/591)\u001b[K\rremote: Counting objects:  45% (266/591)\u001b[K\rremote: Counting objects:  46% (272/591)\u001b[K\rremote: Counting objects:  47% (278/591)\u001b[K\rremote: Counting objects:  48% (284/591)\u001b[K\rremote: Counting objects:  49% (290/591)\u001b[K\rremote: Counting objects:  50% (296/591)\u001b[K\rremote: Counting objects:  51% (302/591)\u001b[K\rremote: Counting objects:  52% (308/591)\u001b[K\rremote: Counting objects:  53% (314/591)\u001b[K\rremote: Counting objects:  54% (320/591)\u001b[K\rremote: Counting objects:  55% (326/591)\u001b[K\rremote: Counting objects:  56% (331/591)\u001b[K\rremote: Counting objects:  57% (337/591)\u001b[K\rremote: Counting objects:  58% (343/591)\u001b[K\rremote: Counting objects:  59% (349/591)\u001b[K\rremote: Counting objects:  60% (355/591)\u001b[K\rremote: Counting objects:  61% (361/591)\u001b[K\rremote: Counting objects:  62% (367/591)\u001b[K\rremote: Counting objects:  63% (373/591)\u001b[K\rremote: Counting objects:  64% (379/591)\u001b[K\rremote: Counting objects:  65% (385/591)\u001b[K\rremote: Counting objects:  66% (391/591)\u001b[K\rremote: Counting objects:  67% (396/591)\u001b[K\rremote: Counting objects:  68% (402/591)\u001b[K\rremote: Counting objects:  69% (408/591)\u001b[K\rremote: Counting objects:  70% (414/591)\u001b[K\rremote: Counting objects:  71% (420/591)\u001b[K\rremote: Counting objects:  72% (426/591)\u001b[K\rremote: Counting objects:  73% (432/591)\u001b[K\rremote: Counting objects:  74% (438/591)\u001b[K\rremote: Counting objects:  75% (444/591)\u001b[K\rremote: Counting objects:  76% (450/591)\u001b[K\rremote: Counting objects:  77% (456/591)\u001b[K\rremote: Counting objects:  78% (461/591)\u001b[K\rremote: Counting objects:  79% (467/591)\u001b[K\rremote: Counting objects:  80% (473/591)\u001b[K\rremote: Counting objects:  81% (479/591)\u001b[K\rremote: Counting objects:  82% (485/591)\u001b[K\rremote: Counting objects:  83% (491/591)\u001b[K\rremote: Counting objects:  84% (497/591)\u001b[K\rremote: Counting objects:  85% (503/591)\u001b[K\rremote: Counting objects:  86% (509/591)\u001b[K\rremote: Counting objects:  87% (515/591)\u001b[K\rremote: Counting objects:  88% (521/591)\u001b[K\rremote: Counting objects:  89% (526/591)\u001b[K\rremote: Counting objects:  90% (532/591)\u001b[K\rremote: Counting objects:  91% (538/591)\u001b[K\rremote: Counting objects:  92% (544/591)\u001b[K\rremote: Counting objects:  93% (550/591)\u001b[K\rremote: Counting objects:  94% (556/591)\u001b[K\rremote: Counting objects:  95% (562/591)\u001b[K\rremote: Counting objects:  96% (568/591)\u001b[K\rremote: Counting objects:  97% (574/591)\u001b[K\rremote: Counting objects:  98% (580/591)\u001b[K\rremote: Counting objects:  99% (586/591)\u001b[K\rremote: Counting objects: 100% (591/591)\u001b[K\rremote: Counting objects: 100% (591/591), done.\u001b[K\n",
            "remote: Compressing objects: 100% (514/514), done.\u001b[K\n",
            "remote: Total 591 (delta 81), reused 575 (delta 71), pack-reused 0\u001b[K\n",
            "Receiving objects: 100% (591/591), 4.80 MiB | 38.38 MiB/s, done.\n",
            "Resolving deltas: 100% (81/81), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd ESP32_EdgeAI"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L-iBemakaxJw",
        "outputId": "121bb584-cc00-4c9a-bd8e-9bf5bfb8aa74"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/ESP32_EdgeAI\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "データセットをnnablaに読み込ませるためのリストファイル(train.csv, test.csv)を作成する。画像は28x28のモノクロに変換する。（CSVの作成と画像の変換を行うPythonスクリプトを用意しました。）"
      ],
      "metadata": {
        "id": "fq9sNf-7a3qC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!python make_dataset_csv.py my_dataset"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8we8BQekb6gN",
        "outputId": "0a050bf2-a731-4996-bb9b-54d18acd7bea"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The current folder is my_dataset\n",
            "The current folder is my_dataset/2\n",
            "listed file : my_dataset/2/0110.jpg\n",
            "listed file : my_dataset/2/0087.jpg\n",
            "listed file : my_dataset/2/0103.jpg\n",
            "listed file : my_dataset/2/0085.jpg\n",
            "listed file : my_dataset/2/0113.jpg\n",
            "listed file : my_dataset/2/0095.jpg\n",
            "listed file : my_dataset/2/0082.jpg\n",
            "listed file : my_dataset/2/0123.jpg\n",
            "listed file : my_dataset/2/0107.jpg\n",
            "listed file : my_dataset/2/0096.jpg\n",
            "listed file : my_dataset/2/0108.jpg\n",
            "listed file : my_dataset/2/0097.jpg\n",
            "listed file : my_dataset/2/0125.jpg\n",
            "listed file : my_dataset/2/0116.jpg\n",
            "listed file : my_dataset/2/0086.jpg\n",
            "listed file : my_dataset/2/0128.jpg\n",
            "listed file : my_dataset/2/0081.jpg\n",
            "listed file : my_dataset/2/0126.jpg\n",
            "listed file : my_dataset/2/0102.jpg\n",
            "listed file : my_dataset/2/0119.jpg\n",
            "listed file : my_dataset/2/0197.jpg\n",
            "listed file : my_dataset/2/0100.jpg\n",
            "listed file : my_dataset/2/0114.jpg\n",
            "listed file : my_dataset/2/0088.jpg\n",
            "listed file : my_dataset/2/0112.jpg\n",
            "listed file : my_dataset/2/0195.jpg\n",
            "listed file : my_dataset/2/0196.jpg\n",
            "listed file : my_dataset/2/0129.jpg\n",
            "listed file : my_dataset/2/0200.jpg\n",
            "listed file : my_dataset/2/0115.jpg\n",
            "listed file : my_dataset/2/0130.jpg\n",
            "listed file : my_dataset/2/0091.jpg\n",
            "listed file : my_dataset/2/0101.jpg\n",
            "listed file : my_dataset/2/0083.jpg\n",
            "listed file : my_dataset/2/0124.jpg\n",
            "listed file : my_dataset/2/0093.jpg\n",
            "listed file : my_dataset/2/0202.jpg\n",
            "listed file : my_dataset/2/0121.jpg\n",
            "listed file : my_dataset/2/0106.jpg\n",
            "listed file : my_dataset/2/0080.jpg\n",
            "listed file : my_dataset/2/0127.jpg\n",
            "listed file : my_dataset/2/0117.jpg\n",
            "listed file : my_dataset/2/0120.jpg\n",
            "listed file : my_dataset/2/0118.jpg\n",
            "listed file : my_dataset/2/0198.jpg\n",
            "listed file : my_dataset/2/0201.jpg\n",
            "listed file : my_dataset/2/0109.jpg\n",
            "listed file : my_dataset/2/0094.jpg\n",
            "listed file : my_dataset/2/0105.jpg\n",
            "listed file : my_dataset/2/0079.jpg\n",
            "listed file : my_dataset/2/0111.jpg\n",
            "listed file : my_dataset/2/0104.jpg\n",
            "listed file : my_dataset/2/0199.jpg\n",
            "listed file : my_dataset/2/0122.jpg\n",
            "listed file : my_dataset/2/0098.jpg\n",
            "listed file : my_dataset/2/0092.jpg\n",
            "listed file : my_dataset/2/0194.jpg\n",
            "listed file : my_dataset/2/0090.jpg\n",
            "listed file : my_dataset/2/0099.jpg\n",
            "listed file : my_dataset/2/0089.jpg\n",
            "listed file : my_dataset/2/0084.jpg\n",
            "The current folder is my_dataset/1\n",
            "listed file : my_dataset/1/0165.jpg\n",
            "listed file : my_dataset/1/0171.jpg\n",
            "listed file : my_dataset/1/0139.jpg\n",
            "listed file : my_dataset/1/0151.jpg\n",
            "listed file : my_dataset/1/0154.jpg\n",
            "listed file : my_dataset/1/0162.jpg\n",
            "listed file : my_dataset/1/0142.jpg\n",
            "listed file : my_dataset/1/0172.jpg\n",
            "listed file : my_dataset/1/0149.jpg\n",
            "listed file : my_dataset/1/0146.jpg\n",
            "listed file : my_dataset/1/0174.jpg\n",
            "listed file : my_dataset/1/0148.jpg\n",
            "listed file : my_dataset/1/0131.jpg\n",
            "listed file : my_dataset/1/0159.jpg\n",
            "listed file : my_dataset/1/0168.jpg\n",
            "listed file : my_dataset/1/0140.jpg\n",
            "listed file : my_dataset/1/0156.jpg\n",
            "listed file : my_dataset/1/0173.jpg\n",
            "listed file : my_dataset/1/0158.jpg\n",
            "listed file : my_dataset/1/0144.jpg\n",
            "listed file : my_dataset/1/0134.jpg\n",
            "listed file : my_dataset/1/0166.jpg\n",
            "listed file : my_dataset/1/0169.jpg\n",
            "listed file : my_dataset/1/0152.jpg\n",
            "listed file : my_dataset/1/0150.jpg\n",
            "listed file : my_dataset/1/0147.jpg\n",
            "listed file : my_dataset/1/0137.jpg\n",
            "listed file : my_dataset/1/0141.jpg\n",
            "listed file : my_dataset/1/0143.jpg\n",
            "listed file : my_dataset/1/0133.jpg\n",
            "listed file : my_dataset/1/0132.jpg\n",
            "listed file : my_dataset/1/0170.jpg\n",
            "listed file : my_dataset/1/0176.jpg\n",
            "listed file : my_dataset/1/0135.jpg\n",
            "listed file : my_dataset/1/0160.jpg\n",
            "listed file : my_dataset/1/0175.jpg\n",
            "listed file : my_dataset/1/0153.jpg\n",
            "listed file : my_dataset/1/0136.jpg\n",
            "listed file : my_dataset/1/0167.jpg\n",
            "listed file : my_dataset/1/0138.jpg\n",
            "listed file : my_dataset/1/0155.jpg\n",
            "listed file : my_dataset/1/0161.jpg\n",
            "listed file : my_dataset/1/0164.jpg\n",
            "listed file : my_dataset/1/0163.jpg\n",
            "listed file : my_dataset/1/0157.jpg\n",
            "listed file : my_dataset/1/0145.jpg\n",
            "The current folder is my_dataset/3\n",
            "listed file : my_dataset/3/0181.jpg\n",
            "listed file : my_dataset/3/0046.jpg\n",
            "listed file : my_dataset/3/0182.jpg\n",
            "listed file : my_dataset/3/0021.jpg\n",
            "listed file : my_dataset/3/0012.jpg\n",
            "listed file : my_dataset/3/0038.jpg\n",
            "listed file : my_dataset/3/0185.jpg\n",
            "listed file : my_dataset/3/0025.jpg\n",
            "listed file : my_dataset/3/0031.jpg\n",
            "listed file : my_dataset/3/0003.jpg\n",
            "listed file : my_dataset/3/0042.jpg\n",
            "listed file : my_dataset/3/0036.jpg\n",
            "listed file : my_dataset/3/0033.jpg\n",
            "listed file : my_dataset/3/0188.jpg\n",
            "listed file : my_dataset/3/0190.jpg\n",
            "listed file : my_dataset/3/0024.jpg\n",
            "listed file : my_dataset/3/0183.jpg\n",
            "listed file : my_dataset/3/0030.jpg\n",
            "listed file : my_dataset/3/0026.jpg\n",
            "listed file : my_dataset/3/0016.jpg\n",
            "listed file : my_dataset/3/0186.jpg\n",
            "listed file : my_dataset/3/0019.jpg\n",
            "listed file : my_dataset/3/0035.jpg\n",
            "listed file : my_dataset/3/0041.jpg\n",
            "listed file : my_dataset/3/0047.jpg\n",
            "listed file : my_dataset/3/0189.jpg\n",
            "listed file : my_dataset/3/0187.jpg\n",
            "listed file : my_dataset/3/0022.jpg\n",
            "listed file : my_dataset/3/0040.jpg\n",
            "listed file : my_dataset/3/0191.jpg\n",
            "listed file : my_dataset/3/0028.jpg\n",
            "listed file : my_dataset/3/0034.jpg\n",
            "listed file : my_dataset/3/0178.jpg\n",
            "listed file : my_dataset/3/0023.jpg\n",
            "listed file : my_dataset/3/0017.jpg\n",
            "listed file : my_dataset/3/0015.jpg\n",
            "listed file : my_dataset/3/0180.jpg\n",
            "listed file : my_dataset/3/0037.jpg\n",
            "listed file : my_dataset/3/0018.jpg\n",
            "listed file : my_dataset/3/0179.jpg\n",
            "listed file : my_dataset/3/0020.jpg\n",
            "listed file : my_dataset/3/0045.jpg\n",
            "listed file : my_dataset/3/0039.jpg\n",
            "listed file : my_dataset/3/0013.jpg\n",
            "listed file : my_dataset/3/0192.jpg\n",
            "listed file : my_dataset/3/0184.jpg\n",
            "listed file : my_dataset/3/0044.jpg\n",
            "listed file : my_dataset/3/0014.jpg\n",
            "listed file : my_dataset/3/0048.jpg\n",
            "listed file : my_dataset/3/0043.jpg\n",
            "listed file : my_dataset/3/0032.jpg\n",
            "listed file : my_dataset/3/0029.jpg\n",
            "listed file : my_dataset/3/0027.jpg\n",
            "The current folder is my_dataset/0\n",
            "listed file : my_dataset/0/0011.jpg\n",
            "listed file : my_dataset/0/0205.jpg\n",
            "listed file : my_dataset/0/0053.jpg\n",
            "listed file : my_dataset/0/0212.jpg\n",
            "listed file : my_dataset/0/0072.jpg\n",
            "listed file : my_dataset/0/0005.jpg\n",
            "listed file : my_dataset/0/0215.jpg\n",
            "listed file : my_dataset/0/0070.jpg\n",
            "listed file : my_dataset/0/0220.jpg\n",
            "listed file : my_dataset/0/0207.jpg\n",
            "listed file : my_dataset/0/0214.jpg\n",
            "listed file : my_dataset/0/0209.jpg\n",
            "listed file : my_dataset/0/0229.jpg\n",
            "listed file : my_dataset/0/0228.jpg\n",
            "listed file : my_dataset/0/0226.jpg\n",
            "listed file : my_dataset/0/0204.jpg\n",
            "listed file : my_dataset/0/0213.jpg\n",
            "listed file : my_dataset/0/0009.jpg\n",
            "listed file : my_dataset/0/0218.jpg\n",
            "listed file : my_dataset/0/0206.jpg\n",
            "listed file : my_dataset/0/0232.jpg\n",
            "listed file : my_dataset/0/0076.jpg\n",
            "listed file : my_dataset/0/0061.jpg\n",
            "listed file : my_dataset/0/0225.jpg\n",
            "listed file : my_dataset/0/0004.jpg\n",
            "listed file : my_dataset/0/0073.jpg\n",
            "listed file : my_dataset/0/0230.jpg\n",
            "listed file : my_dataset/0/0224.jpg\n",
            "listed file : my_dataset/0/0221.jpg\n",
            "listed file : my_dataset/0/0208.jpg\n",
            "listed file : my_dataset/0/0203.jpg\n",
            "listed file : my_dataset/0/0216.jpg\n",
            "listed file : my_dataset/0/0217.jpg\n",
            "listed file : my_dataset/0/0211.jpg\n",
            "listed file : my_dataset/0/0077.jpg\n",
            "listed file : my_dataset/0/0058.jpg\n",
            "listed file : my_dataset/0/0059.jpg\n",
            "listed file : my_dataset/0/0071.jpg\n",
            "listed file : my_dataset/0/0227.jpg\n",
            "listed file : my_dataset/0/0222.jpg\n",
            "listed file : my_dataset/0/0006.jpg\n",
            "listed file : my_dataset/0/0056.jpg\n",
            "listed file : my_dataset/0/0007.jpg\n",
            "listed file : my_dataset/0/0057.jpg\n",
            "listed file : my_dataset/0/0074.jpg\n",
            "listed file : my_dataset/0/0008.jpg\n",
            "listed file : my_dataset/0/0223.jpg\n",
            "listed file : my_dataset/0/0055.jpg\n",
            "listed file : my_dataset/0/0219.jpg\n",
            "listed file : my_dataset/0/0075.jpg\n",
            "listed file : my_dataset/0/0210.jpg\n",
            "listed file : my_dataset/0/0069.jpg\n",
            "listed file : my_dataset/0/0010.jpg\n",
            "listed file : my_dataset/0/0054.jpg\n",
            "listed file : my_dataset/0/0060.jpg\n",
            "listed file : my_dataset/0/0231.jpg\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "train.csv, test.csv, converted_datasetsフォルダが作成されていることを確認する。"
      ],
      "metadata": {
        "id": "4UvfZiD3NUCl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!ls"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z2M39YKWNf3l",
        "outputId": "79401105-5d0c-42f1-d31b-c6ad18795de1"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Arduino\t\t     my_dataset\t\t\t\t       README.md\n",
            "converted_datasets   nnabla_convert_mnist_model_to_csrc.ipynb  test.csv\n",
            "make_dataset_csv.py  nnabla-example-modify\t\t       train.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "nnablaのMNISTサンプルコードのディレクトリに、作成したCSV、データセット（変換後）、改造したプログラムをコピー。"
      ],
      "metadata": {
        "id": "7KqdiU04owwU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!cp *.csv ../nnabla-examples/image-classification/mnist-collection/"
      ],
      "metadata": {
        "id": "qLN_kW59pN7S"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!cp -r converted_datasets ../nnabla-examples/image-classification/mnist-collection/"
      ],
      "metadata": {
        "id": "2f-gxaINq_dB"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!cp nnabla-example-modify/classification_mydata.py ../nnabla-examples/image-classification/mnist-collection/"
      ],
      "metadata": {
        "id": "T58Z5AgTreIJ"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "mnistサンプルプログラムのフォルダに移動し、学習を実行。"
      ],
      "metadata": {
        "id": "loUVBhtMZ4IB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%cd ../nnabla-examples/image-classification/mnist-collection/"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yocj4nJVG-sF",
        "outputId": "eaa8474f-3a80-4b02-caef-af1d0e1c11cd"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/nnabla-examples/image-classification/mnist-collection\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python classification_mydata.py -c cudnn -n lenet -o output"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-CTfat5XH3wl",
        "outputId": "c2b4b7fd-9e30-4725-ba64-6f285c47c6e2"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2023-02-12 14:07:16,368 [nnabla][INFO]: Initializing CPU extension...\n",
            "2023-02-12 14:07:17,712 [nnabla][INFO]: Running in cudnn\n",
            "2023-02-12 14:07:17,937 [nnabla][INFO]: Initializing CUDA extension...\n",
            "2023-02-12 14:07:18,053 [nnabla][INFO]: Initializing cuDNN extension...\n",
            "2023-02-12 14:07:23,530 [nnabla][INFO]: Saving output/lenet_result_epoch0.nnp as nnp\n",
            "2023-02-12 14:07:23,530 [nnabla][INFO]: Saving <_io.StringIO object at 0x7f36f213edc0> as prototxt\n",
            "2023-02-12 14:07:23,545 [nnabla][INFO]: Parameter save (.h5): <_io.BytesIO object at 0x7f36f20f2e50>\n",
            "2023-02-12 14:07:23,546 [nnabla][INFO]: Model file is saved as (.nnp): output/lenet_result_epoch0.nnp\n",
            "2023-02-12 14:07:23,546 [nnabla][INFO]: DataSource with shuffle(True)\n",
            "2023-02-12 14:07:23,549 [nnabla][INFO]: Using DataSourceWithFileCache\n",
            "2023-02-12 14:07:23,549 [nnabla][INFO]: DataSource with shuffle(True)\n",
            "2023-02-12 14:07:23,549 [nnabla][INFO]: Cache Directory is None\n",
            "2023-02-12 14:07:23,549 [nnabla][INFO]: Cache size is 100\n",
            "2023-02-12 14:07:23,549 [nnabla][INFO]: Num of thread is 10\n",
            "2023-02-12 14:07:23,549 [nnabla][INFO]: Cache file format is .npy\n",
            "2023-02-12 14:07:23,550 [nnabla][INFO]: Tempdir for cache /tmp/tmpxx4bt5kr created.\n",
            "2023-02-12 14:07:23,619 [nnabla][INFO]: Creating cache file /tmp/tmpxx4bt5kr/cache_00000000_00000099.npy\n",
            "2023-02-12 14:07:23,667 [nnabla][INFO]: Creating cache file /tmp/tmpxx4bt5kr/cache_00000100_00000171.npy\n",
            "2023-02-12 14:07:23,670 [nnabla][INFO]: Using DataSourceWithMemoryCache\n",
            "2023-02-12 14:07:23,670 [nnabla][INFO]: DataSource with shuffle(True)\n",
            "2023-02-12 14:07:23,679 [nnabla][INFO]: On-memory\n",
            "2023-02-12 14:07:23,679 [nnabla][INFO]: Using DataIterator\n",
            "2023-02-12 14:07:23,680 [nnabla][INFO]: DataSource with shuffle(False)\n",
            "2023-02-12 14:07:23,681 [nnabla][INFO]: Using DataSourceWithFileCache\n",
            "2023-02-12 14:07:23,681 [nnabla][INFO]: DataSource with shuffle(False)\n",
            "2023-02-12 14:07:23,681 [nnabla][INFO]: Cache Directory is None\n",
            "2023-02-12 14:07:23,682 [nnabla][INFO]: Cache size is 100\n",
            "2023-02-12 14:07:23,682 [nnabla][INFO]: Num of thread is 10\n",
            "2023-02-12 14:07:23,682 [nnabla][INFO]: Cache file format is .npy\n",
            "2023-02-12 14:07:23,682 [nnabla][INFO]: Tempdir for cache /tmp/tmp8xrfo58k created.\n",
            "2023-02-12 14:07:23,711 [nnabla][INFO]: Creating cache file /tmp/tmp8xrfo58k/cache_00000000_00000042.npy\n",
            "2023-02-12 14:07:23,713 [nnabla][INFO]: Using DataSourceWithMemoryCache\n",
            "2023-02-12 14:07:23,713 [nnabla][INFO]: DataSource with shuffle(False)\n",
            "2023-02-12 14:07:23,714 [nnabla][INFO]: On-memory\n",
            "2023-02-12 14:07:23,714 [nnabla][INFO]: Using DataIterator\n",
            "2023-02-12 14:07:26,265 [nnabla][INFO]: Solver state save (.h5): output/states_0.h5\n",
            "2023-02-12 14:07:26,272 [nnabla][INFO]: Parameter save (.h5): output/params_0.h5\n",
            "2023-02-12 14:07:26,272 [nnabla][INFO]: Checkpoint save (.json): output/checkpoint_0.json\n",
            "2023-02-12 14:07:26,661 [nnabla][INFO]: iter=9 {Training loss}=1.7988436222076416\n",
            "2023-02-12 14:07:26,661 [nnabla][INFO]: iter=9 {Training error}=0.79453125\n",
            "2023-02-12 14:07:26,699 [nnabla][INFO]: iter=19 {Training loss}=1.2873797416687012\n",
            "2023-02-12 14:07:26,700 [nnabla][INFO]: iter=19 {Training error}=0.46796875\n",
            "2023-02-12 14:07:26,739 [nnabla][INFO]: iter=29 {Training loss}=1.1050760746002197\n",
            "2023-02-12 14:07:26,739 [nnabla][INFO]: iter=29 {Training error}=0.346875\n",
            "2023-02-12 14:07:26,778 [nnabla][INFO]: iter=39 {Training loss}=0.8999457359313965\n",
            "2023-02-12 14:07:26,778 [nnabla][INFO]: iter=39 {Training error}=0.2265625\n",
            "2023-02-12 14:07:26,817 [nnabla][INFO]: iter=49 {Training loss}=0.6807001233100891\n",
            "2023-02-12 14:07:26,817 [nnabla][INFO]: iter=49 {Training error}=0.15546875\n",
            "2023-02-12 14:07:26,856 [nnabla][INFO]: iter=59 {Training loss}=0.5262393951416016\n",
            "2023-02-12 14:07:26,857 [nnabla][INFO]: iter=59 {Training error}=0.15234375\n",
            "2023-02-12 14:07:26,895 [nnabla][INFO]: iter=69 {Training loss}=0.4104100167751312\n",
            "2023-02-12 14:07:26,896 [nnabla][INFO]: iter=69 {Training error}=0.128125\n",
            "2023-02-12 14:07:26,934 [nnabla][INFO]: iter=79 {Training loss}=0.3598726689815521\n",
            "2023-02-12 14:07:26,934 [nnabla][INFO]: iter=79 {Training error}=0.1109375\n",
            "2023-02-12 14:07:26,974 [nnabla][INFO]: iter=89 {Training loss}=0.33428841829299927\n",
            "2023-02-12 14:07:26,975 [nnabla][INFO]: iter=89 {Training error}=0.11484375\n",
            "2023-02-12 14:07:27,012 [nnabla][INFO]: iter=99 {Training loss}=0.2779384255409241\n",
            "2023-02-12 14:07:27,012 [nnabla][INFO]: iter=99 {Training error}=0.090625\n",
            "2023-02-12 14:07:27,013 [nnabla][INFO]: iter=99 {Training time}=3.488737106323242[sec/100iter] 3.488737106323242[sec]\n",
            "2023-02-12 14:07:27,034 [nnabla][INFO]: iter=100 {Test error}=0.500390625\n",
            "2023-02-12 14:07:27,072 [nnabla][INFO]: iter=109 {Training loss}=0.24154475331306458\n",
            "2023-02-12 14:07:27,072 [nnabla][INFO]: iter=109 {Training error}=0.07890625\n",
            "2023-02-12 14:07:27,110 [nnabla][INFO]: iter=119 {Training loss}=0.2129632979631424\n",
            "2023-02-12 14:07:27,110 [nnabla][INFO]: iter=119 {Training error}=0.0609375\n",
            "2023-02-12 14:07:27,148 [nnabla][INFO]: iter=129 {Training loss}=0.19460885226726532\n",
            "2023-02-12 14:07:27,148 [nnabla][INFO]: iter=129 {Training error}=0.0453125\n",
            "2023-02-12 14:07:27,184 [nnabla][INFO]: iter=139 {Training loss}=0.1767590194940567\n",
            "2023-02-12 14:07:27,184 [nnabla][INFO]: iter=139 {Training error}=0.0484375\n",
            "2023-02-12 14:07:27,223 [nnabla][INFO]: iter=149 {Training loss}=0.16666758060455322\n",
            "2023-02-12 14:07:27,223 [nnabla][INFO]: iter=149 {Training error}=0.040625\n",
            "2023-02-12 14:07:27,262 [nnabla][INFO]: iter=159 {Training loss}=0.15472370386123657\n",
            "2023-02-12 14:07:27,262 [nnabla][INFO]: iter=159 {Training error}=0.025\n",
            "2023-02-12 14:07:27,299 [nnabla][INFO]: iter=169 {Training loss}=0.1447964608669281\n",
            "2023-02-12 14:07:27,299 [nnabla][INFO]: iter=169 {Training error}=0.02578125\n",
            "2023-02-12 14:07:27,335 [nnabla][INFO]: iter=179 {Training loss}=0.1272123157978058\n",
            "2023-02-12 14:07:27,335 [nnabla][INFO]: iter=179 {Training error}=0.01796875\n",
            "2023-02-12 14:07:27,375 [nnabla][INFO]: iter=189 {Training loss}=0.13949108123779297\n",
            "2023-02-12 14:07:27,376 [nnabla][INFO]: iter=189 {Training error}=0.02109375\n",
            "2023-02-12 14:07:27,411 [nnabla][INFO]: iter=199 {Training loss}=0.12256071716547012\n",
            "2023-02-12 14:07:27,412 [nnabla][INFO]: iter=199 {Training error}=0.0203125\n",
            "2023-02-12 14:07:27,412 [nnabla][INFO]: iter=199 {Training time}=0.39920473098754883[sec/100iter] 3.887941837310791[sec]\n",
            "2023-02-12 14:07:27,433 [nnabla][INFO]: iter=200 {Test error}=0.046875\n",
            "2023-02-12 14:07:27,473 [nnabla][INFO]: iter=209 {Training loss}=0.11685547977685928\n",
            "2023-02-12 14:07:27,473 [nnabla][INFO]: iter=209 {Training error}=0.01953125\n",
            "2023-02-12 14:07:27,509 [nnabla][INFO]: iter=219 {Training loss}=0.1164962649345398\n",
            "2023-02-12 14:07:27,510 [nnabla][INFO]: iter=219 {Training error}=0.015625\n",
            "2023-02-12 14:07:27,549 [nnabla][INFO]: iter=229 {Training loss}=0.10663367807865143\n",
            "2023-02-12 14:07:27,549 [nnabla][INFO]: iter=229 {Training error}=0.0171875\n",
            "2023-02-12 14:07:27,586 [nnabla][INFO]: iter=239 {Training loss}=0.10440303385257721\n",
            "2023-02-12 14:07:27,586 [nnabla][INFO]: iter=239 {Training error}=0.0171875\n",
            "2023-02-12 14:07:27,626 [nnabla][INFO]: iter=249 {Training loss}=0.10869961977005005\n",
            "2023-02-12 14:07:27,627 [nnabla][INFO]: iter=249 {Training error}=0.015625\n",
            "2023-02-12 14:07:27,666 [nnabla][INFO]: iter=259 {Training loss}=0.10638783872127533\n",
            "2023-02-12 14:07:27,666 [nnabla][INFO]: iter=259 {Training error}=0.01484375\n",
            "2023-02-12 14:07:27,703 [nnabla][INFO]: iter=269 {Training loss}=0.09219781309366226\n",
            "2023-02-12 14:07:27,703 [nnabla][INFO]: iter=269 {Training error}=0.01171875\n",
            "2023-02-12 14:07:27,740 [nnabla][INFO]: iter=279 {Training loss}=0.10262691974639893\n",
            "2023-02-12 14:07:27,740 [nnabla][INFO]: iter=279 {Training error}=0.0140625\n",
            "2023-02-12 14:07:27,776 [nnabla][INFO]: iter=289 {Training loss}=0.10735958814620972\n",
            "2023-02-12 14:07:27,776 [nnabla][INFO]: iter=289 {Training error}=0.01796875\n",
            "2023-02-12 14:07:27,813 [nnabla][INFO]: iter=299 {Training loss}=0.09303885698318481\n",
            "2023-02-12 14:07:27,813 [nnabla][INFO]: iter=299 {Training error}=0.01328125\n",
            "2023-02-12 14:07:27,813 [nnabla][INFO]: iter=299 {Training time}=0.4011380672454834[sec/100iter] 4.289079904556274[sec]\n",
            "2023-02-12 14:07:27,834 [nnabla][INFO]: iter=300 {Test error}=0.04609375\n",
            "2023-02-12 14:07:27,870 [nnabla][INFO]: iter=309 {Training loss}=0.08828366547822952\n",
            "2023-02-12 14:07:27,871 [nnabla][INFO]: iter=309 {Training error}=0.0109375\n",
            "2023-02-12 14:07:27,906 [nnabla][INFO]: iter=319 {Training loss}=0.08363001048564911\n",
            "2023-02-12 14:07:27,907 [nnabla][INFO]: iter=319 {Training error}=0.01171875\n",
            "2023-02-12 14:07:27,942 [nnabla][INFO]: iter=329 {Training loss}=0.0777745172381401\n",
            "2023-02-12 14:07:27,942 [nnabla][INFO]: iter=329 {Training error}=0.0078125\n",
            "2023-02-12 14:07:27,981 [nnabla][INFO]: iter=339 {Training loss}=0.08648493140935898\n",
            "2023-02-12 14:07:27,981 [nnabla][INFO]: iter=339 {Training error}=0.01015625\n",
            "2023-02-12 14:07:28,017 [nnabla][INFO]: iter=349 {Training loss}=0.06515473127365112\n",
            "2023-02-12 14:07:28,017 [nnabla][INFO]: iter=349 {Training error}=0.00703125\n",
            "2023-02-12 14:07:28,052 [nnabla][INFO]: iter=359 {Training loss}=0.07199843972921371\n",
            "2023-02-12 14:07:28,052 [nnabla][INFO]: iter=359 {Training error}=0.00859375\n",
            "2023-02-12 14:07:28,093 [nnabla][INFO]: iter=369 {Training loss}=0.06894038617610931\n",
            "2023-02-12 14:07:28,094 [nnabla][INFO]: iter=369 {Training error}=0.00703125\n",
            "2023-02-12 14:07:28,129 [nnabla][INFO]: iter=379 {Training loss}=0.0727120041847229\n",
            "2023-02-12 14:07:28,129 [nnabla][INFO]: iter=379 {Training error}=0.00859375\n",
            "2023-02-12 14:07:28,164 [nnabla][INFO]: iter=389 {Training loss}=0.06759671866893768\n",
            "2023-02-12 14:07:28,165 [nnabla][INFO]: iter=389 {Training error}=0.00703125\n",
            "2023-02-12 14:07:28,202 [nnabla][INFO]: iter=399 {Training loss}=0.06783229857683182\n",
            "2023-02-12 14:07:28,202 [nnabla][INFO]: iter=399 {Training error}=0.00546875\n",
            "2023-02-12 14:07:28,202 [nnabla][INFO]: iter=399 {Training time}=0.3895390033721924[sec/100iter] 4.678618907928467[sec]\n",
            "2023-02-12 14:07:28,223 [nnabla][INFO]: iter=400 {Test error}=0.04609375\n",
            "2023-02-12 14:07:28,258 [nnabla][INFO]: iter=409 {Training loss}=0.056179482489824295\n",
            "2023-02-12 14:07:28,258 [nnabla][INFO]: iter=409 {Training error}=0.00546875\n",
            "2023-02-12 14:07:28,293 [nnabla][INFO]: iter=419 {Training loss}=0.052665580064058304\n",
            "2023-02-12 14:07:28,293 [nnabla][INFO]: iter=419 {Training error}=0.00546875\n",
            "2023-02-12 14:07:28,328 [nnabla][INFO]: iter=429 {Training loss}=0.057200293987989426\n",
            "2023-02-12 14:07:28,328 [nnabla][INFO]: iter=429 {Training error}=0.00625\n",
            "2023-02-12 14:07:28,367 [nnabla][INFO]: iter=439 {Training loss}=0.05487853288650513\n",
            "2023-02-12 14:07:28,368 [nnabla][INFO]: iter=439 {Training error}=0.00625\n",
            "2023-02-12 14:07:28,402 [nnabla][INFO]: iter=449 {Training loss}=0.044693175703287125\n",
            "2023-02-12 14:07:28,403 [nnabla][INFO]: iter=449 {Training error}=0.0046875\n",
            "2023-02-12 14:07:28,440 [nnabla][INFO]: iter=459 {Training loss}=0.05444703623652458\n",
            "2023-02-12 14:07:28,440 [nnabla][INFO]: iter=459 {Training error}=0.00703125\n",
            "2023-02-12 14:07:28,481 [nnabla][INFO]: iter=469 {Training loss}=0.04805048182606697\n",
            "2023-02-12 14:07:28,481 [nnabla][INFO]: iter=469 {Training error}=0.00546875\n",
            "2023-02-12 14:07:28,519 [nnabla][INFO]: iter=479 {Training loss}=0.0504329614341259\n",
            "2023-02-12 14:07:28,519 [nnabla][INFO]: iter=479 {Training error}=0.0078125\n",
            "2023-02-12 14:07:28,561 [nnabla][INFO]: iter=489 {Training loss}=0.04747964069247246\n",
            "2023-02-12 14:07:28,561 [nnabla][INFO]: iter=489 {Training error}=0.00625\n",
            "2023-02-12 14:07:28,599 [nnabla][INFO]: iter=499 {Training loss}=0.046464480459690094\n",
            "2023-02-12 14:07:28,600 [nnabla][INFO]: iter=499 {Training error}=0.0078125\n",
            "2023-02-12 14:07:28,600 [nnabla][INFO]: iter=499 {Training time}=0.3974640369415283[sec/100iter] 5.076082944869995[sec]\n",
            "2023-02-12 14:07:28,622 [nnabla][INFO]: iter=500 {Test error}=0.0234375\n",
            "2023-02-12 14:07:28,658 [nnabla][INFO]: iter=509 {Training loss}=0.05257755517959595\n",
            "2023-02-12 14:07:28,659 [nnabla][INFO]: iter=509 {Training error}=0.01015625\n",
            "2023-02-12 14:07:28,696 [nnabla][INFO]: iter=519 {Training loss}=0.04012870043516159\n",
            "2023-02-12 14:07:28,696 [nnabla][INFO]: iter=519 {Training error}=0.00625\n",
            "2023-02-12 14:07:28,735 [nnabla][INFO]: iter=529 {Training loss}=0.04643779247999191\n",
            "2023-02-12 14:07:28,735 [nnabla][INFO]: iter=529 {Training error}=0.00859375\n",
            "2023-02-12 14:07:28,773 [nnabla][INFO]: iter=539 {Training loss}=0.04535529017448425\n",
            "2023-02-12 14:07:28,774 [nnabla][INFO]: iter=539 {Training error}=0.0078125\n",
            "2023-02-12 14:07:28,810 [nnabla][INFO]: iter=549 {Training loss}=0.03755989298224449\n",
            "2023-02-12 14:07:28,810 [nnabla][INFO]: iter=549 {Training error}=0.00625\n",
            "2023-02-12 14:07:28,846 [nnabla][INFO]: iter=559 {Training loss}=0.03168145567178726\n",
            "2023-02-12 14:07:28,846 [nnabla][INFO]: iter=559 {Training error}=0.00546875\n",
            "2023-02-12 14:07:28,882 [nnabla][INFO]: iter=569 {Training loss}=0.03342823311686516\n",
            "2023-02-12 14:07:28,882 [nnabla][INFO]: iter=569 {Training error}=0.0078125\n",
            "2023-02-12 14:07:28,918 [nnabla][INFO]: iter=579 {Training loss}=0.03079601190984249\n",
            "2023-02-12 14:07:28,918 [nnabla][INFO]: iter=579 {Training error}=0.00546875\n",
            "2023-02-12 14:07:28,953 [nnabla][INFO]: iter=589 {Training loss}=0.033240459859371185\n",
            "2023-02-12 14:07:28,954 [nnabla][INFO]: iter=589 {Training error}=0.0078125\n",
            "2023-02-12 14:07:28,990 [nnabla][INFO]: iter=599 {Training loss}=0.02811671420931816\n",
            "2023-02-12 14:07:28,990 [nnabla][INFO]: iter=599 {Training error}=0.00703125\n",
            "2023-02-12 14:07:28,990 [nnabla][INFO]: iter=599 {Training time}=0.390026330947876[sec/100iter] 5.466109275817871[sec]\n",
            "2023-02-12 14:07:29,012 [nnabla][INFO]: iter=600 {Test error}=0.046875\n",
            "2023-02-12 14:07:29,048 [nnabla][INFO]: iter=609 {Training loss}=0.02798260748386383\n",
            "2023-02-12 14:07:29,048 [nnabla][INFO]: iter=609 {Training error}=0.00859375\n",
            "2023-02-12 14:07:29,087 [nnabla][INFO]: iter=619 {Training loss}=0.029263967648148537\n",
            "2023-02-12 14:07:29,087 [nnabla][INFO]: iter=619 {Training error}=0.00625\n",
            "2023-02-12 14:07:29,122 [nnabla][INFO]: iter=629 {Training loss}=0.028173942118883133\n",
            "2023-02-12 14:07:29,123 [nnabla][INFO]: iter=629 {Training error}=0.0078125\n",
            "2023-02-12 14:07:29,159 [nnabla][INFO]: iter=639 {Training loss}=0.022691234946250916\n",
            "2023-02-12 14:07:29,159 [nnabla][INFO]: iter=639 {Training error}=0.00625\n",
            "2023-02-12 14:07:29,195 [nnabla][INFO]: iter=649 {Training loss}=0.028261950239539146\n",
            "2023-02-12 14:07:29,195 [nnabla][INFO]: iter=649 {Training error}=0.00703125\n",
            "2023-02-12 14:07:29,232 [nnabla][INFO]: iter=659 {Training loss}=0.02058541029691696\n",
            "2023-02-12 14:07:29,232 [nnabla][INFO]: iter=659 {Training error}=0.00546875\n",
            "2023-02-12 14:07:29,274 [nnabla][INFO]: iter=669 {Training loss}=0.017784588038921356\n",
            "2023-02-12 14:07:29,274 [nnabla][INFO]: iter=669 {Training error}=0.00625\n",
            "2023-02-12 14:07:29,310 [nnabla][INFO]: iter=679 {Training loss}=0.018488088622689247\n",
            "2023-02-12 14:07:29,310 [nnabla][INFO]: iter=679 {Training error}=0.00546875\n",
            "2023-02-12 14:07:29,346 [nnabla][INFO]: iter=689 {Training loss}=0.016674574464559555\n",
            "2023-02-12 14:07:29,346 [nnabla][INFO]: iter=689 {Training error}=0.00546875\n",
            "2023-02-12 14:07:29,382 [nnabla][INFO]: iter=699 {Training loss}=0.015831660479307175\n",
            "2023-02-12 14:07:29,382 [nnabla][INFO]: iter=699 {Training error}=0.00546875\n",
            "2023-02-12 14:07:29,382 [nnabla][INFO]: iter=699 {Training time}=0.39231348037719727[sec/100iter] 5.858422756195068[sec]\n",
            "2023-02-12 14:07:29,403 [nnabla][INFO]: iter=700 {Test error}=0.0234375\n",
            "2023-02-12 14:07:29,439 [nnabla][INFO]: iter=709 {Training loss}=0.015404445119202137\n",
            "2023-02-12 14:07:29,440 [nnabla][INFO]: iter=709 {Training error}=0.00234375\n",
            "2023-02-12 14:07:29,479 [nnabla][INFO]: iter=719 {Training loss}=0.013580625876784325\n",
            "2023-02-12 14:07:29,479 [nnabla][INFO]: iter=719 {Training error}=0.00078125\n",
            "2023-02-12 14:07:29,516 [nnabla][INFO]: iter=729 {Training loss}=0.012590792961418629\n",
            "2023-02-12 14:07:29,516 [nnabla][INFO]: iter=729 {Training error}=0.00078125\n",
            "2023-02-12 14:07:29,558 [nnabla][INFO]: iter=739 {Training loss}=0.011542153544723988\n",
            "2023-02-12 14:07:29,558 [nnabla][INFO]: iter=739 {Training error}=0.0\n",
            "2023-02-12 14:07:29,599 [nnabla][INFO]: iter=749 {Training loss}=0.01288511324673891\n",
            "2023-02-12 14:07:29,600 [nnabla][INFO]: iter=749 {Training error}=0.0015625\n",
            "2023-02-12 14:07:29,636 [nnabla][INFO]: iter=759 {Training loss}=0.011090362444519997\n",
            "2023-02-12 14:07:29,636 [nnabla][INFO]: iter=759 {Training error}=0.0\n",
            "2023-02-12 14:07:29,672 [nnabla][INFO]: iter=769 {Training loss}=0.00999903492629528\n",
            "2023-02-12 14:07:29,672 [nnabla][INFO]: iter=769 {Training error}=0.0\n",
            "2023-02-12 14:07:29,709 [nnabla][INFO]: iter=779 {Training loss}=0.00997755117714405\n",
            "2023-02-12 14:07:29,710 [nnabla][INFO]: iter=779 {Training error}=0.0\n",
            "2023-02-12 14:07:29,745 [nnabla][INFO]: iter=789 {Training loss}=0.010187899693846703\n",
            "2023-02-12 14:07:29,746 [nnabla][INFO]: iter=789 {Training error}=0.00078125\n",
            "2023-02-12 14:07:29,783 [nnabla][INFO]: iter=799 {Training loss}=0.009519064798951149\n",
            "2023-02-12 14:07:29,783 [nnabla][INFO]: iter=799 {Training error}=0.0\n",
            "2023-02-12 14:07:29,783 [nnabla][INFO]: iter=799 {Training time}=0.4006845951080322[sec/100iter] 6.259107351303101[sec]\n",
            "2023-02-12 14:07:29,804 [nnabla][INFO]: iter=800 {Test error}=0.02265625\n",
            "2023-02-12 14:07:29,845 [nnabla][INFO]: iter=809 {Training loss}=0.008498537354171276\n",
            "2023-02-12 14:07:29,845 [nnabla][INFO]: iter=809 {Training error}=0.0\n",
            "2023-02-12 14:07:29,880 [nnabla][INFO]: iter=819 {Training loss}=0.008353506214916706\n",
            "2023-02-12 14:07:29,881 [nnabla][INFO]: iter=819 {Training error}=0.0\n",
            "2023-02-12 14:07:29,918 [nnabla][INFO]: iter=829 {Training loss}=0.00712666567414999\n",
            "2023-02-12 14:07:29,918 [nnabla][INFO]: iter=829 {Training error}=0.0\n",
            "2023-02-12 14:07:29,953 [nnabla][INFO]: iter=839 {Training loss}=0.007159649394452572\n",
            "2023-02-12 14:07:29,954 [nnabla][INFO]: iter=839 {Training error}=0.0\n",
            "2023-02-12 14:07:29,993 [nnabla][INFO]: iter=849 {Training loss}=0.0065484000369906425\n",
            "2023-02-12 14:07:29,993 [nnabla][INFO]: iter=849 {Training error}=0.0\n",
            "2023-02-12 14:07:30,029 [nnabla][INFO]: iter=859 {Training loss}=0.00645161047577858\n",
            "2023-02-12 14:07:30,029 [nnabla][INFO]: iter=859 {Training error}=0.0\n",
            "2023-02-12 14:07:30,065 [nnabla][INFO]: iter=869 {Training loss}=0.005336005240678787\n",
            "2023-02-12 14:07:30,065 [nnabla][INFO]: iter=869 {Training error}=0.0\n",
            "2023-02-12 14:07:30,108 [nnabla][INFO]: iter=879 {Training loss}=0.005666987504810095\n",
            "2023-02-12 14:07:30,108 [nnabla][INFO]: iter=879 {Training error}=0.0\n",
            "2023-02-12 14:07:30,144 [nnabla][INFO]: iter=889 {Training loss}=0.005832668859511614\n",
            "2023-02-12 14:07:30,144 [nnabla][INFO]: iter=889 {Training error}=0.0\n",
            "2023-02-12 14:07:30,180 [nnabla][INFO]: iter=899 {Training loss}=0.005442983936518431\n",
            "2023-02-12 14:07:30,180 [nnabla][INFO]: iter=899 {Training error}=0.0\n",
            "2023-02-12 14:07:30,180 [nnabla][INFO]: iter=899 {Training time}=0.3975353240966797[sec/100iter] 6.65664267539978[sec]\n",
            "2023-02-12 14:07:30,206 [nnabla][INFO]: iter=900 {Test error}=0.0234375\n",
            "2023-02-12 14:07:30,241 [nnabla][INFO]: iter=909 {Training loss}=0.004744783975183964\n",
            "2023-02-12 14:07:30,242 [nnabla][INFO]: iter=909 {Training error}=0.0\n",
            "2023-02-12 14:07:30,278 [nnabla][INFO]: iter=919 {Training loss}=0.0048385923728346825\n",
            "2023-02-12 14:07:30,278 [nnabla][INFO]: iter=919 {Training error}=0.0\n",
            "2023-02-12 14:07:30,314 [nnabla][INFO]: iter=929 {Training loss}=0.004538345616310835\n",
            "2023-02-12 14:07:30,314 [nnabla][INFO]: iter=929 {Training error}=0.0\n",
            "2023-02-12 14:07:30,351 [nnabla][INFO]: iter=939 {Training loss}=0.004068490117788315\n",
            "2023-02-12 14:07:30,351 [nnabla][INFO]: iter=939 {Training error}=0.0\n",
            "2023-02-12 14:07:30,394 [nnabla][INFO]: iter=949 {Training loss}=0.0037066214717924595\n",
            "2023-02-12 14:07:30,394 [nnabla][INFO]: iter=949 {Training error}=0.0\n",
            "2023-02-12 14:07:30,430 [nnabla][INFO]: iter=959 {Training loss}=0.0036510881036520004\n",
            "2023-02-12 14:07:30,431 [nnabla][INFO]: iter=959 {Training error}=0.0\n",
            "2023-02-12 14:07:30,466 [nnabla][INFO]: iter=969 {Training loss}=0.003407395211979747\n",
            "2023-02-12 14:07:30,466 [nnabla][INFO]: iter=969 {Training error}=0.0\n",
            "2023-02-12 14:07:30,503 [nnabla][INFO]: iter=979 {Training loss}=0.00374084385111928\n",
            "2023-02-12 14:07:30,503 [nnabla][INFO]: iter=979 {Training error}=0.0\n",
            "2023-02-12 14:07:30,538 [nnabla][INFO]: iter=989 {Training loss}=0.0029208483174443245\n",
            "2023-02-12 14:07:30,539 [nnabla][INFO]: iter=989 {Training error}=0.0\n",
            "2023-02-12 14:07:30,583 [nnabla][INFO]: iter=999 {Training loss}=0.0031965423841029406\n",
            "2023-02-12 14:07:30,584 [nnabla][INFO]: iter=999 {Training error}=0.0\n",
            "2023-02-12 14:07:30,584 [nnabla][INFO]: iter=999 {Training time}=0.4033830165863037[sec/100iter] 7.060025691986084[sec]\n",
            "2023-02-12 14:07:30,610 [nnabla][INFO]: iter=1000 {Test error}=0.0234375\n",
            "2023-02-12 14:07:30,627 [nnabla][INFO]: Solver state save (.h5): output/states_1000.h5\n",
            "2023-02-12 14:07:30,637 [nnabla][INFO]: Parameter save (.h5): output/params_1000.h5\n",
            "2023-02-12 14:07:30,637 [nnabla][INFO]: Checkpoint save (.json): output/checkpoint_1000.json\n",
            "2023-02-12 14:07:30,673 [nnabla][INFO]: iter=1009 {Training loss}=0.0026792273856699467\n",
            "2023-02-12 14:07:30,673 [nnabla][INFO]: iter=1009 {Training error}=0.0\n",
            "2023-02-12 14:07:30,711 [nnabla][INFO]: iter=1019 {Training loss}=0.0028238357044756413\n",
            "2023-02-12 14:07:30,712 [nnabla][INFO]: iter=1019 {Training error}=0.0\n",
            "2023-02-12 14:07:30,747 [nnabla][INFO]: iter=1029 {Training loss}=0.00255421525798738\n",
            "2023-02-12 14:07:30,748 [nnabla][INFO]: iter=1029 {Training error}=0.0\n",
            "2023-02-12 14:07:30,783 [nnabla][INFO]: iter=1039 {Training loss}=0.0027337330393493176\n",
            "2023-02-12 14:07:30,783 [nnabla][INFO]: iter=1039 {Training error}=0.0\n",
            "2023-02-12 14:07:30,820 [nnabla][INFO]: iter=1049 {Training loss}=0.0025321352295577526\n",
            "2023-02-12 14:07:30,820 [nnabla][INFO]: iter=1049 {Training error}=0.0\n",
            "2023-02-12 14:07:30,856 [nnabla][INFO]: iter=1059 {Training loss}=0.0024886319879442453\n",
            "2023-02-12 14:07:30,856 [nnabla][INFO]: iter=1059 {Training error}=0.0\n",
            "2023-02-12 14:07:30,895 [nnabla][INFO]: iter=1069 {Training loss}=0.002150020794942975\n",
            "2023-02-12 14:07:30,896 [nnabla][INFO]: iter=1069 {Training error}=0.0\n",
            "2023-02-12 14:07:30,933 [nnabla][INFO]: iter=1079 {Training loss}=0.0022653797641396523\n",
            "2023-02-12 14:07:30,933 [nnabla][INFO]: iter=1079 {Training error}=0.0\n",
            "2023-02-12 14:07:30,969 [nnabla][INFO]: iter=1089 {Training loss}=0.0020557385869324207\n",
            "2023-02-12 14:07:30,969 [nnabla][INFO]: iter=1089 {Training error}=0.0\n",
            "2023-02-12 14:07:31,005 [nnabla][INFO]: iter=1099 {Training loss}=0.001955456333234906\n",
            "2023-02-12 14:07:31,005 [nnabla][INFO]: iter=1099 {Training error}=0.0\n",
            "2023-02-12 14:07:31,005 [nnabla][INFO]: iter=1099 {Training time}=0.42155981063842773[sec/100iter] 7.481585502624512[sec]\n",
            "2023-02-12 14:07:31,026 [nnabla][INFO]: iter=1100 {Test error}=0.0234375\n",
            "2023-02-12 14:07:31,062 [nnabla][INFO]: iter=1109 {Training loss}=0.0018976446008309722\n",
            "2023-02-12 14:07:31,062 [nnabla][INFO]: iter=1109 {Training error}=0.0\n",
            "2023-02-12 14:07:31,098 [nnabla][INFO]: iter=1119 {Training loss}=0.0018619541078805923\n",
            "2023-02-12 14:07:31,101 [nnabla][INFO]: iter=1119 {Training error}=0.0\n",
            "2023-02-12 14:07:31,137 [nnabla][INFO]: iter=1129 {Training loss}=0.0021500992588698864\n",
            "2023-02-12 14:07:31,137 [nnabla][INFO]: iter=1129 {Training error}=0.0\n",
            "2023-02-12 14:07:31,179 [nnabla][INFO]: iter=1139 {Training loss}=0.0019768152851611376\n",
            "2023-02-12 14:07:31,179 [nnabla][INFO]: iter=1139 {Training error}=0.0\n",
            "2023-02-12 14:07:31,216 [nnabla][INFO]: iter=1149 {Training loss}=0.001686756731942296\n",
            "2023-02-12 14:07:31,216 [nnabla][INFO]: iter=1149 {Training error}=0.0\n",
            "2023-02-12 14:07:31,252 [nnabla][INFO]: iter=1159 {Training loss}=0.0015753904590383172\n",
            "2023-02-12 14:07:31,253 [nnabla][INFO]: iter=1159 {Training error}=0.0\n",
            "2023-02-12 14:07:31,289 [nnabla][INFO]: iter=1169 {Training loss}=0.0015654496382921934\n",
            "2023-02-12 14:07:31,289 [nnabla][INFO]: iter=1169 {Training error}=0.0\n",
            "2023-02-12 14:07:31,327 [nnabla][INFO]: iter=1179 {Training loss}=0.0014462685212492943\n",
            "2023-02-12 14:07:31,328 [nnabla][INFO]: iter=1179 {Training error}=0.0\n",
            "2023-02-12 14:07:31,364 [nnabla][INFO]: iter=1189 {Training loss}=0.001465366454795003\n",
            "2023-02-12 14:07:31,365 [nnabla][INFO]: iter=1189 {Training error}=0.0\n",
            "2023-02-12 14:07:31,400 [nnabla][INFO]: iter=1199 {Training loss}=0.0014013819163665175\n",
            "2023-02-12 14:07:31,401 [nnabla][INFO]: iter=1199 {Training error}=0.0\n",
            "2023-02-12 14:07:31,401 [nnabla][INFO]: iter=1199 {Training time}=0.3953726291656494[sec/100iter] 7.876958131790161[sec]\n",
            "2023-02-12 14:07:31,425 [nnabla][INFO]: iter=1200 {Test error}=0.02265625\n",
            "2023-02-12 14:07:31,461 [nnabla][INFO]: iter=1209 {Training loss}=0.0013923193328082561\n",
            "2023-02-12 14:07:31,462 [nnabla][INFO]: iter=1209 {Training error}=0.0\n",
            "2023-02-12 14:07:31,499 [nnabla][INFO]: iter=1219 {Training loss}=0.0013721173163503408\n",
            "2023-02-12 14:07:31,499 [nnabla][INFO]: iter=1219 {Training error}=0.0\n",
            "2023-02-12 14:07:31,539 [nnabla][INFO]: iter=1229 {Training loss}=0.0013654535869136453\n",
            "2023-02-12 14:07:31,539 [nnabla][INFO]: iter=1229 {Training error}=0.0\n",
            "2023-02-12 14:07:31,576 [nnabla][INFO]: iter=1239 {Training loss}=0.001384720322676003\n",
            "2023-02-12 14:07:31,577 [nnabla][INFO]: iter=1239 {Training error}=0.0\n",
            "2023-02-12 14:07:31,616 [nnabla][INFO]: iter=1249 {Training loss}=0.0011996307875961065\n",
            "2023-02-12 14:07:31,616 [nnabla][INFO]: iter=1249 {Training error}=0.0\n",
            "2023-02-12 14:07:31,652 [nnabla][INFO]: iter=1259 {Training loss}=0.00118401856161654\n",
            "2023-02-12 14:07:31,653 [nnabla][INFO]: iter=1259 {Training error}=0.0\n",
            "2023-02-12 14:07:31,695 [nnabla][INFO]: iter=1269 {Training loss}=0.001133591984398663\n",
            "2023-02-12 14:07:31,695 [nnabla][INFO]: iter=1269 {Training error}=0.0\n",
            "2023-02-12 14:07:31,731 [nnabla][INFO]: iter=1279 {Training loss}=0.0010878751054406166\n",
            "2023-02-12 14:07:31,731 [nnabla][INFO]: iter=1279 {Training error}=0.0\n",
            "2023-02-12 14:07:31,767 [nnabla][INFO]: iter=1289 {Training loss}=0.001117453328333795\n",
            "2023-02-12 14:07:31,767 [nnabla][INFO]: iter=1289 {Training error}=0.0\n",
            "2023-02-12 14:07:31,803 [nnabla][INFO]: iter=1299 {Training loss}=0.0010347332572564483\n",
            "2023-02-12 14:07:31,804 [nnabla][INFO]: iter=1299 {Training error}=0.0\n",
            "2023-02-12 14:07:31,804 [nnabla][INFO]: iter=1299 {Training time}=0.40293169021606445[sec/100iter] 8.279889822006226[sec]\n",
            "2023-02-12 14:07:31,828 [nnabla][INFO]: iter=1300 {Test error}=0.0234375\n",
            "2023-02-12 14:07:31,864 [nnabla][INFO]: iter=1309 {Training loss}=0.0010447429958730936\n",
            "2023-02-12 14:07:31,864 [nnabla][INFO]: iter=1309 {Training error}=0.0\n",
            "2023-02-12 14:07:31,900 [nnabla][INFO]: iter=1319 {Training loss}=0.0009967386722564697\n",
            "2023-02-12 14:07:31,900 [nnabla][INFO]: iter=1319 {Training error}=0.0\n",
            "2023-02-12 14:07:31,942 [nnabla][INFO]: iter=1329 {Training loss}=0.000967913307249546\n",
            "2023-02-12 14:07:31,942 [nnabla][INFO]: iter=1329 {Training error}=0.0\n",
            "2023-02-12 14:07:31,978 [nnabla][INFO]: iter=1339 {Training loss}=0.000980100710876286\n",
            "2023-02-12 14:07:31,978 [nnabla][INFO]: iter=1339 {Training error}=0.0\n",
            "2023-02-12 14:07:32,014 [nnabla][INFO]: iter=1349 {Training loss}=0.0009381393902003765\n",
            "2023-02-12 14:07:32,014 [nnabla][INFO]: iter=1349 {Training error}=0.0\n",
            "2023-02-12 14:07:32,049 [nnabla][INFO]: iter=1359 {Training loss}=0.0008685849606990814\n",
            "2023-02-12 14:07:32,050 [nnabla][INFO]: iter=1359 {Training error}=0.0\n",
            "2023-02-12 14:07:32,086 [nnabla][INFO]: iter=1369 {Training loss}=0.0008688373491168022\n",
            "2023-02-12 14:07:32,086 [nnabla][INFO]: iter=1369 {Training error}=0.0\n",
            "2023-02-12 14:07:32,126 [nnabla][INFO]: iter=1379 {Training loss}=0.0008872590260580182\n",
            "2023-02-12 14:07:32,126 [nnabla][INFO]: iter=1379 {Training error}=0.0\n",
            "2023-02-12 14:07:32,162 [nnabla][INFO]: iter=1389 {Training loss}=0.0008504271390847862\n",
            "2023-02-12 14:07:32,163 [nnabla][INFO]: iter=1389 {Training error}=0.0\n",
            "2023-02-12 14:07:32,206 [nnabla][INFO]: iter=1399 {Training loss}=0.0008089320035651326\n",
            "2023-02-12 14:07:32,206 [nnabla][INFO]: iter=1399 {Training error}=0.0\n",
            "2023-02-12 14:07:32,207 [nnabla][INFO]: iter=1399 {Training time}=0.4028444290161133[sec/100iter] 8.682734251022339[sec]\n",
            "2023-02-12 14:07:32,228 [nnabla][INFO]: iter=1400 {Test error}=0.0234375\n",
            "2023-02-12 14:07:32,265 [nnabla][INFO]: iter=1409 {Training loss}=0.0007972723105922341\n",
            "2023-02-12 14:07:32,265 [nnabla][INFO]: iter=1409 {Training error}=0.0\n",
            "2023-02-12 14:07:32,302 [nnabla][INFO]: iter=1419 {Training loss}=0.0008185238693840802\n",
            "2023-02-12 14:07:32,302 [nnabla][INFO]: iter=1419 {Training error}=0.0\n",
            "2023-02-12 14:07:32,338 [nnabla][INFO]: iter=1429 {Training loss}=0.0007499051280319691\n",
            "2023-02-12 14:07:32,338 [nnabla][INFO]: iter=1429 {Training error}=0.0\n",
            "2023-02-12 14:07:32,374 [nnabla][INFO]: iter=1439 {Training loss}=0.0007331824162974954\n",
            "2023-02-12 14:07:32,374 [nnabla][INFO]: iter=1439 {Training error}=0.0\n",
            "2023-02-12 14:07:32,409 [nnabla][INFO]: iter=1449 {Training loss}=0.0007341566379182041\n",
            "2023-02-12 14:07:32,409 [nnabla][INFO]: iter=1449 {Training error}=0.0\n",
            "2023-02-12 14:07:32,447 [nnabla][INFO]: iter=1459 {Training loss}=0.0007316929986700416\n",
            "2023-02-12 14:07:32,447 [nnabla][INFO]: iter=1459 {Training error}=0.0\n",
            "2023-02-12 14:07:32,489 [nnabla][INFO]: iter=1469 {Training loss}=0.0006771351909264922\n",
            "2023-02-12 14:07:32,489 [nnabla][INFO]: iter=1469 {Training error}=0.0\n",
            "2023-02-12 14:07:32,525 [nnabla][INFO]: iter=1479 {Training loss}=0.0006597749888896942\n",
            "2023-02-12 14:07:32,525 [nnabla][INFO]: iter=1479 {Training error}=0.0\n",
            "2023-02-12 14:07:32,561 [nnabla][INFO]: iter=1489 {Training loss}=0.000679172168020159\n",
            "2023-02-12 14:07:32,562 [nnabla][INFO]: iter=1489 {Training error}=0.0\n",
            "2023-02-12 14:07:32,598 [nnabla][INFO]: iter=1499 {Training loss}=0.0006314422353170812\n",
            "2023-02-12 14:07:32,598 [nnabla][INFO]: iter=1499 {Training error}=0.0\n",
            "2023-02-12 14:07:32,598 [nnabla][INFO]: iter=1499 {Training time}=0.391430139541626[sec/100iter] 9.074164390563965[sec]\n",
            "2023-02-12 14:07:32,623 [nnabla][INFO]: iter=1500 {Test error}=0.0234375\n",
            "2023-02-12 14:07:32,663 [nnabla][INFO]: iter=1509 {Training loss}=0.0006596170132979751\n",
            "2023-02-12 14:07:32,663 [nnabla][INFO]: iter=1509 {Training error}=0.0\n",
            "2023-02-12 14:07:32,698 [nnabla][INFO]: iter=1519 {Training loss}=0.0006099363672547042\n",
            "2023-02-12 14:07:32,698 [nnabla][INFO]: iter=1519 {Training error}=0.0\n",
            "2023-02-12 14:07:32,738 [nnabla][INFO]: iter=1529 {Training loss}=0.0006317860097624362\n",
            "2023-02-12 14:07:32,739 [nnabla][INFO]: iter=1529 {Training error}=0.0\n",
            "2023-02-12 14:07:32,774 [nnabla][INFO]: iter=1539 {Training loss}=0.0005949867190793157\n",
            "2023-02-12 14:07:32,774 [nnabla][INFO]: iter=1539 {Training error}=0.0\n",
            "2023-02-12 14:07:32,810 [nnabla][INFO]: iter=1549 {Training loss}=0.0005972577491775155\n",
            "2023-02-12 14:07:32,810 [nnabla][INFO]: iter=1549 {Training error}=0.0\n",
            "2023-02-12 14:07:32,846 [nnabla][INFO]: iter=1559 {Training loss}=0.0005982444854453206\n",
            "2023-02-12 14:07:32,846 [nnabla][INFO]: iter=1559 {Training error}=0.0\n",
            "2023-02-12 14:07:32,882 [nnabla][INFO]: iter=1569 {Training loss}=0.0005753585137426853\n",
            "2023-02-12 14:07:32,882 [nnabla][INFO]: iter=1569 {Training error}=0.0\n",
            "2023-02-12 14:07:32,919 [nnabla][INFO]: iter=1579 {Training loss}=0.0005330269923433661\n",
            "2023-02-12 14:07:32,920 [nnabla][INFO]: iter=1579 {Training error}=0.0\n",
            "2023-02-12 14:07:32,956 [nnabla][INFO]: iter=1589 {Training loss}=0.0005443646805360913\n",
            "2023-02-12 14:07:32,957 [nnabla][INFO]: iter=1589 {Training error}=0.0\n",
            "2023-02-12 14:07:32,992 [nnabla][INFO]: iter=1599 {Training loss}=0.0005346434190869331\n",
            "2023-02-12 14:07:32,993 [nnabla][INFO]: iter=1599 {Training error}=0.0\n",
            "2023-02-12 14:07:32,993 [nnabla][INFO]: iter=1599 {Training time}=0.3951072692871094[sec/100iter] 9.469271659851074[sec]\n",
            "2023-02-12 14:07:33,019 [nnabla][INFO]: iter=1600 {Test error}=0.02265625\n",
            "2023-02-12 14:07:33,055 [nnabla][INFO]: iter=1609 {Training loss}=0.0005310677806846797\n",
            "2023-02-12 14:07:33,055 [nnabla][INFO]: iter=1609 {Training error}=0.0\n",
            "2023-02-12 14:07:33,091 [nnabla][INFO]: iter=1619 {Training loss}=0.0005131965735927224\n",
            "2023-02-12 14:07:33,091 [nnabla][INFO]: iter=1619 {Training error}=0.0\n",
            "2023-02-12 14:07:33,129 [nnabla][INFO]: iter=1629 {Training loss}=0.000503081944771111\n",
            "2023-02-12 14:07:33,129 [nnabla][INFO]: iter=1629 {Training error}=0.0\n",
            "2023-02-12 14:07:33,164 [nnabla][INFO]: iter=1639 {Training loss}=0.0004759000730700791\n",
            "2023-02-12 14:07:33,165 [nnabla][INFO]: iter=1639 {Training error}=0.0\n",
            "2023-02-12 14:07:33,201 [nnabla][INFO]: iter=1649 {Training loss}=0.0004688236804213375\n",
            "2023-02-12 14:07:33,202 [nnabla][INFO]: iter=1649 {Training error}=0.0\n",
            "2023-02-12 14:07:33,237 [nnabla][INFO]: iter=1659 {Training loss}=0.00045283566578291357\n",
            "2023-02-12 14:07:33,237 [nnabla][INFO]: iter=1659 {Training error}=0.0\n",
            "2023-02-12 14:07:33,277 [nnabla][INFO]: iter=1669 {Training loss}=0.0004757475107908249\n",
            "2023-02-12 14:07:33,277 [nnabla][INFO]: iter=1669 {Training error}=0.0\n",
            "2023-02-12 14:07:33,312 [nnabla][INFO]: iter=1679 {Training loss}=0.00046241888776421547\n",
            "2023-02-12 14:07:33,312 [nnabla][INFO]: iter=1679 {Training error}=0.0\n",
            "2023-02-12 14:07:33,348 [nnabla][INFO]: iter=1689 {Training loss}=0.0004346862551756203\n",
            "2023-02-12 14:07:33,348 [nnabla][INFO]: iter=1689 {Training error}=0.0\n",
            "2023-02-12 14:07:33,383 [nnabla][INFO]: iter=1699 {Training loss}=0.00043746078154072165\n",
            "2023-02-12 14:07:33,384 [nnabla][INFO]: iter=1699 {Training error}=0.0\n",
            "2023-02-12 14:07:33,384 [nnabla][INFO]: iter=1699 {Training time}=0.39068603515625[sec/100iter] 9.859957695007324[sec]\n",
            "2023-02-12 14:07:33,405 [nnabla][INFO]: iter=1700 {Test error}=0.0234375\n",
            "2023-02-12 14:07:33,442 [nnabla][INFO]: iter=1709 {Training loss}=0.0004294667742215097\n",
            "2023-02-12 14:07:33,442 [nnabla][INFO]: iter=1709 {Training error}=0.0\n",
            "2023-02-12 14:07:33,477 [nnabla][INFO]: iter=1719 {Training loss}=0.00042603822657838464\n",
            "2023-02-12 14:07:33,477 [nnabla][INFO]: iter=1719 {Training error}=0.0\n",
            "2023-02-12 14:07:33,513 [nnabla][INFO]: iter=1729 {Training loss}=0.00042781600495800376\n",
            "2023-02-12 14:07:33,513 [nnabla][INFO]: iter=1729 {Training error}=0.0\n",
            "2023-02-12 14:07:33,549 [nnabla][INFO]: iter=1739 {Training loss}=0.0003941089962609112\n",
            "2023-02-12 14:07:33,549 [nnabla][INFO]: iter=1739 {Training error}=0.0\n",
            "2023-02-12 14:07:33,589 [nnabla][INFO]: iter=1749 {Training loss}=0.0003888371866196394\n",
            "2023-02-12 14:07:33,589 [nnabla][INFO]: iter=1749 {Training error}=0.0\n",
            "2023-02-12 14:07:33,624 [nnabla][INFO]: iter=1759 {Training loss}=0.00039737019687891006\n",
            "2023-02-12 14:07:33,625 [nnabla][INFO]: iter=1759 {Training error}=0.0\n",
            "2023-02-12 14:07:33,664 [nnabla][INFO]: iter=1769 {Training loss}=0.00037971718120388687\n",
            "2023-02-12 14:07:33,665 [nnabla][INFO]: iter=1769 {Training error}=0.0\n",
            "2023-02-12 14:07:33,700 [nnabla][INFO]: iter=1779 {Training loss}=0.0003780874249059707\n",
            "2023-02-12 14:07:33,700 [nnabla][INFO]: iter=1779 {Training error}=0.0\n",
            "2023-02-12 14:07:33,739 [nnabla][INFO]: iter=1789 {Training loss}=0.0003649332793429494\n",
            "2023-02-12 14:07:33,739 [nnabla][INFO]: iter=1789 {Training error}=0.0\n",
            "2023-02-12 14:07:33,775 [nnabla][INFO]: iter=1799 {Training loss}=0.0003726711729541421\n",
            "2023-02-12 14:07:33,775 [nnabla][INFO]: iter=1799 {Training error}=0.0\n",
            "2023-02-12 14:07:33,775 [nnabla][INFO]: iter=1799 {Training time}=0.3912630081176758[sec/100iter] 10.251220703125[sec]\n",
            "2023-02-12 14:07:33,796 [nnabla][INFO]: iter=1800 {Test error}=0.0234375\n",
            "2023-02-12 14:07:33,832 [nnabla][INFO]: iter=1809 {Training loss}=0.0003346493176650256\n",
            "2023-02-12 14:07:33,832 [nnabla][INFO]: iter=1809 {Training error}=0.0\n",
            "2023-02-12 14:07:33,867 [nnabla][INFO]: iter=1819 {Training loss}=0.00036337776691652834\n",
            "2023-02-12 14:07:33,868 [nnabla][INFO]: iter=1819 {Training error}=0.0\n",
            "2023-02-12 14:07:33,911 [nnabla][INFO]: iter=1829 {Training loss}=0.000352448143530637\n",
            "2023-02-12 14:07:33,911 [nnabla][INFO]: iter=1829 {Training error}=0.0\n",
            "2023-02-12 14:07:33,947 [nnabla][INFO]: iter=1839 {Training loss}=0.00035938728251494467\n",
            "2023-02-12 14:07:33,947 [nnabla][INFO]: iter=1839 {Training error}=0.0\n",
            "2023-02-12 14:07:33,982 [nnabla][INFO]: iter=1849 {Training loss}=0.00034374429378658533\n",
            "2023-02-12 14:07:33,983 [nnabla][INFO]: iter=1849 {Training error}=0.0\n",
            "2023-02-12 14:07:34,018 [nnabla][INFO]: iter=1859 {Training loss}=0.00032100858516059816\n",
            "2023-02-12 14:07:34,018 [nnabla][INFO]: iter=1859 {Training error}=0.0\n",
            "2023-02-12 14:07:34,056 [nnabla][INFO]: iter=1869 {Training loss}=0.0003241656522732228\n",
            "2023-02-12 14:07:34,056 [nnabla][INFO]: iter=1869 {Training error}=0.0\n",
            "2023-02-12 14:07:34,092 [nnabla][INFO]: iter=1879 {Training loss}=0.00031832544482313097\n",
            "2023-02-12 14:07:34,092 [nnabla][INFO]: iter=1879 {Training error}=0.0\n",
            "2023-02-12 14:07:34,128 [nnabla][INFO]: iter=1889 {Training loss}=0.00031223107362166047\n",
            "2023-02-12 14:07:34,128 [nnabla][INFO]: iter=1889 {Training error}=0.0\n",
            "2023-02-12 14:07:34,167 [nnabla][INFO]: iter=1899 {Training loss}=0.00030485051684081554\n",
            "2023-02-12 14:07:34,168 [nnabla][INFO]: iter=1899 {Training error}=0.0\n",
            "2023-02-12 14:07:34,168 [nnabla][INFO]: iter=1899 {Training time}=0.39273929595947266[sec/100iter] 10.643959999084473[sec]\n",
            "2023-02-12 14:07:34,188 [nnabla][INFO]: iter=1900 {Test error}=0.0234375\n",
            "2023-02-12 14:07:34,225 [nnabla][INFO]: iter=1909 {Training loss}=0.0003079311572946608\n",
            "2023-02-12 14:07:34,225 [nnabla][INFO]: iter=1909 {Training error}=0.0\n",
            "2023-02-12 14:07:34,263 [nnabla][INFO]: iter=1919 {Training loss}=0.00029606561292894185\n",
            "2023-02-12 14:07:34,264 [nnabla][INFO]: iter=1919 {Training error}=0.0\n",
            "2023-02-12 14:07:34,298 [nnabla][INFO]: iter=1929 {Training loss}=0.00029671634547412395\n",
            "2023-02-12 14:07:34,299 [nnabla][INFO]: iter=1929 {Training error}=0.0\n",
            "2023-02-12 14:07:34,334 [nnabla][INFO]: iter=1939 {Training loss}=0.0002876582439057529\n",
            "2023-02-12 14:07:34,334 [nnabla][INFO]: iter=1939 {Training error}=0.0\n",
            "2023-02-12 14:07:34,370 [nnabla][INFO]: iter=1949 {Training loss}=0.0002718234318308532\n",
            "2023-02-12 14:07:34,370 [nnabla][INFO]: iter=1949 {Training error}=0.0\n",
            "2023-02-12 14:07:34,411 [nnabla][INFO]: iter=1959 {Training loss}=0.00028713190113194287\n",
            "2023-02-12 14:07:34,411 [nnabla][INFO]: iter=1959 {Training error}=0.0\n",
            "2023-02-12 14:07:34,448 [nnabla][INFO]: iter=1969 {Training loss}=0.000271602941211313\n",
            "2023-02-12 14:07:34,448 [nnabla][INFO]: iter=1969 {Training error}=0.0\n",
            "2023-02-12 14:07:34,485 [nnabla][INFO]: iter=1979 {Training loss}=0.0002651072572916746\n",
            "2023-02-12 14:07:34,485 [nnabla][INFO]: iter=1979 {Training error}=0.0\n",
            "2023-02-12 14:07:34,522 [nnabla][INFO]: iter=1989 {Training loss}=0.0002720752381719649\n",
            "2023-02-12 14:07:34,522 [nnabla][INFO]: iter=1989 {Training error}=0.0\n",
            "2023-02-12 14:07:34,559 [nnabla][INFO]: iter=1999 {Training loss}=0.00025940584600903094\n",
            "2023-02-12 14:07:34,559 [nnabla][INFO]: iter=1999 {Training error}=0.0\n",
            "2023-02-12 14:07:34,560 [nnabla][INFO]: iter=1999 {Training time}=0.3917577266693115[sec/100iter] 11.035717725753784[sec]\n",
            "2023-02-12 14:07:34,581 [nnabla][INFO]: iter=2000 {Test error}=0.0234375\n",
            "2023-02-12 14:07:34,597 [nnabla][INFO]: Solver state save (.h5): output/states_2000.h5\n",
            "2023-02-12 14:07:34,605 [nnabla][INFO]: Parameter save (.h5): output/params_2000.h5\n",
            "2023-02-12 14:07:34,606 [nnabla][INFO]: Checkpoint save (.json): output/checkpoint_2000.json\n",
            "2023-02-12 14:07:34,643 [nnabla][INFO]: iter=2009 {Training loss}=0.0002622016763780266\n",
            "2023-02-12 14:07:34,643 [nnabla][INFO]: iter=2009 {Training error}=0.0\n",
            "2023-02-12 14:07:34,683 [nnabla][INFO]: iter=2019 {Training loss}=0.0002597153070382774\n",
            "2023-02-12 14:07:34,683 [nnabla][INFO]: iter=2019 {Training error}=0.0\n",
            "2023-02-12 14:07:34,718 [nnabla][INFO]: iter=2029 {Training loss}=0.0002473351778462529\n",
            "2023-02-12 14:07:34,719 [nnabla][INFO]: iter=2029 {Training error}=0.0\n",
            "2023-02-12 14:07:34,757 [nnabla][INFO]: iter=2039 {Training loss}=0.00024372148618567735\n",
            "2023-02-12 14:07:34,757 [nnabla][INFO]: iter=2039 {Training error}=0.0\n",
            "2023-02-12 14:07:34,793 [nnabla][INFO]: iter=2049 {Training loss}=0.0002452452026773244\n",
            "2023-02-12 14:07:34,793 [nnabla][INFO]: iter=2049 {Training error}=0.0\n",
            "2023-02-12 14:07:34,830 [nnabla][INFO]: iter=2059 {Training loss}=0.00023753245477564633\n",
            "2023-02-12 14:07:34,831 [nnabla][INFO]: iter=2059 {Training error}=0.0\n",
            "2023-02-12 14:07:34,867 [nnabla][INFO]: iter=2069 {Training loss}=0.0002444703713990748\n",
            "2023-02-12 14:07:34,867 [nnabla][INFO]: iter=2069 {Training error}=0.0\n",
            "2023-02-12 14:07:34,905 [nnabla][INFO]: iter=2079 {Training loss}=0.00022888777311891317\n",
            "2023-02-12 14:07:34,905 [nnabla][INFO]: iter=2079 {Training error}=0.0\n",
            "2023-02-12 14:07:34,942 [nnabla][INFO]: iter=2089 {Training loss}=0.00022690018522553146\n",
            "2023-02-12 14:07:34,943 [nnabla][INFO]: iter=2089 {Training error}=0.0\n",
            "2023-02-12 14:07:34,982 [nnabla][INFO]: iter=2099 {Training loss}=0.00022841067402623594\n",
            "2023-02-12 14:07:34,982 [nnabla][INFO]: iter=2099 {Training error}=0.0\n",
            "2023-02-12 14:07:34,982 [nnabla][INFO]: iter=2099 {Training time}=0.42253923416137695[sec/100iter] 11.458256959915161[sec]\n",
            "2023-02-12 14:07:35,013 [nnabla][INFO]: iter=2100 {Test error}=0.02265625\n",
            "2023-02-12 14:07:35,060 [nnabla][INFO]: iter=2109 {Training loss}=0.00022703432478010654\n",
            "2023-02-12 14:07:35,063 [nnabla][INFO]: iter=2109 {Training error}=0.0\n",
            "2023-02-12 14:07:35,114 [nnabla][INFO]: iter=2119 {Training loss}=0.0002177300921175629\n",
            "2023-02-12 14:07:35,114 [nnabla][INFO]: iter=2119 {Training error}=0.0\n",
            "2023-02-12 14:07:35,160 [nnabla][INFO]: iter=2129 {Training loss}=0.00021125006605871022\n",
            "2023-02-12 14:07:35,161 [nnabla][INFO]: iter=2129 {Training error}=0.0\n",
            "2023-02-12 14:07:35,206 [nnabla][INFO]: iter=2139 {Training loss}=0.0002154035901185125\n",
            "2023-02-12 14:07:35,206 [nnabla][INFO]: iter=2139 {Training error}=0.0\n",
            "2023-02-12 14:07:35,251 [nnabla][INFO]: iter=2149 {Training loss}=0.00021122556063346565\n",
            "2023-02-12 14:07:35,251 [nnabla][INFO]: iter=2149 {Training error}=0.0\n",
            "2023-02-12 14:07:35,299 [nnabla][INFO]: iter=2159 {Training loss}=0.00021271550212986767\n",
            "2023-02-12 14:07:35,300 [nnabla][INFO]: iter=2159 {Training error}=0.0\n",
            "2023-02-12 14:07:35,340 [nnabla][INFO]: iter=2169 {Training loss}=0.00020191613293718547\n",
            "2023-02-12 14:07:35,340 [nnabla][INFO]: iter=2169 {Training error}=0.0\n",
            "2023-02-12 14:07:35,385 [nnabla][INFO]: iter=2179 {Training loss}=0.00020240685262251645\n",
            "2023-02-12 14:07:35,386 [nnabla][INFO]: iter=2179 {Training error}=0.0\n",
            "2023-02-12 14:07:35,427 [nnabla][INFO]: iter=2189 {Training loss}=0.0001997749350266531\n",
            "2023-02-12 14:07:35,427 [nnabla][INFO]: iter=2189 {Training error}=0.0\n",
            "2023-02-12 14:07:35,469 [nnabla][INFO]: iter=2199 {Training loss}=0.00019911726121790707\n",
            "2023-02-12 14:07:35,469 [nnabla][INFO]: iter=2199 {Training error}=0.0\n",
            "2023-02-12 14:07:35,469 [nnabla][INFO]: iter=2199 {Training time}=0.4870305061340332[sec/100iter] 11.945287466049194[sec]\n",
            "2023-02-12 14:07:35,498 [nnabla][INFO]: iter=2200 {Test error}=0.0234375\n",
            "2023-02-12 14:07:35,538 [nnabla][INFO]: iter=2209 {Training loss}=0.0001877104223240167\n",
            "2023-02-12 14:07:35,538 [nnabla][INFO]: iter=2209 {Training error}=0.0\n",
            "2023-02-12 14:07:35,580 [nnabla][INFO]: iter=2219 {Training loss}=0.00019184444681741297\n",
            "2023-02-12 14:07:35,580 [nnabla][INFO]: iter=2219 {Training error}=0.0\n",
            "2023-02-12 14:07:35,620 [nnabla][INFO]: iter=2229 {Training loss}=0.00019196637731511146\n",
            "2023-02-12 14:07:35,620 [nnabla][INFO]: iter=2229 {Training error}=0.0\n",
            "2023-02-12 14:07:35,660 [nnabla][INFO]: iter=2239 {Training loss}=0.000190684077097103\n",
            "2023-02-12 14:07:35,660 [nnabla][INFO]: iter=2239 {Training error}=0.0\n",
            "2023-02-12 14:07:35,716 [nnabla][INFO]: iter=2249 {Training loss}=0.0001809288514778018\n",
            "2023-02-12 14:07:35,717 [nnabla][INFO]: iter=2249 {Training error}=0.0\n",
            "2023-02-12 14:07:35,760 [nnabla][INFO]: iter=2259 {Training loss}=0.00018989437376149\n",
            "2023-02-12 14:07:35,760 [nnabla][INFO]: iter=2259 {Training error}=0.0\n",
            "2023-02-12 14:07:35,800 [nnabla][INFO]: iter=2269 {Training loss}=0.00017991797358263284\n",
            "2023-02-12 14:07:35,800 [nnabla][INFO]: iter=2269 {Training error}=0.0\n",
            "2023-02-12 14:07:35,840 [nnabla][INFO]: iter=2279 {Training loss}=0.00017054277122952044\n",
            "2023-02-12 14:07:35,840 [nnabla][INFO]: iter=2279 {Training error}=0.0\n",
            "2023-02-12 14:07:35,889 [nnabla][INFO]: iter=2289 {Training loss}=0.00018056368571706116\n",
            "2023-02-12 14:07:35,890 [nnabla][INFO]: iter=2289 {Training error}=0.0\n",
            "2023-02-12 14:07:35,929 [nnabla][INFO]: iter=2299 {Training loss}=0.00016579777002334595\n",
            "2023-02-12 14:07:35,929 [nnabla][INFO]: iter=2299 {Training error}=0.0\n",
            "2023-02-12 14:07:35,929 [nnabla][INFO]: iter=2299 {Training time}=0.4601869583129883[sec/100iter] 12.405474424362183[sec]\n",
            "2023-02-12 14:07:35,955 [nnabla][INFO]: iter=2300 {Test error}=0.0234375\n",
            "2023-02-12 14:07:35,997 [nnabla][INFO]: iter=2309 {Training loss}=0.00017175768152810633\n",
            "2023-02-12 14:07:35,997 [nnabla][INFO]: iter=2309 {Training error}=0.0\n",
            "2023-02-12 14:07:36,038 [nnabla][INFO]: iter=2319 {Training loss}=0.0001651757484069094\n",
            "2023-02-12 14:07:36,039 [nnabla][INFO]: iter=2319 {Training error}=0.0\n",
            "2023-02-12 14:07:36,085 [nnabla][INFO]: iter=2329 {Training loss}=0.00016749094356782734\n",
            "2023-02-12 14:07:36,086 [nnabla][INFO]: iter=2329 {Training error}=0.0\n",
            "2023-02-12 14:07:36,128 [nnabla][INFO]: iter=2339 {Training loss}=0.00016583090473432094\n",
            "2023-02-12 14:07:36,128 [nnabla][INFO]: iter=2339 {Training error}=0.0\n",
            "2023-02-12 14:07:36,171 [nnabla][INFO]: iter=2349 {Training loss}=0.0001549969892948866\n",
            "2023-02-12 14:07:36,171 [nnabla][INFO]: iter=2349 {Training error}=0.0\n",
            "2023-02-12 14:07:36,214 [nnabla][INFO]: iter=2359 {Training loss}=0.0001602037373231724\n",
            "2023-02-12 14:07:36,214 [nnabla][INFO]: iter=2359 {Training error}=0.0\n",
            "2023-02-12 14:07:36,257 [nnabla][INFO]: iter=2369 {Training loss}=0.00015817042731214315\n",
            "2023-02-12 14:07:36,257 [nnabla][INFO]: iter=2369 {Training error}=0.0\n",
            "2023-02-12 14:07:36,299 [nnabla][INFO]: iter=2379 {Training loss}=0.00015448321937583387\n",
            "2023-02-12 14:07:36,299 [nnabla][INFO]: iter=2379 {Training error}=0.0\n",
            "2023-02-12 14:07:36,340 [nnabla][INFO]: iter=2389 {Training loss}=0.00015244480164255947\n",
            "2023-02-12 14:07:36,341 [nnabla][INFO]: iter=2389 {Training error}=0.0\n",
            "2023-02-12 14:07:36,387 [nnabla][INFO]: iter=2399 {Training loss}=0.00015213608276098967\n",
            "2023-02-12 14:07:36,388 [nnabla][INFO]: iter=2399 {Training error}=0.0\n",
            "2023-02-12 14:07:36,388 [nnabla][INFO]: iter=2399 {Training time}=0.4583461284637451[sec/100iter] 12.863820552825928[sec]\n",
            "2023-02-12 14:07:36,413 [nnabla][INFO]: iter=2400 {Test error}=0.0234375\n",
            "2023-02-12 14:07:36,454 [nnabla][INFO]: iter=2409 {Training loss}=0.00014562904834747314\n",
            "2023-02-12 14:07:36,455 [nnabla][INFO]: iter=2409 {Training error}=0.0\n",
            "2023-02-12 14:07:36,496 [nnabla][INFO]: iter=2419 {Training loss}=0.0001493110175943002\n",
            "2023-02-12 14:07:36,496 [nnabla][INFO]: iter=2419 {Training error}=0.0\n",
            "2023-02-12 14:07:36,538 [nnabla][INFO]: iter=2429 {Training loss}=0.00014769869449082762\n",
            "2023-02-12 14:07:36,539 [nnabla][INFO]: iter=2429 {Training error}=0.0\n",
            "2023-02-12 14:07:36,582 [nnabla][INFO]: iter=2439 {Training loss}=0.00014115581871010363\n",
            "2023-02-12 14:07:36,582 [nnabla][INFO]: iter=2439 {Training error}=0.0\n",
            "2023-02-12 14:07:36,625 [nnabla][INFO]: iter=2449 {Training loss}=0.00014335516607388854\n",
            "2023-02-12 14:07:36,625 [nnabla][INFO]: iter=2449 {Training error}=0.0\n",
            "2023-02-12 14:07:36,665 [nnabla][INFO]: iter=2459 {Training loss}=0.00013958227646071464\n",
            "2023-02-12 14:07:36,665 [nnabla][INFO]: iter=2459 {Training error}=0.0\n",
            "2023-02-12 14:07:36,712 [nnabla][INFO]: iter=2469 {Training loss}=0.00014173708041198552\n",
            "2023-02-12 14:07:36,712 [nnabla][INFO]: iter=2469 {Training error}=0.0\n",
            "2023-02-12 14:07:36,763 [nnabla][INFO]: iter=2479 {Training loss}=0.00014081130211707205\n",
            "2023-02-12 14:07:36,763 [nnabla][INFO]: iter=2479 {Training error}=0.0\n",
            "2023-02-12 14:07:36,806 [nnabla][INFO]: iter=2489 {Training loss}=0.00013190186291467398\n",
            "2023-02-12 14:07:36,806 [nnabla][INFO]: iter=2489 {Training error}=0.0\n",
            "2023-02-12 14:07:36,851 [nnabla][INFO]: iter=2499 {Training loss}=0.00013058078184258193\n",
            "2023-02-12 14:07:36,851 [nnabla][INFO]: iter=2499 {Training error}=0.0\n",
            "2023-02-12 14:07:36,852 [nnabla][INFO]: iter=2499 {Training time}=0.463909387588501[sec/100iter] 13.327729940414429[sec]\n",
            "2023-02-12 14:07:36,879 [nnabla][INFO]: iter=2500 {Test error}=0.02265625\n",
            "2023-02-12 14:07:36,920 [nnabla][INFO]: iter=2509 {Training loss}=0.00013291130017023534\n",
            "2023-02-12 14:07:36,920 [nnabla][INFO]: iter=2509 {Training error}=0.0\n",
            "2023-02-12 14:07:36,965 [nnabla][INFO]: iter=2519 {Training loss}=0.00013295005192048848\n",
            "2023-02-12 14:07:36,965 [nnabla][INFO]: iter=2519 {Training error}=0.0\n",
            "2023-02-12 14:07:37,011 [nnabla][INFO]: iter=2529 {Training loss}=0.00012890383368358016\n",
            "2023-02-12 14:07:37,011 [nnabla][INFO]: iter=2529 {Training error}=0.0\n",
            "2023-02-12 14:07:37,051 [nnabla][INFO]: iter=2539 {Training loss}=0.00013022703933529556\n",
            "2023-02-12 14:07:37,052 [nnabla][INFO]: iter=2539 {Training error}=0.0\n",
            "2023-02-12 14:07:37,099 [nnabla][INFO]: iter=2549 {Training loss}=0.0001273884263355285\n",
            "2023-02-12 14:07:37,100 [nnabla][INFO]: iter=2549 {Training error}=0.0\n",
            "2023-02-12 14:07:37,141 [nnabla][INFO]: iter=2559 {Training loss}=0.0001203962747240439\n",
            "2023-02-12 14:07:37,141 [nnabla][INFO]: iter=2559 {Training error}=0.0\n",
            "2023-02-12 14:07:37,187 [nnabla][INFO]: iter=2569 {Training loss}=0.00012816285016015172\n",
            "2023-02-12 14:07:37,187 [nnabla][INFO]: iter=2569 {Training error}=0.0\n",
            "2023-02-12 14:07:37,229 [nnabla][INFO]: iter=2579 {Training loss}=0.00011846036795759574\n",
            "2023-02-12 14:07:37,229 [nnabla][INFO]: iter=2579 {Training error}=0.0\n",
            "2023-02-12 14:07:37,271 [nnabla][INFO]: iter=2589 {Training loss}=0.00011999278649454936\n",
            "2023-02-12 14:07:37,271 [nnabla][INFO]: iter=2589 {Training error}=0.0\n",
            "2023-02-12 14:07:37,313 [nnabla][INFO]: iter=2599 {Training loss}=0.00012053798127453774\n",
            "2023-02-12 14:07:37,313 [nnabla][INFO]: iter=2599 {Training error}=0.0\n",
            "2023-02-12 14:07:37,313 [nnabla][INFO]: iter=2599 {Training time}=0.4619109630584717[sec/100iter] 13.7896409034729[sec]\n",
            "2023-02-12 14:07:37,340 [nnabla][INFO]: iter=2600 {Test error}=0.0234375\n",
            "2023-02-12 14:07:37,385 [nnabla][INFO]: iter=2609 {Training loss}=0.00011478675151010975\n",
            "2023-02-12 14:07:37,385 [nnabla][INFO]: iter=2609 {Training error}=0.0\n",
            "2023-02-12 14:07:37,435 [nnabla][INFO]: iter=2619 {Training loss}=0.00011825429101008922\n",
            "2023-02-12 14:07:37,435 [nnabla][INFO]: iter=2619 {Training error}=0.0\n",
            "2023-02-12 14:07:37,478 [nnabla][INFO]: iter=2629 {Training loss}=0.00011481541150715202\n",
            "2023-02-12 14:07:37,479 [nnabla][INFO]: iter=2629 {Training error}=0.0\n",
            "2023-02-12 14:07:37,521 [nnabla][INFO]: iter=2639 {Training loss}=0.00011657540744636208\n",
            "2023-02-12 14:07:37,521 [nnabla][INFO]: iter=2639 {Training error}=0.0\n",
            "2023-02-12 14:07:37,564 [nnabla][INFO]: iter=2649 {Training loss}=0.00011007767170667648\n",
            "2023-02-12 14:07:37,565 [nnabla][INFO]: iter=2649 {Training error}=0.0\n",
            "2023-02-12 14:07:37,611 [nnabla][INFO]: iter=2659 {Training loss}=0.00010799204756040126\n",
            "2023-02-12 14:07:37,612 [nnabla][INFO]: iter=2659 {Training error}=0.0\n",
            "2023-02-12 14:07:37,653 [nnabla][INFO]: iter=2669 {Training loss}=0.00011309461842756718\n",
            "2023-02-12 14:07:37,654 [nnabla][INFO]: iter=2669 {Training error}=0.0\n",
            "2023-02-12 14:07:37,698 [nnabla][INFO]: iter=2679 {Training loss}=0.00010898255277425051\n",
            "2023-02-12 14:07:37,699 [nnabla][INFO]: iter=2679 {Training error}=0.0\n",
            "2023-02-12 14:07:37,745 [nnabla][INFO]: iter=2689 {Training loss}=0.00010553713946137577\n",
            "2023-02-12 14:07:37,746 [nnabla][INFO]: iter=2689 {Training error}=0.0\n",
            "2023-02-12 14:07:37,795 [nnabla][INFO]: iter=2699 {Training loss}=0.00010457423923071474\n",
            "2023-02-12 14:07:37,796 [nnabla][INFO]: iter=2699 {Training error}=0.0\n",
            "2023-02-12 14:07:37,796 [nnabla][INFO]: iter=2699 {Training time}=0.4823470115661621[sec/100iter] 14.271987915039062[sec]\n",
            "2023-02-12 14:07:37,822 [nnabla][INFO]: iter=2700 {Test error}=0.0234375\n",
            "2023-02-12 14:07:37,866 [nnabla][INFO]: iter=2709 {Training loss}=0.00010729514906415716\n",
            "2023-02-12 14:07:37,866 [nnabla][INFO]: iter=2709 {Training error}=0.0\n",
            "2023-02-12 14:07:37,916 [nnabla][INFO]: iter=2719 {Training loss}=0.00010111008305102587\n",
            "2023-02-12 14:07:37,916 [nnabla][INFO]: iter=2719 {Training error}=0.0\n",
            "2023-02-12 14:07:37,959 [nnabla][INFO]: iter=2729 {Training loss}=0.00010786040365928784\n",
            "2023-02-12 14:07:37,959 [nnabla][INFO]: iter=2729 {Training error}=0.0\n",
            "2023-02-12 14:07:38,004 [nnabla][INFO]: iter=2739 {Training loss}=9.812100324779749e-05\n",
            "2023-02-12 14:07:38,004 [nnabla][INFO]: iter=2739 {Training error}=0.0\n",
            "2023-02-12 14:07:38,047 [nnabla][INFO]: iter=2749 {Training loss}=9.721683454699814e-05\n",
            "2023-02-12 14:07:38,047 [nnabla][INFO]: iter=2749 {Training error}=0.0\n",
            "2023-02-12 14:07:38,091 [nnabla][INFO]: iter=2759 {Training loss}=0.00010245702287647873\n",
            "2023-02-12 14:07:38,092 [nnabla][INFO]: iter=2759 {Training error}=0.0\n",
            "2023-02-12 14:07:38,134 [nnabla][INFO]: iter=2769 {Training loss}=9.853546362137422e-05\n",
            "2023-02-12 14:07:38,135 [nnabla][INFO]: iter=2769 {Training error}=0.0\n",
            "2023-02-12 14:07:38,179 [nnabla][INFO]: iter=2779 {Training loss}=9.547278750687838e-05\n",
            "2023-02-12 14:07:38,179 [nnabla][INFO]: iter=2779 {Training error}=0.0\n",
            "2023-02-12 14:07:38,225 [nnabla][INFO]: iter=2789 {Training loss}=9.647432307247072e-05\n",
            "2023-02-12 14:07:38,225 [nnabla][INFO]: iter=2789 {Training error}=0.0\n",
            "2023-02-12 14:07:38,269 [nnabla][INFO]: iter=2799 {Training loss}=9.576466254657134e-05\n",
            "2023-02-12 14:07:38,269 [nnabla][INFO]: iter=2799 {Training error}=0.0\n",
            "2023-02-12 14:07:38,269 [nnabla][INFO]: iter=2799 {Training time}=0.47349023818969727[sec/100iter] 14.74547815322876[sec]\n",
            "2023-02-12 14:07:38,299 [nnabla][INFO]: iter=2800 {Test error}=0.0234375\n",
            "2023-02-12 14:07:38,342 [nnabla][INFO]: iter=2809 {Training loss}=9.182342182612047e-05\n",
            "2023-02-12 14:07:38,342 [nnabla][INFO]: iter=2809 {Training error}=0.0\n",
            "2023-02-12 14:07:38,385 [nnabla][INFO]: iter=2819 {Training loss}=9.362727723782882e-05\n",
            "2023-02-12 14:07:38,386 [nnabla][INFO]: iter=2819 {Training error}=0.0\n",
            "2023-02-12 14:07:38,430 [nnabla][INFO]: iter=2829 {Training loss}=9.079281880985945e-05\n",
            "2023-02-12 14:07:38,430 [nnabla][INFO]: iter=2829 {Training error}=0.0\n",
            "2023-02-12 14:07:38,474 [nnabla][INFO]: iter=2839 {Training loss}=9.100007446249947e-05\n",
            "2023-02-12 14:07:38,474 [nnabla][INFO]: iter=2839 {Training error}=0.0\n",
            "2023-02-12 14:07:38,520 [nnabla][INFO]: iter=2849 {Training loss}=9.205227252095938e-05\n",
            "2023-02-12 14:07:38,520 [nnabla][INFO]: iter=2849 {Training error}=0.0\n",
            "2023-02-12 14:07:38,565 [nnabla][INFO]: iter=2859 {Training loss}=8.729072578717023e-05\n",
            "2023-02-12 14:07:38,565 [nnabla][INFO]: iter=2859 {Training error}=0.0\n",
            "2023-02-12 14:07:38,617 [nnabla][INFO]: iter=2869 {Training loss}=8.948128379415721e-05\n",
            "2023-02-12 14:07:38,617 [nnabla][INFO]: iter=2869 {Training error}=0.0\n",
            "2023-02-12 14:07:38,663 [nnabla][INFO]: iter=2879 {Training loss}=8.85528716025874e-05\n",
            "2023-02-12 14:07:38,663 [nnabla][INFO]: iter=2879 {Training error}=0.0\n",
            "2023-02-12 14:07:38,711 [nnabla][INFO]: iter=2889 {Training loss}=8.515865192748606e-05\n",
            "2023-02-12 14:07:38,711 [nnabla][INFO]: iter=2889 {Training error}=0.0\n",
            "2023-02-12 14:07:38,756 [nnabla][INFO]: iter=2899 {Training loss}=8.766561222728342e-05\n",
            "2023-02-12 14:07:38,756 [nnabla][INFO]: iter=2899 {Training error}=0.0\n",
            "2023-02-12 14:07:38,756 [nnabla][INFO]: iter=2899 {Training time}=0.48683619499206543[sec/100iter] 15.232314348220825[sec]\n",
            "2023-02-12 14:07:38,784 [nnabla][INFO]: iter=2900 {Test error}=0.02265625\n",
            "2023-02-12 14:07:38,837 [nnabla][INFO]: iter=2909 {Training loss}=8.367943519260734e-05\n",
            "2023-02-12 14:07:38,838 [nnabla][INFO]: iter=2909 {Training error}=0.0\n",
            "2023-02-12 14:07:38,884 [nnabla][INFO]: iter=2919 {Training loss}=8.296262967633083e-05\n",
            "2023-02-12 14:07:38,884 [nnabla][INFO]: iter=2919 {Training error}=0.0\n",
            "2023-02-12 14:07:38,930 [nnabla][INFO]: iter=2929 {Training loss}=8.425024134339765e-05\n",
            "2023-02-12 14:07:38,930 [nnabla][INFO]: iter=2929 {Training error}=0.0\n",
            "2023-02-12 14:07:38,973 [nnabla][INFO]: iter=2939 {Training loss}=7.96861932030879e-05\n",
            "2023-02-12 14:07:38,973 [nnabla][INFO]: iter=2939 {Training error}=0.0\n",
            "2023-02-12 14:07:39,020 [nnabla][INFO]: iter=2949 {Training loss}=7.918527990113944e-05\n",
            "2023-02-12 14:07:39,021 [nnabla][INFO]: iter=2949 {Training error}=0.0\n",
            "2023-02-12 14:07:39,065 [nnabla][INFO]: iter=2959 {Training loss}=8.318048639921471e-05\n",
            "2023-02-12 14:07:39,066 [nnabla][INFO]: iter=2959 {Training error}=0.0\n",
            "2023-02-12 14:07:39,111 [nnabla][INFO]: iter=2969 {Training loss}=7.805521454429254e-05\n",
            "2023-02-12 14:07:39,111 [nnabla][INFO]: iter=2969 {Training error}=0.0\n",
            "2023-02-12 14:07:39,155 [nnabla][INFO]: iter=2979 {Training loss}=7.45578872738406e-05\n",
            "2023-02-12 14:07:39,156 [nnabla][INFO]: iter=2979 {Training error}=0.0\n",
            "2023-02-12 14:07:39,200 [nnabla][INFO]: iter=2989 {Training loss}=8.020204404601827e-05\n",
            "2023-02-12 14:07:39,200 [nnabla][INFO]: iter=2989 {Training error}=0.0\n",
            "2023-02-12 14:07:39,245 [nnabla][INFO]: iter=2999 {Training loss}=7.606616418343037e-05\n",
            "2023-02-12 14:07:39,245 [nnabla][INFO]: iter=2999 {Training error}=0.0\n",
            "2023-02-12 14:07:39,245 [nnabla][INFO]: iter=2999 {Training time}=0.48894524574279785[sec/100iter] 15.721259593963623[sec]\n",
            "2023-02-12 14:07:39,273 [nnabla][INFO]: iter=3000 {Test error}=0.0234375\n",
            "2023-02-12 14:07:39,295 [nnabla][INFO]: Solver state save (.h5): output/states_3000.h5\n",
            "2023-02-12 14:07:39,308 [nnabla][INFO]: Parameter save (.h5): output/params_3000.h5\n",
            "2023-02-12 14:07:39,308 [nnabla][INFO]: Checkpoint save (.json): output/checkpoint_3000.json\n",
            "2023-02-12 14:07:39,353 [nnabla][INFO]: iter=3009 {Training loss}=7.746744813630357e-05\n",
            "2023-02-12 14:07:39,354 [nnabla][INFO]: iter=3009 {Training error}=0.0\n",
            "2023-02-12 14:07:39,400 [nnabla][INFO]: iter=3019 {Training loss}=7.701257709413767e-05\n",
            "2023-02-12 14:07:39,400 [nnabla][INFO]: iter=3019 {Training error}=0.0\n",
            "2023-02-12 14:07:39,445 [nnabla][INFO]: iter=3029 {Training loss}=7.084630487952381e-05\n",
            "2023-02-12 14:07:39,446 [nnabla][INFO]: iter=3029 {Training error}=0.0\n",
            "2023-02-12 14:07:39,490 [nnabla][INFO]: iter=3039 {Training loss}=7.532201561843976e-05\n",
            "2023-02-12 14:07:39,491 [nnabla][INFO]: iter=3039 {Training error}=0.0\n",
            "2023-02-12 14:07:39,539 [nnabla][INFO]: iter=3049 {Training loss}=7.37128866603598e-05\n",
            "2023-02-12 14:07:39,539 [nnabla][INFO]: iter=3049 {Training error}=0.0\n",
            "2023-02-12 14:07:39,583 [nnabla][INFO]: iter=3059 {Training loss}=7.188753806985915e-05\n",
            "2023-02-12 14:07:39,583 [nnabla][INFO]: iter=3059 {Training error}=0.0\n",
            "2023-02-12 14:07:39,633 [nnabla][INFO]: iter=3069 {Training loss}=7.14312627678737e-05\n",
            "2023-02-12 14:07:39,634 [nnabla][INFO]: iter=3069 {Training error}=0.0\n",
            "2023-02-12 14:07:39,676 [nnabla][INFO]: iter=3079 {Training loss}=7.088333222782239e-05\n",
            "2023-02-12 14:07:39,676 [nnabla][INFO]: iter=3079 {Training error}=0.0\n",
            "2023-02-12 14:07:39,721 [nnabla][INFO]: iter=3089 {Training loss}=6.892692181281745e-05\n",
            "2023-02-12 14:07:39,721 [nnabla][INFO]: iter=3089 {Training error}=0.0\n",
            "2023-02-12 14:07:39,763 [nnabla][INFO]: iter=3099 {Training loss}=7.001660560490564e-05\n",
            "2023-02-12 14:07:39,764 [nnabla][INFO]: iter=3099 {Training error}=0.0\n",
            "2023-02-12 14:07:39,764 [nnabla][INFO]: iter=3099 {Training time}=0.5185990333557129[sec/100iter] 16.239858627319336[sec]\n",
            "2023-02-12 14:07:39,791 [nnabla][INFO]: iter=3100 {Test error}=0.0234375\n",
            "2023-02-12 14:07:39,844 [nnabla][INFO]: iter=3109 {Training loss}=6.965683132875711e-05\n",
            "2023-02-12 14:07:39,847 [nnabla][INFO]: iter=3109 {Training error}=0.0\n",
            "2023-02-12 14:07:39,892 [nnabla][INFO]: iter=3119 {Training loss}=6.745957216480747e-05\n",
            "2023-02-12 14:07:39,894 [nnabla][INFO]: iter=3119 {Training error}=0.0\n",
            "2023-02-12 14:07:39,941 [nnabla][INFO]: iter=3129 {Training loss}=6.624936941079795e-05\n",
            "2023-02-12 14:07:39,942 [nnabla][INFO]: iter=3129 {Training error}=0.0\n",
            "2023-02-12 14:07:39,986 [nnabla][INFO]: iter=3139 {Training loss}=6.801269773859531e-05\n",
            "2023-02-12 14:07:39,987 [nnabla][INFO]: iter=3139 {Training error}=0.0\n",
            "2023-02-12 14:07:40,031 [nnabla][INFO]: iter=3149 {Training loss}=6.566393130924553e-05\n",
            "2023-02-12 14:07:40,032 [nnabla][INFO]: iter=3149 {Training error}=0.0\n",
            "2023-02-12 14:07:40,081 [nnabla][INFO]: iter=3159 {Training loss}=6.419976853067055e-05\n",
            "2023-02-12 14:07:40,081 [nnabla][INFO]: iter=3159 {Training error}=0.0\n",
            "2023-02-12 14:07:40,129 [nnabla][INFO]: iter=3169 {Training loss}=6.548576493514702e-05\n",
            "2023-02-12 14:07:40,130 [nnabla][INFO]: iter=3169 {Training error}=0.0\n",
            "2023-02-12 14:07:40,177 [nnabla][INFO]: iter=3179 {Training loss}=6.538134766742587e-05\n",
            "2023-02-12 14:07:40,177 [nnabla][INFO]: iter=3179 {Training error}=0.0\n",
            "2023-02-12 14:07:40,223 [nnabla][INFO]: iter=3189 {Training loss}=6.274923362070695e-05\n",
            "2023-02-12 14:07:40,224 [nnabla][INFO]: iter=3189 {Training error}=0.0\n",
            "2023-02-12 14:07:40,265 [nnabla][INFO]: iter=3199 {Training loss}=6.32911323918961e-05\n",
            "2023-02-12 14:07:40,265 [nnabla][INFO]: iter=3199 {Training error}=0.0\n",
            "2023-02-12 14:07:40,265 [nnabla][INFO]: iter=3199 {Training time}=0.5015058517456055[sec/100iter] 16.74136447906494[sec]\n",
            "2023-02-12 14:07:40,288 [nnabla][INFO]: iter=3200 {Test error}=0.0234375\n",
            "2023-02-12 14:07:40,327 [nnabla][INFO]: iter=3209 {Training loss}=6.091445538913831e-05\n",
            "2023-02-12 14:07:40,327 [nnabla][INFO]: iter=3209 {Training error}=0.0\n",
            "2023-02-12 14:07:40,367 [nnabla][INFO]: iter=3219 {Training loss}=6.191579450387508e-05\n",
            "2023-02-12 14:07:40,367 [nnabla][INFO]: iter=3219 {Training error}=0.0\n",
            "2023-02-12 14:07:40,405 [nnabla][INFO]: iter=3229 {Training loss}=6.106891669332981e-05\n",
            "2023-02-12 14:07:40,406 [nnabla][INFO]: iter=3229 {Training error}=0.0\n",
            "2023-02-12 14:07:40,452 [nnabla][INFO]: iter=3239 {Training loss}=5.868139487574808e-05\n",
            "2023-02-12 14:07:40,452 [nnabla][INFO]: iter=3239 {Training error}=0.0\n",
            "2023-02-12 14:07:40,491 [nnabla][INFO]: iter=3249 {Training loss}=5.9224646975053474e-05\n",
            "2023-02-12 14:07:40,492 [nnabla][INFO]: iter=3249 {Training error}=0.0\n",
            "2023-02-12 14:07:40,532 [nnabla][INFO]: iter=3259 {Training loss}=6.158428732305765e-05\n",
            "2023-02-12 14:07:40,533 [nnabla][INFO]: iter=3259 {Training error}=0.0\n",
            "2023-02-12 14:07:40,572 [nnabla][INFO]: iter=3269 {Training loss}=5.6816672440618277e-05\n",
            "2023-02-12 14:07:40,572 [nnabla][INFO]: iter=3269 {Training error}=0.0\n",
            "2023-02-12 14:07:40,611 [nnabla][INFO]: iter=3279 {Training loss}=5.859824159415439e-05\n",
            "2023-02-12 14:07:40,611 [nnabla][INFO]: iter=3279 {Training error}=0.0\n",
            "2023-02-12 14:07:40,650 [nnabla][INFO]: iter=3289 {Training loss}=5.73305478610564e-05\n",
            "2023-02-12 14:07:40,650 [nnabla][INFO]: iter=3289 {Training error}=0.0\n",
            "2023-02-12 14:07:40,692 [nnabla][INFO]: iter=3299 {Training loss}=5.6799435697030276e-05\n",
            "2023-02-12 14:07:40,693 [nnabla][INFO]: iter=3299 {Training error}=0.0\n",
            "2023-02-12 14:07:40,693 [nnabla][INFO]: iter=3299 {Training time}=0.42760467529296875[sec/100iter] 17.16896915435791[sec]\n",
            "2023-02-12 14:07:40,715 [nnabla][INFO]: iter=3300 {Test error}=0.02265625\n",
            "2023-02-12 14:07:40,759 [nnabla][INFO]: iter=3309 {Training loss}=5.5790071201045066e-05\n",
            "2023-02-12 14:07:40,759 [nnabla][INFO]: iter=3309 {Training error}=0.0\n",
            "2023-02-12 14:07:40,797 [nnabla][INFO]: iter=3319 {Training loss}=5.690368197974749e-05\n",
            "2023-02-12 14:07:40,797 [nnabla][INFO]: iter=3319 {Training error}=0.0\n",
            "2023-02-12 14:07:40,835 [nnabla][INFO]: iter=3329 {Training loss}=5.450962999020703e-05\n",
            "2023-02-12 14:07:40,836 [nnabla][INFO]: iter=3329 {Training error}=0.0\n",
            "2023-02-12 14:07:40,877 [nnabla][INFO]: iter=3339 {Training loss}=5.4897165682632476e-05\n",
            "2023-02-12 14:07:40,877 [nnabla][INFO]: iter=3339 {Training error}=0.0\n",
            "2023-02-12 14:07:40,916 [nnabla][INFO]: iter=3349 {Training loss}=5.382896779337898e-05\n",
            "2023-02-12 14:07:40,917 [nnabla][INFO]: iter=3349 {Training error}=0.0\n",
            "2023-02-12 14:07:40,963 [nnabla][INFO]: iter=3359 {Training loss}=5.437801155494526e-05\n",
            "2023-02-12 14:07:40,964 [nnabla][INFO]: iter=3359 {Training error}=0.0\n",
            "2023-02-12 14:07:41,002 [nnabla][INFO]: iter=3369 {Training loss}=5.247141234576702e-05\n",
            "2023-02-12 14:07:41,002 [nnabla][INFO]: iter=3369 {Training error}=0.0\n",
            "2023-02-12 14:07:41,040 [nnabla][INFO]: iter=3379 {Training loss}=5.314950249157846e-05\n",
            "2023-02-12 14:07:41,040 [nnabla][INFO]: iter=3379 {Training error}=0.0\n",
            "2023-02-12 14:07:41,078 [nnabla][INFO]: iter=3389 {Training loss}=5.145510658621788e-05\n",
            "2023-02-12 14:07:41,078 [nnabla][INFO]: iter=3389 {Training error}=0.0\n",
            "2023-02-12 14:07:41,116 [nnabla][INFO]: iter=3399 {Training loss}=5.096049790154211e-05\n",
            "2023-02-12 14:07:41,116 [nnabla][INFO]: iter=3399 {Training error}=0.0\n",
            "2023-02-12 14:07:41,116 [nnabla][INFO]: iter=3399 {Training time}=0.42360353469848633[sec/100iter] 17.592572689056396[sec]\n",
            "2023-02-12 14:07:41,139 [nnabla][INFO]: iter=3400 {Test error}=0.0234375\n",
            "2023-02-12 14:07:41,177 [nnabla][INFO]: iter=3409 {Training loss}=5.321927528711967e-05\n",
            "2023-02-12 14:07:41,178 [nnabla][INFO]: iter=3409 {Training error}=0.0\n",
            "2023-02-12 14:07:41,216 [nnabla][INFO]: iter=3419 {Training loss}=4.912769509246573e-05\n",
            "2023-02-12 14:07:41,216 [nnabla][INFO]: iter=3419 {Training error}=0.0\n",
            "2023-02-12 14:07:41,255 [nnabla][INFO]: iter=3429 {Training loss}=5.1084945880575106e-05\n",
            "2023-02-12 14:07:41,256 [nnabla][INFO]: iter=3429 {Training error}=0.0\n",
            "2023-02-12 14:07:41,293 [nnabla][INFO]: iter=3439 {Training loss}=4.929153874400072e-05\n",
            "2023-02-12 14:07:41,293 [nnabla][INFO]: iter=3439 {Training error}=0.0\n",
            "2023-02-12 14:07:41,331 [nnabla][INFO]: iter=3449 {Training loss}=5.004720151191577e-05\n",
            "2023-02-12 14:07:41,332 [nnabla][INFO]: iter=3449 {Training error}=0.0\n",
            "2023-02-12 14:07:41,372 [nnabla][INFO]: iter=3459 {Training loss}=4.802087278221734e-05\n",
            "2023-02-12 14:07:41,372 [nnabla][INFO]: iter=3459 {Training error}=0.0\n",
            "2023-02-12 14:07:41,409 [nnabla][INFO]: iter=3469 {Training loss}=4.885626185568981e-05\n",
            "2023-02-12 14:07:41,410 [nnabla][INFO]: iter=3469 {Training error}=0.0\n",
            "2023-02-12 14:07:41,450 [nnabla][INFO]: iter=3479 {Training loss}=4.836771040572785e-05\n",
            "2023-02-12 14:07:41,450 [nnabla][INFO]: iter=3479 {Training error}=0.0\n",
            "2023-02-12 14:07:41,487 [nnabla][INFO]: iter=3489 {Training loss}=4.79755035485141e-05\n",
            "2023-02-12 14:07:41,487 [nnabla][INFO]: iter=3489 {Training error}=0.0\n",
            "2023-02-12 14:07:41,525 [nnabla][INFO]: iter=3499 {Training loss}=4.618627281161025e-05\n",
            "2023-02-12 14:07:41,525 [nnabla][INFO]: iter=3499 {Training error}=0.0\n",
            "2023-02-12 14:07:41,526 [nnabla][INFO]: iter=3499 {Training time}=0.40918707847595215[sec/100iter] 18.00175976753235[sec]\n",
            "2023-02-12 14:07:41,550 [nnabla][INFO]: iter=3500 {Test error}=0.0234375\n",
            "2023-02-12 14:07:41,588 [nnabla][INFO]: iter=3509 {Training loss}=4.629480463336222e-05\n",
            "2023-02-12 14:07:41,588 [nnabla][INFO]: iter=3509 {Training error}=0.0\n",
            "2023-02-12 14:07:41,629 [nnabla][INFO]: iter=3519 {Training loss}=4.708619962912053e-05\n",
            "2023-02-12 14:07:41,629 [nnabla][INFO]: iter=3519 {Training error}=0.0\n",
            "2023-02-12 14:07:41,667 [nnabla][INFO]: iter=3529 {Training loss}=4.5547283662017435e-05\n",
            "2023-02-12 14:07:41,668 [nnabla][INFO]: iter=3529 {Training error}=0.0\n",
            "2023-02-12 14:07:41,705 [nnabla][INFO]: iter=3539 {Training loss}=4.528397766989656e-05\n",
            "2023-02-12 14:07:41,705 [nnabla][INFO]: iter=3539 {Training error}=0.0\n",
            "2023-02-12 14:07:41,743 [nnabla][INFO]: iter=3549 {Training loss}=4.4145235733594745e-05\n",
            "2023-02-12 14:07:41,743 [nnabla][INFO]: iter=3549 {Training error}=0.0\n",
            "2023-02-12 14:07:41,781 [nnabla][INFO]: iter=3559 {Training loss}=4.4512387830764055e-05\n",
            "2023-02-12 14:07:41,781 [nnabla][INFO]: iter=3559 {Training error}=0.0\n",
            "2023-02-12 14:07:41,818 [nnabla][INFO]: iter=3569 {Training loss}=4.53891116194427e-05\n",
            "2023-02-12 14:07:41,818 [nnabla][INFO]: iter=3569 {Training error}=0.0\n",
            "2023-02-12 14:07:41,855 [nnabla][INFO]: iter=3579 {Training loss}=4.395427095005289e-05\n",
            "2023-02-12 14:07:41,855 [nnabla][INFO]: iter=3579 {Training error}=0.0\n",
            "2023-02-12 14:07:41,896 [nnabla][INFO]: iter=3589 {Training loss}=4.264817835064605e-05\n",
            "2023-02-12 14:07:41,896 [nnabla][INFO]: iter=3589 {Training error}=0.0\n",
            "2023-02-12 14:07:41,934 [nnabla][INFO]: iter=3599 {Training loss}=4.391367838252336e-05\n",
            "2023-02-12 14:07:41,934 [nnabla][INFO]: iter=3599 {Training error}=0.0\n",
            "2023-02-12 14:07:41,934 [nnabla][INFO]: iter=3599 {Training time}=0.40830516815185547[sec/100iter] 18.410064935684204[sec]\n",
            "2023-02-12 14:07:41,963 [nnabla][INFO]: iter=3600 {Test error}=0.0234375\n",
            "2023-02-12 14:07:42,001 [nnabla][INFO]: iter=3609 {Training loss}=4.1185303416568786e-05\n",
            "2023-02-12 14:07:42,002 [nnabla][INFO]: iter=3609 {Training error}=0.0\n",
            "2023-02-12 14:07:42,039 [nnabla][INFO]: iter=3619 {Training loss}=4.3757645471487194e-05\n",
            "2023-02-12 14:07:42,040 [nnabla][INFO]: iter=3619 {Training error}=0.0\n",
            "2023-02-12 14:07:42,081 [nnabla][INFO]: iter=3629 {Training loss}=4.2361614760011435e-05\n",
            "2023-02-12 14:07:42,082 [nnabla][INFO]: iter=3629 {Training error}=0.0\n",
            "2023-02-12 14:07:42,119 [nnabla][INFO]: iter=3639 {Training loss}=4.136976713198237e-05\n",
            "2023-02-12 14:07:42,119 [nnabla][INFO]: iter=3639 {Training error}=0.0\n",
            "2023-02-12 14:07:42,158 [nnabla][INFO]: iter=3649 {Training loss}=4.050870120408945e-05\n",
            "2023-02-12 14:07:42,159 [nnabla][INFO]: iter=3649 {Training error}=0.0\n",
            "2023-02-12 14:07:42,195 [nnabla][INFO]: iter=3659 {Training loss}=4.185335274087265e-05\n",
            "2023-02-12 14:07:42,196 [nnabla][INFO]: iter=3659 {Training error}=0.0\n",
            "2023-02-12 14:07:42,233 [nnabla][INFO]: iter=3669 {Training loss}=4.0677601646166295e-05\n",
            "2023-02-12 14:07:42,233 [nnabla][INFO]: iter=3669 {Training error}=0.0\n",
            "2023-02-12 14:07:42,270 [nnabla][INFO]: iter=3679 {Training loss}=3.960313188144937e-05\n",
            "2023-02-12 14:07:42,270 [nnabla][INFO]: iter=3679 {Training error}=0.0\n",
            "2023-02-12 14:07:42,307 [nnabla][INFO]: iter=3689 {Training loss}=4.040016938233748e-05\n",
            "2023-02-12 14:07:42,308 [nnabla][INFO]: iter=3689 {Training error}=0.0\n",
            "2023-02-12 14:07:42,348 [nnabla][INFO]: iter=3699 {Training loss}=3.962887421948835e-05\n",
            "2023-02-12 14:07:42,348 [nnabla][INFO]: iter=3699 {Training error}=0.0\n",
            "2023-02-12 14:07:42,348 [nnabla][INFO]: iter=3699 {Training time}=0.41399264335632324[sec/100iter] 18.824057579040527[sec]\n",
            "2023-02-12 14:07:42,370 [nnabla][INFO]: iter=3700 {Test error}=0.0234375\n",
            "2023-02-12 14:07:42,407 [nnabla][INFO]: iter=3709 {Training loss}=3.815458694589324e-05\n",
            "2023-02-12 14:07:42,408 [nnabla][INFO]: iter=3709 {Training error}=0.0\n",
            "2023-02-12 14:07:42,444 [nnabla][INFO]: iter=3719 {Training loss}=3.968879900639877e-05\n",
            "2023-02-12 14:07:42,445 [nnabla][INFO]: iter=3719 {Training error}=0.0\n",
            "2023-02-12 14:07:42,484 [nnabla][INFO]: iter=3729 {Training loss}=3.8104397390270606e-05\n",
            "2023-02-12 14:07:42,484 [nnabla][INFO]: iter=3729 {Training error}=0.0\n",
            "2023-02-12 14:07:42,522 [nnabla][INFO]: iter=3739 {Training loss}=3.778502286877483e-05\n",
            "2023-02-12 14:07:42,522 [nnabla][INFO]: iter=3739 {Training error}=0.0\n",
            "2023-02-12 14:07:42,559 [nnabla][INFO]: iter=3749 {Training loss}=3.838445991277695e-05\n",
            "2023-02-12 14:07:42,560 [nnabla][INFO]: iter=3749 {Training error}=0.0\n",
            "2023-02-12 14:07:42,596 [nnabla][INFO]: iter=3759 {Training loss}=3.6908124457113445e-05\n",
            "2023-02-12 14:07:42,596 [nnabla][INFO]: iter=3759 {Training error}=0.0\n",
            "2023-02-12 14:07:42,633 [nnabla][INFO]: iter=3769 {Training loss}=3.642255251179449e-05\n",
            "2023-02-12 14:07:42,633 [nnabla][INFO]: iter=3769 {Training error}=0.0\n",
            "2023-02-12 14:07:42,669 [nnabla][INFO]: iter=3779 {Training loss}=3.7465986679308116e-05\n",
            "2023-02-12 14:07:42,669 [nnabla][INFO]: iter=3779 {Training error}=0.0\n",
            "2023-02-12 14:07:42,711 [nnabla][INFO]: iter=3789 {Training loss}=3.6040910345036536e-05\n",
            "2023-02-12 14:07:42,712 [nnabla][INFO]: iter=3789 {Training error}=0.0\n",
            "2023-02-12 14:07:42,748 [nnabla][INFO]: iter=3799 {Training loss}=3.670911610242911e-05\n",
            "2023-02-12 14:07:42,749 [nnabla][INFO]: iter=3799 {Training error}=0.0\n",
            "2023-02-12 14:07:42,749 [nnabla][INFO]: iter=3799 {Training time}=0.40104126930236816[sec/100iter] 19.225098848342896[sec]\n",
            "2023-02-12 14:07:42,772 [nnabla][INFO]: iter=3800 {Test error}=0.02265625\n",
            "2023-02-12 14:07:42,808 [nnabla][INFO]: iter=3809 {Training loss}=3.559310789569281e-05\n",
            "2023-02-12 14:07:42,808 [nnabla][INFO]: iter=3809 {Training error}=0.0\n",
            "2023-02-12 14:07:42,844 [nnabla][INFO]: iter=3819 {Training loss}=3.617632683017291e-05\n",
            "2023-02-12 14:07:42,844 [nnabla][INFO]: iter=3819 {Training error}=0.0\n",
            "2023-02-12 14:07:42,879 [nnabla][INFO]: iter=3829 {Training loss}=3.5239572753198445e-05\n",
            "2023-02-12 14:07:42,880 [nnabla][INFO]: iter=3829 {Training error}=0.0\n",
            "2023-02-12 14:07:42,920 [nnabla][INFO]: iter=3839 {Training loss}=3.542638660292141e-05\n",
            "2023-02-12 14:07:42,921 [nnabla][INFO]: iter=3839 {Training error}=0.0\n",
            "2023-02-12 14:07:42,961 [nnabla][INFO]: iter=3849 {Training loss}=3.454718898865394e-05\n",
            "2023-02-12 14:07:42,961 [nnabla][INFO]: iter=3849 {Training error}=0.0\n",
            "2023-02-12 14:07:42,998 [nnabla][INFO]: iter=3859 {Training loss}=3.414699312997982e-05\n",
            "2023-02-12 14:07:42,998 [nnabla][INFO]: iter=3859 {Training error}=0.0\n",
            "2023-02-12 14:07:43,041 [nnabla][INFO]: iter=3869 {Training loss}=3.3708081900840625e-05\n",
            "2023-02-12 14:07:43,041 [nnabla][INFO]: iter=3869 {Training error}=0.0\n",
            "2023-02-12 14:07:43,078 [nnabla][INFO]: iter=3879 {Training loss}=3.44262043654453e-05\n",
            "2023-02-12 14:07:43,078 [nnabla][INFO]: iter=3879 {Training error}=0.0\n",
            "2023-02-12 14:07:43,114 [nnabla][INFO]: iter=3889 {Training loss}=3.296141585451551e-05\n",
            "2023-02-12 14:07:43,115 [nnabla][INFO]: iter=3889 {Training error}=0.0\n",
            "2023-02-12 14:07:43,151 [nnabla][INFO]: iter=3899 {Training loss}=3.2781841582618654e-05\n",
            "2023-02-12 14:07:43,151 [nnabla][INFO]: iter=3899 {Training error}=0.0\n",
            "2023-02-12 14:07:43,151 [nnabla][INFO]: iter=3899 {Training time}=0.4019601345062256[sec/100iter] 19.62705898284912[sec]\n",
            "2023-02-12 14:07:43,172 [nnabla][INFO]: iter=3900 {Test error}=0.0234375\n",
            "2023-02-12 14:07:43,208 [nnabla][INFO]: iter=3909 {Training loss}=3.305474092485383e-05\n",
            "2023-02-12 14:07:43,209 [nnabla][INFO]: iter=3909 {Training error}=0.0\n",
            "2023-02-12 14:07:43,244 [nnabla][INFO]: iter=3919 {Training loss}=3.2710151572246104e-05\n",
            "2023-02-12 14:07:43,245 [nnabla][INFO]: iter=3919 {Training error}=0.0\n",
            "2023-02-12 14:07:43,289 [nnabla][INFO]: iter=3929 {Training loss}=3.388049663044512e-05\n",
            "2023-02-12 14:07:43,289 [nnabla][INFO]: iter=3929 {Training error}=0.0\n",
            "2023-02-12 14:07:43,325 [nnabla][INFO]: iter=3939 {Training loss}=3.203860978828743e-05\n",
            "2023-02-12 14:07:43,326 [nnabla][INFO]: iter=3939 {Training error}=0.0\n",
            "2023-02-12 14:07:43,361 [nnabla][INFO]: iter=3949 {Training loss}=3.1382231099996716e-05\n",
            "2023-02-12 14:07:43,362 [nnabla][INFO]: iter=3949 {Training error}=0.0\n",
            "2023-02-12 14:07:43,398 [nnabla][INFO]: iter=3959 {Training loss}=3.124850263702683e-05\n",
            "2023-02-12 14:07:43,398 [nnabla][INFO]: iter=3959 {Training error}=0.0\n",
            "2023-02-12 14:07:43,434 [nnabla][INFO]: iter=3969 {Training loss}=3.060843300772831e-05\n",
            "2023-02-12 14:07:43,434 [nnabla][INFO]: iter=3969 {Training error}=0.0\n",
            "2023-02-12 14:07:43,470 [nnabla][INFO]: iter=3979 {Training loss}=3.2365227525588125e-05\n",
            "2023-02-12 14:07:43,471 [nnabla][INFO]: iter=3979 {Training error}=0.0\n",
            "2023-02-12 14:07:43,507 [nnabla][INFO]: iter=3989 {Training loss}=3.134362123091705e-05\n",
            "2023-02-12 14:07:43,507 [nnabla][INFO]: iter=3989 {Training error}=0.0\n",
            "2023-02-12 14:07:43,545 [nnabla][INFO]: iter=3999 {Training loss}=2.939460682682693e-05\n",
            "2023-02-12 14:07:43,545 [nnabla][INFO]: iter=3999 {Training error}=0.0\n",
            "2023-02-12 14:07:43,545 [nnabla][INFO]: iter=3999 {Training time}=0.3944578170776367[sec/100iter] 20.021516799926758[sec]\n",
            "2023-02-12 14:07:43,570 [nnabla][INFO]: iter=4000 {Test error}=0.0234375\n",
            "2023-02-12 14:07:43,585 [nnabla][INFO]: Solver state save (.h5): output/states_4000.h5\n",
            "2023-02-12 14:07:43,593 [nnabla][INFO]: Parameter save (.h5): output/params_4000.h5\n",
            "2023-02-12 14:07:43,593 [nnabla][INFO]: Checkpoint save (.json): output/checkpoint_4000.json\n",
            "2023-02-12 14:07:43,630 [nnabla][INFO]: iter=4009 {Training loss}=3.0376852009794675e-05\n",
            "2023-02-12 14:07:43,630 [nnabla][INFO]: iter=4009 {Training error}=0.0\n",
            "2023-02-12 14:07:43,666 [nnabla][INFO]: iter=4019 {Training loss}=3.072550316574052e-05\n",
            "2023-02-12 14:07:43,667 [nnabla][INFO]: iter=4019 {Training error}=0.0\n",
            "2023-02-12 14:07:43,703 [nnabla][INFO]: iter=4029 {Training loss}=2.9608496333821677e-05\n",
            "2023-02-12 14:07:43,703 [nnabla][INFO]: iter=4029 {Training error}=0.0\n",
            "2023-02-12 14:07:43,739 [nnabla][INFO]: iter=4039 {Training loss}=3.0003129722899757e-05\n",
            "2023-02-12 14:07:43,740 [nnabla][INFO]: iter=4039 {Training error}=0.0\n",
            "2023-02-12 14:07:43,779 [nnabla][INFO]: iter=4049 {Training loss}=2.908722308347933e-05\n",
            "2023-02-12 14:07:43,779 [nnabla][INFO]: iter=4049 {Training error}=0.0\n",
            "2023-02-12 14:07:43,822 [nnabla][INFO]: iter=4059 {Training loss}=2.862593646568712e-05\n",
            "2023-02-12 14:07:43,822 [nnabla][INFO]: iter=4059 {Training error}=0.0\n",
            "2023-02-12 14:07:43,859 [nnabla][INFO]: iter=4069 {Training loss}=2.9084118068567477e-05\n",
            "2023-02-12 14:07:43,859 [nnabla][INFO]: iter=4069 {Training error}=0.0\n",
            "2023-02-12 14:07:43,896 [nnabla][INFO]: iter=4079 {Training loss}=2.9009703212068416e-05\n",
            "2023-02-12 14:07:43,896 [nnabla][INFO]: iter=4079 {Training error}=0.0\n",
            "2023-02-12 14:07:43,935 [nnabla][INFO]: iter=4089 {Training loss}=2.8121605282649398e-05\n",
            "2023-02-12 14:07:43,936 [nnabla][INFO]: iter=4089 {Training error}=0.0\n",
            "2023-02-12 14:07:43,973 [nnabla][INFO]: iter=4099 {Training loss}=2.770148785202764e-05\n",
            "2023-02-12 14:07:43,973 [nnabla][INFO]: iter=4099 {Training error}=0.0\n",
            "2023-02-12 14:07:43,973 [nnabla][INFO]: iter=4099 {Training time}=0.42815113067626953[sec/100iter] 20.449667930603027[sec]\n",
            "2023-02-12 14:07:43,994 [nnabla][INFO]: iter=4100 {Test error}=0.0234375\n",
            "2023-02-12 14:07:44,032 [nnabla][INFO]: iter=4109 {Training loss}=2.865685019060038e-05\n",
            "2023-02-12 14:07:44,032 [nnabla][INFO]: iter=4109 {Training error}=0.0\n",
            "2023-02-12 14:07:44,068 [nnabla][INFO]: iter=4119 {Training loss}=2.763991869869642e-05\n",
            "2023-02-12 14:07:44,069 [nnabla][INFO]: iter=4119 {Training error}=0.0\n",
            "2023-02-12 14:07:44,113 [nnabla][INFO]: iter=4129 {Training loss}=2.746382961049676e-05\n",
            "2023-02-12 14:07:44,113 [nnabla][INFO]: iter=4129 {Training error}=0.0\n",
            "2023-02-12 14:07:44,149 [nnabla][INFO]: iter=4139 {Training loss}=2.660656718944665e-05\n",
            "2023-02-12 14:07:44,149 [nnabla][INFO]: iter=4139 {Training error}=0.0\n",
            "2023-02-12 14:07:44,185 [nnabla][INFO]: iter=4149 {Training loss}=2.704560392885469e-05\n",
            "2023-02-12 14:07:44,186 [nnabla][INFO]: iter=4149 {Training error}=0.0\n",
            "2023-02-12 14:07:44,222 [nnabla][INFO]: iter=4159 {Training loss}=2.6682337193051353e-05\n",
            "2023-02-12 14:07:44,222 [nnabla][INFO]: iter=4159 {Training error}=0.0\n",
            "2023-02-12 14:07:44,258 [nnabla][INFO]: iter=4169 {Training loss}=2.671964102773927e-05\n",
            "2023-02-12 14:07:44,258 [nnabla][INFO]: iter=4169 {Training error}=0.0\n",
            "2023-02-12 14:07:44,296 [nnabla][INFO]: iter=4179 {Training loss}=2.6291620088159107e-05\n",
            "2023-02-12 14:07:44,296 [nnabla][INFO]: iter=4179 {Training error}=0.0\n",
            "2023-02-12 14:07:44,332 [nnabla][INFO]: iter=4189 {Training loss}=2.6559000616543926e-05\n",
            "2023-02-12 14:07:44,332 [nnabla][INFO]: iter=4189 {Training error}=0.0\n",
            "2023-02-12 14:07:44,376 [nnabla][INFO]: iter=4199 {Training loss}=2.5612307581468485e-05\n",
            "2023-02-12 14:07:44,376 [nnabla][INFO]: iter=4199 {Training error}=0.0\n",
            "2023-02-12 14:07:44,377 [nnabla][INFO]: iter=4199 {Training time}=0.4030642509460449[sec/100iter] 20.852732181549072[sec]\n",
            "2023-02-12 14:07:44,399 [nnabla][INFO]: iter=4200 {Test error}=0.02265625\n",
            "2023-02-12 14:07:44,436 [nnabla][INFO]: iter=4209 {Training loss}=2.5711473426781595e-05\n",
            "2023-02-12 14:07:44,436 [nnabla][INFO]: iter=4209 {Training error}=0.0\n",
            "2023-02-12 14:07:44,473 [nnabla][INFO]: iter=4219 {Training loss}=2.603112989163492e-05\n",
            "2023-02-12 14:07:44,473 [nnabla][INFO]: iter=4219 {Training error}=0.0\n",
            "2023-02-12 14:07:44,513 [nnabla][INFO]: iter=4229 {Training loss}=2.4702118025743403e-05\n",
            "2023-02-12 14:07:44,513 [nnabla][INFO]: iter=4229 {Training error}=0.0\n",
            "2023-02-12 14:07:44,550 [nnabla][INFO]: iter=4239 {Training loss}=2.5269113393733278e-05\n",
            "2023-02-12 14:07:44,550 [nnabla][INFO]: iter=4239 {Training error}=0.0\n",
            "2023-02-12 14:07:44,587 [nnabla][INFO]: iter=4249 {Training loss}=2.4810477043502033e-05\n",
            "2023-02-12 14:07:44,587 [nnabla][INFO]: iter=4249 {Training error}=0.0\n",
            "2023-02-12 14:07:44,623 [nnabla][INFO]: iter=4259 {Training loss}=2.4494511308148503e-05\n",
            "2023-02-12 14:07:44,623 [nnabla][INFO]: iter=4259 {Training error}=0.0\n",
            "2023-02-12 14:07:44,664 [nnabla][INFO]: iter=4269 {Training loss}=2.5153585738735273e-05\n",
            "2023-02-12 14:07:44,664 [nnabla][INFO]: iter=4269 {Training error}=0.0\n",
            "2023-02-12 14:07:44,700 [nnabla][INFO]: iter=4279 {Training loss}=2.3812404833734035e-05\n",
            "2023-02-12 14:07:44,700 [nnabla][INFO]: iter=4279 {Training error}=0.0\n",
            "2023-02-12 14:07:44,737 [nnabla][INFO]: iter=4289 {Training loss}=2.4982777176774107e-05\n",
            "2023-02-12 14:07:44,737 [nnabla][INFO]: iter=4289 {Training error}=0.0\n",
            "2023-02-12 14:07:44,774 [nnabla][INFO]: iter=4299 {Training loss}=2.3484910343540832e-05\n",
            "2023-02-12 14:07:44,775 [nnabla][INFO]: iter=4299 {Training error}=0.0\n",
            "2023-02-12 14:07:44,775 [nnabla][INFO]: iter=4299 {Training time}=0.3982055187225342[sec/100iter] 21.250937700271606[sec]\n",
            "2023-02-12 14:07:44,800 [nnabla][INFO]: iter=4300 {Test error}=0.0234375\n",
            "2023-02-12 14:07:44,836 [nnabla][INFO]: iter=4309 {Training loss}=2.425941056571901e-05\n",
            "2023-02-12 14:07:44,837 [nnabla][INFO]: iter=4309 {Training error}=0.0\n",
            "2023-02-12 14:07:44,873 [nnabla][INFO]: iter=4319 {Training loss}=2.349560054426547e-05\n",
            "2023-02-12 14:07:44,873 [nnabla][INFO]: iter=4319 {Training error}=0.0\n",
            "2023-02-12 14:07:44,916 [nnabla][INFO]: iter=4329 {Training loss}=2.3648586648050696e-05\n",
            "2023-02-12 14:07:44,917 [nnabla][INFO]: iter=4329 {Training error}=0.0\n",
            "2023-02-12 14:07:44,955 [nnabla][INFO]: iter=4339 {Training loss}=2.2330130377667956e-05\n",
            "2023-02-12 14:07:44,955 [nnabla][INFO]: iter=4339 {Training error}=0.0\n",
            "2023-02-12 14:07:44,993 [nnabla][INFO]: iter=4349 {Training loss}=2.4053602828644216e-05\n",
            "2023-02-12 14:07:44,993 [nnabla][INFO]: iter=4349 {Training error}=0.0\n",
            "2023-02-12 14:07:45,029 [nnabla][INFO]: iter=4359 {Training loss}=2.2556441763299517e-05\n",
            "2023-02-12 14:07:45,029 [nnabla][INFO]: iter=4359 {Training error}=0.0\n",
            "2023-02-12 14:07:45,066 [nnabla][INFO]: iter=4369 {Training loss}=2.2454749341704883e-05\n",
            "2023-02-12 14:07:45,066 [nnabla][INFO]: iter=4369 {Training error}=0.0\n",
            "2023-02-12 14:07:45,105 [nnabla][INFO]: iter=4379 {Training loss}=2.316821519343648e-05\n",
            "2023-02-12 14:07:45,105 [nnabla][INFO]: iter=4379 {Training error}=0.0\n",
            "2023-02-12 14:07:45,142 [nnabla][INFO]: iter=4389 {Training loss}=2.2073925720178522e-05\n",
            "2023-02-12 14:07:45,142 [nnabla][INFO]: iter=4389 {Training error}=0.0\n",
            "2023-02-12 14:07:45,186 [nnabla][INFO]: iter=4399 {Training loss}=2.173166285501793e-05\n",
            "2023-02-12 14:07:45,186 [nnabla][INFO]: iter=4399 {Training error}=0.0\n",
            "2023-02-12 14:07:45,187 [nnabla][INFO]: iter=4399 {Training time}=0.41185927391052246[sec/100iter] 21.66279697418213[sec]\n",
            "2023-02-12 14:07:45,210 [nnabla][INFO]: iter=4400 {Test error}=0.0234375\n",
            "2023-02-12 14:07:45,248 [nnabla][INFO]: iter=4409 {Training loss}=2.2425894712796435e-05\n",
            "2023-02-12 14:07:45,248 [nnabla][INFO]: iter=4409 {Training error}=0.0\n",
            "2023-02-12 14:07:45,286 [nnabla][INFO]: iter=4419 {Training loss}=2.1842302885488607e-05\n",
            "2023-02-12 14:07:45,287 [nnabla][INFO]: iter=4419 {Training error}=0.0\n",
            "2023-02-12 14:07:45,332 [nnabla][INFO]: iter=4429 {Training loss}=2.106488864228595e-05\n",
            "2023-02-12 14:07:45,332 [nnabla][INFO]: iter=4429 {Training error}=0.0\n",
            "2023-02-12 14:07:45,375 [nnabla][INFO]: iter=4439 {Training loss}=2.17759697989095e-05\n",
            "2023-02-12 14:07:45,375 [nnabla][INFO]: iter=4439 {Training error}=0.0\n",
            "2023-02-12 14:07:45,413 [nnabla][INFO]: iter=4449 {Training loss}=2.1301502783899195e-05\n",
            "2023-02-12 14:07:45,413 [nnabla][INFO]: iter=4449 {Training error}=0.0\n",
            "2023-02-12 14:07:45,450 [nnabla][INFO]: iter=4459 {Training loss}=2.1158373783691786e-05\n",
            "2023-02-12 14:07:45,450 [nnabla][INFO]: iter=4459 {Training error}=0.0\n",
            "2023-02-12 14:07:45,493 [nnabla][INFO]: iter=4469 {Training loss}=2.05448450287804e-05\n",
            "2023-02-12 14:07:45,493 [nnabla][INFO]: iter=4469 {Training error}=0.0\n",
            "2023-02-12 14:07:45,531 [nnabla][INFO]: iter=4479 {Training loss}=2.1204157746979035e-05\n",
            "2023-02-12 14:07:45,531 [nnabla][INFO]: iter=4479 {Training error}=0.0\n",
            "2023-02-12 14:07:45,569 [nnabla][INFO]: iter=4489 {Training loss}=2.091958958772011e-05\n",
            "2023-02-12 14:07:45,570 [nnabla][INFO]: iter=4489 {Training error}=0.0\n",
            "2023-02-12 14:07:45,611 [nnabla][INFO]: iter=4499 {Training loss}=2.0428773495950736e-05\n",
            "2023-02-12 14:07:45,611 [nnabla][INFO]: iter=4499 {Training error}=0.0\n",
            "2023-02-12 14:07:45,611 [nnabla][INFO]: iter=4499 {Training time}=0.42444586753845215[sec/100iter] 22.08724284172058[sec]\n",
            "2023-02-12 14:07:45,633 [nnabla][INFO]: iter=4500 {Test error}=0.0234375\n",
            "2023-02-12 14:07:45,671 [nnabla][INFO]: iter=4509 {Training loss}=2.019766361627262e-05\n",
            "2023-02-12 14:07:45,671 [nnabla][INFO]: iter=4509 {Training error}=0.0\n",
            "2023-02-12 14:07:45,707 [nnabla][INFO]: iter=4519 {Training loss}=2.0179788407403976e-05\n",
            "2023-02-12 14:07:45,707 [nnabla][INFO]: iter=4519 {Training error}=0.0\n",
            "2023-02-12 14:07:45,748 [nnabla][INFO]: iter=4529 {Training loss}=1.984903974516783e-05\n",
            "2023-02-12 14:07:45,748 [nnabla][INFO]: iter=4529 {Training error}=0.0\n",
            "2023-02-12 14:07:45,785 [nnabla][INFO]: iter=4539 {Training loss}=1.9554607206373475e-05\n",
            "2023-02-12 14:07:45,785 [nnabla][INFO]: iter=4539 {Training error}=0.0\n",
            "2023-02-12 14:07:45,824 [nnabla][INFO]: iter=4549 {Training loss}=2.0650924852816388e-05\n",
            "2023-02-12 14:07:45,825 [nnabla][INFO]: iter=4549 {Training error}=0.0\n",
            "2023-02-12 14:07:45,860 [nnabla][INFO]: iter=4559 {Training loss}=1.910020728246309e-05\n",
            "2023-02-12 14:07:45,860 [nnabla][INFO]: iter=4559 {Training error}=0.0\n",
            "2023-02-12 14:07:45,896 [nnabla][INFO]: iter=4569 {Training loss}=1.9506001990521327e-05\n",
            "2023-02-12 14:07:45,897 [nnabla][INFO]: iter=4569 {Training error}=0.0\n",
            "2023-02-12 14:07:45,932 [nnabla][INFO]: iter=4579 {Training loss}=1.9741633877856657e-05\n",
            "2023-02-12 14:07:45,932 [nnabla][INFO]: iter=4579 {Training error}=0.0\n",
            "2023-02-12 14:07:45,971 [nnabla][INFO]: iter=4589 {Training loss}=1.891950159915723e-05\n",
            "2023-02-12 14:07:45,971 [nnabla][INFO]: iter=4589 {Training error}=0.0\n",
            "2023-02-12 14:07:46,012 [nnabla][INFO]: iter=4599 {Training loss}=1.8829450709745288e-05\n",
            "2023-02-12 14:07:46,013 [nnabla][INFO]: iter=4599 {Training error}=0.0\n",
            "2023-02-12 14:07:46,013 [nnabla][INFO]: iter=4599 {Training time}=0.4017815589904785[sec/100iter] 22.48902440071106[sec]\n",
            "2023-02-12 14:07:46,034 [nnabla][INFO]: iter=4600 {Test error}=0.02265625\n",
            "2023-02-12 14:07:46,069 [nnabla][INFO]: iter=4609 {Training loss}=1.8949600416817702e-05\n",
            "2023-02-12 14:07:46,069 [nnabla][INFO]: iter=4609 {Training error}=0.0\n",
            "2023-02-12 14:07:46,106 [nnabla][INFO]: iter=4619 {Training loss}=1.8443290173308924e-05\n",
            "2023-02-12 14:07:46,107 [nnabla][INFO]: iter=4619 {Training error}=0.0\n",
            "2023-02-12 14:07:46,142 [nnabla][INFO]: iter=4629 {Training loss}=1.8614951841300353e-05\n",
            "2023-02-12 14:07:46,143 [nnabla][INFO]: iter=4629 {Training error}=0.0\n",
            "2023-02-12 14:07:46,178 [nnabla][INFO]: iter=4639 {Training loss}=1.857397728599608e-05\n",
            "2023-02-12 14:07:46,178 [nnabla][INFO]: iter=4639 {Training error}=0.0\n",
            "2023-02-12 14:07:46,215 [nnabla][INFO]: iter=4649 {Training loss}=1.8701586668612435e-05\n",
            "2023-02-12 14:07:46,215 [nnabla][INFO]: iter=4649 {Training error}=0.0\n",
            "2023-02-12 14:07:46,250 [nnabla][INFO]: iter=4659 {Training loss}=1.7511283658677712e-05\n",
            "2023-02-12 14:07:46,250 [nnabla][INFO]: iter=4659 {Training error}=0.0\n",
            "2023-02-12 14:07:46,291 [nnabla][INFO]: iter=4669 {Training loss}=1.8063399693346582e-05\n",
            "2023-02-12 14:07:46,292 [nnabla][INFO]: iter=4669 {Training error}=0.0\n",
            "2023-02-12 14:07:46,329 [nnabla][INFO]: iter=4679 {Training loss}=1.8231983631267212e-05\n",
            "2023-02-12 14:07:46,330 [nnabla][INFO]: iter=4679 {Training error}=0.0\n",
            "2023-02-12 14:07:46,365 [nnabla][INFO]: iter=4689 {Training loss}=1.737581260385923e-05\n",
            "2023-02-12 14:07:46,365 [nnabla][INFO]: iter=4689 {Training error}=0.0\n",
            "2023-02-12 14:07:46,401 [nnabla][INFO]: iter=4699 {Training loss}=1.788243571354542e-05\n",
            "2023-02-12 14:07:46,402 [nnabla][INFO]: iter=4699 {Training error}=0.0\n",
            "2023-02-12 14:07:46,402 [nnabla][INFO]: iter=4699 {Training time}=0.3890044689178467[sec/100iter] 22.878028869628906[sec]\n",
            "2023-02-12 14:07:46,423 [nnabla][INFO]: iter=4700 {Test error}=0.0234375\n",
            "2023-02-12 14:07:46,459 [nnabla][INFO]: iter=4709 {Training loss}=1.704515489109326e-05\n",
            "2023-02-12 14:07:46,459 [nnabla][INFO]: iter=4709 {Training error}=0.0\n",
            "2023-02-12 14:07:46,496 [nnabla][INFO]: iter=4719 {Training loss}=1.756102210492827e-05\n",
            "2023-02-12 14:07:46,496 [nnabla][INFO]: iter=4719 {Training error}=0.0\n",
            "2023-02-12 14:07:46,539 [nnabla][INFO]: iter=4729 {Training loss}=1.7105136066675186e-05\n",
            "2023-02-12 14:07:46,539 [nnabla][INFO]: iter=4729 {Training error}=0.0\n",
            "2023-02-12 14:07:46,576 [nnabla][INFO]: iter=4739 {Training loss}=1.7449978258810006e-05\n",
            "2023-02-12 14:07:46,577 [nnabla][INFO]: iter=4739 {Training error}=0.0\n",
            "2023-02-12 14:07:46,617 [nnabla][INFO]: iter=4749 {Training loss}=1.6633599443593994e-05\n",
            "2023-02-12 14:07:46,618 [nnabla][INFO]: iter=4749 {Training error}=0.0\n",
            "2023-02-12 14:07:46,654 [nnabla][INFO]: iter=4759 {Training loss}=1.6801015590317547e-05\n",
            "2023-02-12 14:07:46,654 [nnabla][INFO]: iter=4759 {Training error}=0.0\n",
            "2023-02-12 14:07:46,690 [nnabla][INFO]: iter=4769 {Training loss}=1.6801101082819514e-05\n",
            "2023-02-12 14:07:46,690 [nnabla][INFO]: iter=4769 {Training error}=0.0\n",
            "2023-02-12 14:07:46,726 [nnabla][INFO]: iter=4779 {Training loss}=1.621427873033099e-05\n",
            "2023-02-12 14:07:46,726 [nnabla][INFO]: iter=4779 {Training error}=0.0\n",
            "2023-02-12 14:07:46,762 [nnabla][INFO]: iter=4789 {Training loss}=1.6447747839265503e-05\n",
            "2023-02-12 14:07:46,763 [nnabla][INFO]: iter=4789 {Training error}=0.0\n",
            "2023-02-12 14:07:46,804 [nnabla][INFO]: iter=4799 {Training loss}=1.6607318684691563e-05\n",
            "2023-02-12 14:07:46,806 [nnabla][INFO]: iter=4799 {Training error}=0.0\n",
            "2023-02-12 14:07:46,806 [nnabla][INFO]: iter=4799 {Training time}=0.4043731689453125[sec/100iter] 23.28240203857422[sec]\n",
            "2023-02-12 14:07:46,829 [nnabla][INFO]: iter=4800 {Test error}=0.0234375\n",
            "2023-02-12 14:07:46,865 [nnabla][INFO]: iter=4809 {Training loss}=1.5659943528589793e-05\n",
            "2023-02-12 14:07:46,865 [nnabla][INFO]: iter=4809 {Training error}=0.0\n",
            "2023-02-12 14:07:46,901 [nnabla][INFO]: iter=4819 {Training loss}=1.644419171498157e-05\n",
            "2023-02-12 14:07:46,901 [nnabla][INFO]: iter=4819 {Training error}=0.0\n",
            "2023-02-12 14:07:46,938 [nnabla][INFO]: iter=4829 {Training loss}=1.5862709915381856e-05\n",
            "2023-02-12 14:07:46,938 [nnabla][INFO]: iter=4829 {Training error}=0.0\n",
            "2023-02-12 14:07:46,974 [nnabla][INFO]: iter=4839 {Training loss}=1.599152892595157e-05\n",
            "2023-02-12 14:07:46,974 [nnabla][INFO]: iter=4839 {Training error}=0.0\n",
            "2023-02-12 14:07:47,016 [nnabla][INFO]: iter=4849 {Training loss}=1.6012103515095077e-05\n",
            "2023-02-12 14:07:47,016 [nnabla][INFO]: iter=4849 {Training error}=0.0\n",
            "2023-02-12 14:07:47,056 [nnabla][INFO]: iter=4859 {Training loss}=1.5709607396274805e-05\n",
            "2023-02-12 14:07:47,056 [nnabla][INFO]: iter=4859 {Training error}=0.0\n",
            "2023-02-12 14:07:47,091 [nnabla][INFO]: iter=4869 {Training loss}=1.491546026954893e-05\n",
            "2023-02-12 14:07:47,092 [nnabla][INFO]: iter=4869 {Training error}=0.0\n",
            "2023-02-12 14:07:47,131 [nnabla][INFO]: iter=4879 {Training loss}=1.5214371160254814e-05\n",
            "2023-02-12 14:07:47,131 [nnabla][INFO]: iter=4879 {Training error}=0.0\n",
            "2023-02-12 14:07:47,167 [nnabla][INFO]: iter=4889 {Training loss}=1.5578765669488348e-05\n",
            "2023-02-12 14:07:47,167 [nnabla][INFO]: iter=4889 {Training error}=0.0\n",
            "2023-02-12 14:07:47,205 [nnabla][INFO]: iter=4899 {Training loss}=1.529678047518246e-05\n",
            "2023-02-12 14:07:47,205 [nnabla][INFO]: iter=4899 {Training error}=0.0\n",
            "2023-02-12 14:07:47,205 [nnabla][INFO]: iter=4899 {Training time}=0.3988037109375[sec/100iter] 23.68120574951172[sec]\n",
            "2023-02-12 14:07:47,226 [nnabla][INFO]: iter=4900 {Test error}=0.0234375\n",
            "2023-02-12 14:07:47,262 [nnabla][INFO]: iter=4909 {Training loss}=1.5057444215926807e-05\n",
            "2023-02-12 14:07:47,262 [nnabla][INFO]: iter=4909 {Training error}=0.0\n",
            "2023-02-12 14:07:47,298 [nnabla][INFO]: iter=4919 {Training loss}=1.5233294107019901e-05\n",
            "2023-02-12 14:07:47,298 [nnabla][INFO]: iter=4919 {Training error}=0.0\n",
            "2023-02-12 14:07:47,338 [nnabla][INFO]: iter=4929 {Training loss}=1.4735551303601824e-05\n",
            "2023-02-12 14:07:47,338 [nnabla][INFO]: iter=4929 {Training error}=0.0\n",
            "2023-02-12 14:07:47,382 [nnabla][INFO]: iter=4939 {Training loss}=1.4597229892387986e-05\n",
            "2023-02-12 14:07:47,382 [nnabla][INFO]: iter=4939 {Training error}=0.0\n",
            "2023-02-12 14:07:47,418 [nnabla][INFO]: iter=4949 {Training loss}=1.4714243661728688e-05\n",
            "2023-02-12 14:07:47,419 [nnabla][INFO]: iter=4949 {Training error}=0.0\n",
            "2023-02-12 14:07:47,455 [nnabla][INFO]: iter=4959 {Training loss}=1.4399241081264336e-05\n",
            "2023-02-12 14:07:47,455 [nnabla][INFO]: iter=4959 {Training error}=0.0\n",
            "2023-02-12 14:07:47,492 [nnabla][INFO]: iter=4969 {Training loss}=1.4156032193568535e-05\n",
            "2023-02-12 14:07:47,492 [nnabla][INFO]: iter=4969 {Training error}=0.0\n",
            "2023-02-12 14:07:47,529 [nnabla][INFO]: iter=4979 {Training loss}=1.4408268725674134e-05\n",
            "2023-02-12 14:07:47,529 [nnabla][INFO]: iter=4979 {Training error}=0.0\n",
            "2023-02-12 14:07:47,566 [nnabla][INFO]: iter=4989 {Training loss}=1.4025618838786613e-05\n",
            "2023-02-12 14:07:47,567 [nnabla][INFO]: iter=4989 {Training error}=0.0\n",
            "2023-02-12 14:07:47,603 [nnabla][INFO]: iter=4999 {Training loss}=1.4086757801123895e-05\n",
            "2023-02-12 14:07:47,604 [nnabla][INFO]: iter=4999 {Training error}=0.0\n",
            "2023-02-12 14:07:47,604 [nnabla][INFO]: iter=4999 {Training time}=0.3987593650817871[sec/100iter] 24.079965114593506[sec]\n",
            "2023-02-12 14:07:47,635 [nnabla][INFO]: iter=5000 {Test error}=0.0234375\n",
            "2023-02-12 14:07:47,650 [nnabla][INFO]: Solver state save (.h5): output/states_5000.h5\n",
            "2023-02-12 14:07:47,657 [nnabla][INFO]: Parameter save (.h5): output/params_5000.h5\n",
            "2023-02-12 14:07:47,658 [nnabla][INFO]: Checkpoint save (.json): output/checkpoint_5000.json\n",
            "2023-02-12 14:07:47,694 [nnabla][INFO]: iter=5009 {Training loss}=1.4276874026108999e-05\n",
            "2023-02-12 14:07:47,695 [nnabla][INFO]: iter=5009 {Training error}=0.0\n",
            "2023-02-12 14:07:47,731 [nnabla][INFO]: iter=5019 {Training loss}=1.3775726074527483e-05\n",
            "2023-02-12 14:07:47,731 [nnabla][INFO]: iter=5019 {Training error}=0.0\n",
            "2023-02-12 14:07:47,768 [nnabla][INFO]: iter=5029 {Training loss}=1.3414221939456183e-05\n",
            "2023-02-12 14:07:47,768 [nnabla][INFO]: iter=5029 {Training error}=0.0\n",
            "2023-02-12 14:07:47,804 [nnabla][INFO]: iter=5039 {Training loss}=1.3958725503471214e-05\n",
            "2023-02-12 14:07:47,804 [nnabla][INFO]: iter=5039 {Training error}=0.0\n",
            "2023-02-12 14:07:47,845 [nnabla][INFO]: iter=5049 {Training loss}=1.3441950613923836e-05\n",
            "2023-02-12 14:07:47,845 [nnabla][INFO]: iter=5049 {Training error}=0.0\n",
            "2023-02-12 14:07:47,884 [nnabla][INFO]: iter=5059 {Training loss}=1.3438749192573596e-05\n",
            "2023-02-12 14:07:47,884 [nnabla][INFO]: iter=5059 {Training error}=0.0\n",
            "2023-02-12 14:07:47,925 [nnabla][INFO]: iter=5069 {Training loss}=1.3542626220441889e-05\n",
            "2023-02-12 14:07:47,925 [nnabla][INFO]: iter=5069 {Training error}=0.0\n",
            "2023-02-12 14:07:47,962 [nnabla][INFO]: iter=5079 {Training loss}=1.3237753591965884e-05\n",
            "2023-02-12 14:07:47,962 [nnabla][INFO]: iter=5079 {Training error}=0.0\n",
            "2023-02-12 14:07:47,999 [nnabla][INFO]: iter=5089 {Training loss}=1.3225658221927006e-05\n",
            "2023-02-12 14:07:47,999 [nnabla][INFO]: iter=5089 {Training error}=0.0\n",
            "2023-02-12 14:07:48,041 [nnabla][INFO]: iter=5099 {Training loss}=1.3106368896842469e-05\n",
            "2023-02-12 14:07:48,041 [nnabla][INFO]: iter=5099 {Training error}=0.0\n",
            "2023-02-12 14:07:48,041 [nnabla][INFO]: iter=5099 {Training time}=0.4371352195739746[sec/100iter] 24.51710033416748[sec]\n",
            "2023-02-12 14:07:48,063 [nnabla][INFO]: iter=5100 {Test error}=0.02265625\n",
            "2023-02-12 14:07:48,099 [nnabla][INFO]: iter=5109 {Training loss}=1.3209483768150676e-05\n",
            "2023-02-12 14:07:48,100 [nnabla][INFO]: iter=5109 {Training error}=0.0\n",
            "2023-02-12 14:07:48,140 [nnabla][INFO]: iter=5119 {Training loss}=1.28602523545851e-05\n",
            "2023-02-12 14:07:48,140 [nnabla][INFO]: iter=5119 {Training error}=0.0\n",
            "2023-02-12 14:07:48,183 [nnabla][INFO]: iter=5129 {Training loss}=1.2835031157010235e-05\n",
            "2023-02-12 14:07:48,184 [nnabla][INFO]: iter=5129 {Training error}=0.0\n",
            "2023-02-12 14:07:48,220 [nnabla][INFO]: iter=5139 {Training loss}=1.2264451470400672e-05\n",
            "2023-02-12 14:07:48,220 [nnabla][INFO]: iter=5139 {Training error}=0.0\n",
            "2023-02-12 14:07:48,256 [nnabla][INFO]: iter=5149 {Training loss}=1.2729612535622437e-05\n",
            "2023-02-12 14:07:48,256 [nnabla][INFO]: iter=5149 {Training error}=0.0\n",
            "2023-02-12 14:07:48,292 [nnabla][INFO]: iter=5159 {Training loss}=1.2590038750204258e-05\n",
            "2023-02-12 14:07:48,292 [nnabla][INFO]: iter=5159 {Training error}=0.0\n",
            "2023-02-12 14:07:48,329 [nnabla][INFO]: iter=5169 {Training loss}=1.2386728485580534e-05\n",
            "2023-02-12 14:07:48,329 [nnabla][INFO]: iter=5169 {Training error}=0.0\n",
            "2023-02-12 14:07:48,364 [nnabla][INFO]: iter=5179 {Training loss}=1.2472233720473014e-05\n",
            "2023-02-12 14:07:48,365 [nnabla][INFO]: iter=5179 {Training error}=0.0\n",
            "2023-02-12 14:07:48,401 [nnabla][INFO]: iter=5189 {Training loss}=1.2067444004060235e-05\n",
            "2023-02-12 14:07:48,401 [nnabla][INFO]: iter=5189 {Training error}=0.0\n",
            "2023-02-12 14:07:48,436 [nnabla][INFO]: iter=5199 {Training loss}=1.238561344507616e-05\n",
            "2023-02-12 14:07:48,437 [nnabla][INFO]: iter=5199 {Training error}=0.0\n",
            "2023-02-12 14:07:48,437 [nnabla][INFO]: iter=5199 {Training time}=0.39582252502441406[sec/100iter] 24.912922859191895[sec]\n",
            "2023-02-12 14:07:48,457 [nnabla][INFO]: iter=5200 {Test error}=0.0234375\n",
            "2023-02-12 14:07:48,493 [nnabla][INFO]: iter=5209 {Training loss}=1.2230319953232538e-05\n",
            "2023-02-12 14:07:48,493 [nnabla][INFO]: iter=5209 {Training error}=0.0\n",
            "2023-02-12 14:07:48,530 [nnabla][INFO]: iter=5219 {Training loss}=1.175704164779745e-05\n",
            "2023-02-12 14:07:48,530 [nnabla][INFO]: iter=5219 {Training error}=0.0\n",
            "2023-02-12 14:07:48,565 [nnabla][INFO]: iter=5229 {Training loss}=1.2006360520899761e-05\n",
            "2023-02-12 14:07:48,565 [nnabla][INFO]: iter=5229 {Training error}=0.0\n",
            "2023-02-12 14:07:48,603 [nnabla][INFO]: iter=5239 {Training loss}=1.1608708518906496e-05\n",
            "2023-02-12 14:07:48,603 [nnabla][INFO]: iter=5239 {Training error}=0.0\n",
            "2023-02-12 14:07:48,642 [nnabla][INFO]: iter=5249 {Training loss}=1.1943593563046306e-05\n",
            "2023-02-12 14:07:48,643 [nnabla][INFO]: iter=5249 {Training error}=0.0\n",
            "2023-02-12 14:07:48,682 [nnabla][INFO]: iter=5259 {Training loss}=1.153916400653543e-05\n",
            "2023-02-12 14:07:48,682 [nnabla][INFO]: iter=5259 {Training error}=0.0\n",
            "2023-02-12 14:07:48,719 [nnabla][INFO]: iter=5269 {Training loss}=1.1762642316170968e-05\n",
            "2023-02-12 14:07:48,719 [nnabla][INFO]: iter=5269 {Training error}=0.0\n",
            "2023-02-12 14:07:48,756 [nnabla][INFO]: iter=5279 {Training loss}=1.1682471267704386e-05\n",
            "2023-02-12 14:07:48,756 [nnabla][INFO]: iter=5279 {Training error}=0.0\n",
            "2023-02-12 14:07:48,791 [nnabla][INFO]: iter=5289 {Training loss}=1.1081928278144915e-05\n",
            "2023-02-12 14:07:48,791 [nnabla][INFO]: iter=5289 {Training error}=0.0\n",
            "2023-02-12 14:07:48,829 [nnabla][INFO]: iter=5299 {Training loss}=1.1262040061410517e-05\n",
            "2023-02-12 14:07:48,829 [nnabla][INFO]: iter=5299 {Training error}=0.0\n",
            "2023-02-12 14:07:48,829 [nnabla][INFO]: iter=5299 {Training time}=0.3924567699432373[sec/100iter] 25.305379629135132[sec]\n",
            "2023-02-12 14:07:48,851 [nnabla][INFO]: iter=5300 {Test error}=0.0234375\n",
            "2023-02-12 14:07:48,895 [nnabla][INFO]: iter=5309 {Training loss}=1.1450877536844928e-05\n",
            "2023-02-12 14:07:48,895 [nnabla][INFO]: iter=5309 {Training error}=0.0\n",
            "2023-02-12 14:07:48,937 [nnabla][INFO]: iter=5319 {Training loss}=1.105277806345839e-05\n",
            "2023-02-12 14:07:48,937 [nnabla][INFO]: iter=5319 {Training error}=0.0\n",
            "2023-02-12 14:07:48,974 [nnabla][INFO]: iter=5329 {Training loss}=1.1227950380998664e-05\n",
            "2023-02-12 14:07:48,974 [nnabla][INFO]: iter=5329 {Training error}=0.0\n",
            "2023-02-12 14:07:49,010 [nnabla][INFO]: iter=5339 {Training loss}=1.0967110938508995e-05\n",
            "2023-02-12 14:07:49,010 [nnabla][INFO]: iter=5339 {Training error}=0.0\n",
            "2023-02-12 14:07:49,051 [nnabla][INFO]: iter=5349 {Training loss}=1.0684858352760784e-05\n",
            "2023-02-12 14:07:49,051 [nnabla][INFO]: iter=5349 {Training error}=0.0\n",
            "2023-02-12 14:07:49,087 [nnabla][INFO]: iter=5359 {Training loss}=1.0925399692496285e-05\n",
            "2023-02-12 14:07:49,087 [nnabla][INFO]: iter=5359 {Training error}=0.0\n",
            "2023-02-12 14:07:49,123 [nnabla][INFO]: iter=5369 {Training loss}=1.0732715963968076e-05\n",
            "2023-02-12 14:07:49,123 [nnabla][INFO]: iter=5369 {Training error}=0.0\n",
            "2023-02-12 14:07:49,167 [nnabla][INFO]: iter=5379 {Training loss}=1.057992358255433e-05\n",
            "2023-02-12 14:07:49,168 [nnabla][INFO]: iter=5379 {Training error}=0.0\n",
            "2023-02-12 14:07:49,205 [nnabla][INFO]: iter=5389 {Training loss}=1.078961395251099e-05\n",
            "2023-02-12 14:07:49,205 [nnabla][INFO]: iter=5389 {Training error}=0.0\n",
            "2023-02-12 14:07:49,242 [nnabla][INFO]: iter=5399 {Training loss}=1.066185905074235e-05\n",
            "2023-02-12 14:07:49,242 [nnabla][INFO]: iter=5399 {Training error}=0.0\n",
            "2023-02-12 14:07:49,242 [nnabla][INFO]: iter=5399 {Training time}=0.41289687156677246[sec/100iter] 25.718276500701904[sec]\n",
            "2023-02-12 14:07:49,263 [nnabla][INFO]: iter=5400 {Test error}=0.0234375\n",
            "2023-02-12 14:07:49,299 [nnabla][INFO]: iter=5409 {Training loss}=1.0468173059052788e-05\n",
            "2023-02-12 14:07:49,299 [nnabla][INFO]: iter=5409 {Training error}=0.0\n",
            "2023-02-12 14:07:49,335 [nnabla][INFO]: iter=5419 {Training loss}=1.0184705388383009e-05\n",
            "2023-02-12 14:07:49,335 [nnabla][INFO]: iter=5419 {Training error}=0.0\n",
            "2023-02-12 14:07:49,370 [nnabla][INFO]: iter=5429 {Training loss}=1.0616697181831114e-05\n",
            "2023-02-12 14:07:49,370 [nnabla][INFO]: iter=5429 {Training error}=0.0\n",
            "2023-02-12 14:07:49,406 [nnabla][INFO]: iter=5439 {Training loss}=1.0203601959801745e-05\n",
            "2023-02-12 14:07:49,407 [nnabla][INFO]: iter=5439 {Training error}=0.0\n",
            "2023-02-12 14:07:49,446 [nnabla][INFO]: iter=5449 {Training loss}=1.0124820619239472e-05\n",
            "2023-02-12 14:07:49,446 [nnabla][INFO]: iter=5449 {Training error}=0.0\n",
            "2023-02-12 14:07:49,485 [nnabla][INFO]: iter=5459 {Training loss}=1.0376526915933937e-05\n",
            "2023-02-12 14:07:49,485 [nnabla][INFO]: iter=5459 {Training error}=0.0\n",
            "2023-02-12 14:07:49,520 [nnabla][INFO]: iter=5469 {Training loss}=9.67010510066757e-06\n",
            "2023-02-12 14:07:49,520 [nnabla][INFO]: iter=5469 {Training error}=0.0\n",
            "2023-02-12 14:07:49,557 [nnabla][INFO]: iter=5479 {Training loss}=1.0283982192049734e-05\n",
            "2023-02-12 14:07:49,558 [nnabla][INFO]: iter=5479 {Training error}=0.0\n",
            "2023-02-12 14:07:49,593 [nnabla][INFO]: iter=5489 {Training loss}=9.749154742166866e-06\n",
            "2023-02-12 14:07:49,594 [nnabla][INFO]: iter=5489 {Training error}=0.0\n",
            "2023-02-12 14:07:49,629 [nnabla][INFO]: iter=5499 {Training loss}=9.960193892766256e-06\n",
            "2023-02-12 14:07:49,630 [nnabla][INFO]: iter=5499 {Training error}=0.0\n",
            "2023-02-12 14:07:49,630 [nnabla][INFO]: iter=5499 {Training time}=0.3876314163208008[sec/100iter] 26.105907917022705[sec]\n",
            "2023-02-12 14:07:49,650 [nnabla][INFO]: iter=5500 {Test error}=0.02265625\n",
            "2023-02-12 14:07:49,686 [nnabla][INFO]: iter=5509 {Training loss}=9.822659194469452e-06\n",
            "2023-02-12 14:07:49,686 [nnabla][INFO]: iter=5509 {Training error}=0.0\n",
            "2023-02-12 14:07:49,722 [nnabla][INFO]: iter=5519 {Training loss}=9.698115718492772e-06\n",
            "2023-02-12 14:07:49,722 [nnabla][INFO]: iter=5519 {Training error}=0.0\n",
            "2023-02-12 14:07:49,767 [nnabla][INFO]: iter=5529 {Training loss}=9.453966413275339e-06\n",
            "2023-02-12 14:07:49,768 [nnabla][INFO]: iter=5529 {Training error}=0.0\n",
            "2023-02-12 14:07:49,803 [nnabla][INFO]: iter=5539 {Training loss}=9.828791007748805e-06\n",
            "2023-02-12 14:07:49,803 [nnabla][INFO]: iter=5539 {Training error}=0.0\n",
            "2023-02-12 14:07:49,839 [nnabla][INFO]: iter=5549 {Training loss}=9.159596629615407e-06\n",
            "2023-02-12 14:07:49,840 [nnabla][INFO]: iter=5549 {Training error}=0.0\n",
            "2023-02-12 14:07:49,875 [nnabla][INFO]: iter=5559 {Training loss}=9.423974915989675e-06\n",
            "2023-02-12 14:07:49,875 [nnabla][INFO]: iter=5559 {Training error}=0.0\n",
            "2023-02-12 14:07:49,910 [nnabla][INFO]: iter=5569 {Training loss}=9.55836549110245e-06\n",
            "2023-02-12 14:07:49,911 [nnabla][INFO]: iter=5569 {Training error}=0.0\n",
            "2023-02-12 14:07:49,948 [nnabla][INFO]: iter=5579 {Training loss}=9.449775461689569e-06\n",
            "2023-02-12 14:07:49,949 [nnabla][INFO]: iter=5579 {Training error}=0.0\n",
            "2023-02-12 14:07:49,984 [nnabla][INFO]: iter=5589 {Training loss}=9.079984010895714e-06\n",
            "2023-02-12 14:07:49,984 [nnabla][INFO]: iter=5589 {Training error}=0.0\n",
            "2023-02-12 14:07:50,024 [nnabla][INFO]: iter=5599 {Training loss}=9.513573786534835e-06\n",
            "2023-02-12 14:07:50,024 [nnabla][INFO]: iter=5599 {Training error}=0.0\n",
            "2023-02-12 14:07:50,024 [nnabla][INFO]: iter=5599 {Training time}=0.394733190536499[sec/100iter] 26.500641107559204[sec]\n",
            "2023-02-12 14:07:50,048 [nnabla][INFO]: iter=5600 {Test error}=0.0234375\n",
            "2023-02-12 14:07:50,087 [nnabla][INFO]: iter=5609 {Training loss}=8.85173176357057e-06\n",
            "2023-02-12 14:07:50,087 [nnabla][INFO]: iter=5609 {Training error}=0.0\n",
            "2023-02-12 14:07:50,123 [nnabla][INFO]: iter=5619 {Training loss}=8.949332368501928e-06\n",
            "2023-02-12 14:07:50,123 [nnabla][INFO]: iter=5619 {Training error}=0.0\n",
            "2023-02-12 14:07:50,159 [nnabla][INFO]: iter=5629 {Training loss}=8.981648534245323e-06\n",
            "2023-02-12 14:07:50,160 [nnabla][INFO]: iter=5629 {Training error}=0.0\n",
            "2023-02-12 14:07:50,194 [nnabla][INFO]: iter=5639 {Training loss}=8.816057743388228e-06\n",
            "2023-02-12 14:07:50,195 [nnabla][INFO]: iter=5639 {Training error}=0.0\n",
            "2023-02-12 14:07:50,230 [nnabla][INFO]: iter=5649 {Training loss}=8.818302376312204e-06\n",
            "2023-02-12 14:07:50,230 [nnabla][INFO]: iter=5649 {Training error}=0.0\n",
            "2023-02-12 14:07:50,274 [nnabla][INFO]: iter=5659 {Training loss}=8.966091627371497e-06\n",
            "2023-02-12 14:07:50,274 [nnabla][INFO]: iter=5659 {Training error}=0.0\n",
            "2023-02-12 14:07:50,314 [nnabla][INFO]: iter=5669 {Training loss}=8.652551514387596e-06\n",
            "2023-02-12 14:07:50,314 [nnabla][INFO]: iter=5669 {Training error}=0.0\n",
            "2023-02-12 14:07:50,355 [nnabla][INFO]: iter=5679 {Training loss}=8.832735147734638e-06\n",
            "2023-02-12 14:07:50,355 [nnabla][INFO]: iter=5679 {Training error}=0.0\n",
            "2023-02-12 14:07:50,398 [nnabla][INFO]: iter=5689 {Training loss}=8.698078090674244e-06\n",
            "2023-02-12 14:07:50,399 [nnabla][INFO]: iter=5689 {Training error}=0.0\n",
            "2023-02-12 14:07:50,456 [nnabla][INFO]: iter=5699 {Training loss}=8.58326893649064e-06\n",
            "2023-02-12 14:07:50,457 [nnabla][INFO]: iter=5699 {Training error}=0.0\n",
            "2023-02-12 14:07:50,457 [nnabla][INFO]: iter=5699 {Training time}=0.432323694229126[sec/100iter] 26.93296480178833[sec]\n",
            "2023-02-12 14:07:50,486 [nnabla][INFO]: iter=5700 {Test error}=0.0234375\n",
            "2023-02-12 14:07:50,528 [nnabla][INFO]: iter=5709 {Training loss}=8.380711733479984e-06\n",
            "2023-02-12 14:07:50,528 [nnabla][INFO]: iter=5709 {Training error}=0.0\n",
            "2023-02-12 14:07:50,574 [nnabla][INFO]: iter=5719 {Training loss}=8.347846232936718e-06\n",
            "2023-02-12 14:07:50,574 [nnabla][INFO]: iter=5719 {Training error}=0.0\n",
            "2023-02-12 14:07:50,617 [nnabla][INFO]: iter=5729 {Training loss}=8.36776143842144e-06\n",
            "2023-02-12 14:07:50,617 [nnabla][INFO]: iter=5729 {Training error}=0.0\n",
            "2023-02-12 14:07:50,657 [nnabla][INFO]: iter=5739 {Training loss}=8.584474926465191e-06\n",
            "2023-02-12 14:07:50,657 [nnabla][INFO]: iter=5739 {Training error}=0.0\n",
            "2023-02-12 14:07:50,699 [nnabla][INFO]: iter=5749 {Training loss}=8.025906936381944e-06\n",
            "2023-02-12 14:07:50,699 [nnabla][INFO]: iter=5749 {Training error}=0.0\n",
            "2023-02-12 14:07:50,743 [nnabla][INFO]: iter=5759 {Training loss}=8.030563549255021e-06\n",
            "2023-02-12 14:07:50,744 [nnabla][INFO]: iter=5759 {Training error}=0.0\n",
            "2023-02-12 14:07:50,794 [nnabla][INFO]: iter=5769 {Training loss}=8.369630450033583e-06\n",
            "2023-02-12 14:07:50,794 [nnabla][INFO]: iter=5769 {Training error}=0.0\n",
            "2023-02-12 14:07:50,837 [nnabla][INFO]: iter=5779 {Training loss}=7.98679957370041e-06\n",
            "2023-02-12 14:07:50,838 [nnabla][INFO]: iter=5779 {Training error}=0.0\n",
            "2023-02-12 14:07:50,878 [nnabla][INFO]: iter=5789 {Training loss}=8.330524906341452e-06\n",
            "2023-02-12 14:07:50,878 [nnabla][INFO]: iter=5789 {Training error}=0.0\n",
            "2023-02-12 14:07:50,919 [nnabla][INFO]: iter=5799 {Training loss}=7.846162588975858e-06\n",
            "2023-02-12 14:07:50,919 [nnabla][INFO]: iter=5799 {Training error}=0.0\n",
            "2023-02-12 14:07:50,919 [nnabla][INFO]: iter=5799 {Training time}=0.46230125427246094[sec/100iter] 27.39526605606079[sec]\n",
            "2023-02-12 14:07:50,946 [nnabla][INFO]: iter=5800 {Test error}=0.0234375\n",
            "2023-02-12 14:07:50,985 [nnabla][INFO]: iter=5809 {Training loss}=8.148568667820655e-06\n",
            "2023-02-12 14:07:50,986 [nnabla][INFO]: iter=5809 {Training error}=0.0\n",
            "2023-02-12 14:07:51,026 [nnabla][INFO]: iter=5819 {Training loss}=7.714870662312023e-06\n",
            "2023-02-12 14:07:51,027 [nnabla][INFO]: iter=5819 {Training error}=0.0\n",
            "2023-02-12 14:07:51,083 [nnabla][INFO]: iter=5829 {Training loss}=7.871319212426897e-06\n",
            "2023-02-12 14:07:51,083 [nnabla][INFO]: iter=5829 {Training error}=0.0\n",
            "2023-02-12 14:07:51,129 [nnabla][INFO]: iter=5839 {Training loss}=7.694296073168516e-06\n",
            "2023-02-12 14:07:51,129 [nnabla][INFO]: iter=5839 {Training error}=0.0\n",
            "2023-02-12 14:07:51,169 [nnabla][INFO]: iter=5849 {Training loss}=7.550134796474595e-06\n",
            "2023-02-12 14:07:51,170 [nnabla][INFO]: iter=5849 {Training error}=0.0\n",
            "2023-02-12 14:07:51,215 [nnabla][INFO]: iter=5859 {Training loss}=7.876631570979953e-06\n",
            "2023-02-12 14:07:51,215 [nnabla][INFO]: iter=5859 {Training error}=0.0\n",
            "2023-02-12 14:07:51,259 [nnabla][INFO]: iter=5869 {Training loss}=7.629193078173557e-06\n",
            "2023-02-12 14:07:51,260 [nnabla][INFO]: iter=5869 {Training error}=0.0\n",
            "2023-02-12 14:07:51,300 [nnabla][INFO]: iter=5879 {Training loss}=7.67250639910344e-06\n",
            "2023-02-12 14:07:51,300 [nnabla][INFO]: iter=5879 {Training error}=0.0\n",
            "2023-02-12 14:07:51,353 [nnabla][INFO]: iter=5889 {Training loss}=7.285470019269269e-06\n",
            "2023-02-12 14:07:51,353 [nnabla][INFO]: iter=5889 {Training error}=0.0\n",
            "2023-02-12 14:07:51,398 [nnabla][INFO]: iter=5899 {Training loss}=7.574438313895371e-06\n",
            "2023-02-12 14:07:51,398 [nnabla][INFO]: iter=5899 {Training error}=0.0\n",
            "2023-02-12 14:07:51,398 [nnabla][INFO]: iter=5899 {Training time}=0.47908878326416016[sec/100iter] 27.87435483932495[sec]\n",
            "2023-02-12 14:07:51,426 [nnabla][INFO]: iter=5900 {Test error}=0.02265625\n",
            "2023-02-12 14:07:51,472 [nnabla][INFO]: iter=5909 {Training loss}=7.3864262049028184e-06\n",
            "2023-02-12 14:07:51,472 [nnabla][INFO]: iter=5909 {Training error}=0.0\n",
            "2023-02-12 14:07:51,513 [nnabla][INFO]: iter=5919 {Training loss}=7.361735242739087e-06\n",
            "2023-02-12 14:07:51,514 [nnabla][INFO]: iter=5919 {Training error}=0.0\n",
            "2023-02-12 14:07:51,556 [nnabla][INFO]: iter=5929 {Training loss}=7.2997268034669105e-06\n",
            "2023-02-12 14:07:51,556 [nnabla][INFO]: iter=5929 {Training error}=0.0\n",
            "2023-02-12 14:07:51,603 [nnabla][INFO]: iter=5939 {Training loss}=7.345724498009076e-06\n",
            "2023-02-12 14:07:51,603 [nnabla][INFO]: iter=5939 {Training error}=0.0\n",
            "2023-02-12 14:07:51,642 [nnabla][INFO]: iter=5949 {Training loss}=7.198679668363184e-06\n",
            "2023-02-12 14:07:51,642 [nnabla][INFO]: iter=5949 {Training error}=0.0\n",
            "2023-02-12 14:07:51,683 [nnabla][INFO]: iter=5959 {Training loss}=7.061508313199738e-06\n",
            "2023-02-12 14:07:51,684 [nnabla][INFO]: iter=5959 {Training error}=0.0\n",
            "2023-02-12 14:07:51,724 [nnabla][INFO]: iter=5969 {Training loss}=7.289475433935877e-06\n",
            "2023-02-12 14:07:51,725 [nnabla][INFO]: iter=5969 {Training error}=0.0\n",
            "2023-02-12 14:07:51,770 [nnabla][INFO]: iter=5979 {Training loss}=6.797582500439603e-06\n",
            "2023-02-12 14:07:51,770 [nnabla][INFO]: iter=5979 {Training error}=0.0\n",
            "2023-02-12 14:07:51,812 [nnabla][INFO]: iter=5989 {Training loss}=7.156867013691226e-06\n",
            "2023-02-12 14:07:51,813 [nnabla][INFO]: iter=5989 {Training error}=0.0\n",
            "2023-02-12 14:07:51,854 [nnabla][INFO]: iter=5999 {Training loss}=7.012434252828825e-06\n",
            "2023-02-12 14:07:51,855 [nnabla][INFO]: iter=5999 {Training error}=0.0\n",
            "2023-02-12 14:07:51,855 [nnabla][INFO]: iter=5999 {Training time}=0.4566812515258789[sec/100iter] 28.33103609085083[sec]\n",
            "2023-02-12 14:07:51,880 [nnabla][INFO]: iter=6000 {Test error}=0.0234375\n",
            "2023-02-12 14:07:51,901 [nnabla][INFO]: Solver state save (.h5): output/states_6000.h5\n",
            "2023-02-12 14:07:51,912 [nnabla][INFO]: Parameter save (.h5): output/params_6000.h5\n",
            "2023-02-12 14:07:51,912 [nnabla][INFO]: Checkpoint save (.json): output/checkpoint_6000.json\n",
            "2023-02-12 14:07:51,956 [nnabla][INFO]: iter=6009 {Training loss}=6.98160602041753e-06\n",
            "2023-02-12 14:07:51,957 [nnabla][INFO]: iter=6009 {Training error}=0.0\n",
            "2023-02-12 14:07:52,001 [nnabla][INFO]: iter=6019 {Training loss}=6.8467511482595e-06\n",
            "2023-02-12 14:07:52,001 [nnabla][INFO]: iter=6019 {Training error}=0.0\n",
            "2023-02-12 14:07:52,043 [nnabla][INFO]: iter=6029 {Training loss}=6.785112873330945e-06\n",
            "2023-02-12 14:07:52,043 [nnabla][INFO]: iter=6029 {Training error}=0.0\n",
            "2023-02-12 14:07:52,091 [nnabla][INFO]: iter=6039 {Training loss}=6.782866876164917e-06\n",
            "2023-02-12 14:07:52,091 [nnabla][INFO]: iter=6039 {Training error}=0.0\n",
            "2023-02-12 14:07:52,144 [nnabla][INFO]: iter=6049 {Training loss}=6.695245247101411e-06\n",
            "2023-02-12 14:07:52,146 [nnabla][INFO]: iter=6049 {Training error}=0.0\n",
            "2023-02-12 14:07:52,190 [nnabla][INFO]: iter=6059 {Training loss}=6.8779545472352766e-06\n",
            "2023-02-12 14:07:52,191 [nnabla][INFO]: iter=6059 {Training error}=0.0\n",
            "2023-02-12 14:07:52,238 [nnabla][INFO]: iter=6069 {Training loss}=6.586661129404092e-06\n",
            "2023-02-12 14:07:52,239 [nnabla][INFO]: iter=6069 {Training error}=0.0\n",
            "2023-02-12 14:07:52,286 [nnabla][INFO]: iter=6079 {Training loss}=6.517830570373917e-06\n",
            "2023-02-12 14:07:52,286 [nnabla][INFO]: iter=6079 {Training error}=0.0\n",
            "2023-02-12 14:07:52,329 [nnabla][INFO]: iter=6089 {Training loss}=6.58879935144796e-06\n",
            "2023-02-12 14:07:52,329 [nnabla][INFO]: iter=6089 {Training error}=0.0\n",
            "2023-02-12 14:07:52,371 [nnabla][INFO]: iter=6099 {Training loss}=6.588711585209239e-06\n",
            "2023-02-12 14:07:52,371 [nnabla][INFO]: iter=6099 {Training error}=0.0\n",
            "2023-02-12 14:07:52,371 [nnabla][INFO]: iter=6099 {Training time}=0.51633620262146[sec/100iter] 28.84737229347229[sec]\n",
            "2023-02-12 14:07:52,400 [nnabla][INFO]: iter=6100 {Test error}=0.0234375\n",
            "2023-02-12 14:07:52,442 [nnabla][INFO]: iter=6109 {Training loss}=6.423217655537883e-06\n",
            "2023-02-12 14:07:52,443 [nnabla][INFO]: iter=6109 {Training error}=0.0\n",
            "2023-02-12 14:07:52,494 [nnabla][INFO]: iter=6119 {Training loss}=6.467362254625186e-06\n",
            "2023-02-12 14:07:52,495 [nnabla][INFO]: iter=6119 {Training error}=0.0\n",
            "2023-02-12 14:07:52,540 [nnabla][INFO]: iter=6129 {Training loss}=6.357565325743053e-06\n",
            "2023-02-12 14:07:52,540 [nnabla][INFO]: iter=6129 {Training error}=0.0\n",
            "2023-02-12 14:07:52,585 [nnabla][INFO]: iter=6139 {Training loss}=6.434490842366358e-06\n",
            "2023-02-12 14:07:52,585 [nnabla][INFO]: iter=6139 {Training error}=0.0\n",
            "2023-02-12 14:07:52,631 [nnabla][INFO]: iter=6149 {Training loss}=6.281852165557211e-06\n",
            "2023-02-12 14:07:52,631 [nnabla][INFO]: iter=6149 {Training error}=0.0\n",
            "2023-02-12 14:07:52,676 [nnabla][INFO]: iter=6159 {Training loss}=6.285029030550504e-06\n",
            "2023-02-12 14:07:52,677 [nnabla][INFO]: iter=6159 {Training error}=0.0\n",
            "2023-02-12 14:07:52,721 [nnabla][INFO]: iter=6169 {Training loss}=6.168892468849663e-06\n",
            "2023-02-12 14:07:52,721 [nnabla][INFO]: iter=6169 {Training error}=0.0\n",
            "2023-02-12 14:07:52,773 [nnabla][INFO]: iter=6179 {Training loss}=6.178574949444737e-06\n",
            "2023-02-12 14:07:52,774 [nnabla][INFO]: iter=6179 {Training error}=0.0\n",
            "2023-02-12 14:07:52,820 [nnabla][INFO]: iter=6189 {Training loss}=6.043173470970942e-06\n",
            "2023-02-12 14:07:52,820 [nnabla][INFO]: iter=6189 {Training error}=0.0\n",
            "2023-02-12 14:07:52,864 [nnabla][INFO]: iter=6199 {Training loss}=6.255407697608462e-06\n",
            "2023-02-12 14:07:52,864 [nnabla][INFO]: iter=6199 {Training error}=0.0\n",
            "2023-02-12 14:07:52,864 [nnabla][INFO]: iter=6199 {Training time}=0.4928605556488037[sec/100iter] 29.340232849121094[sec]\n",
            "2023-02-12 14:07:52,892 [nnabla][INFO]: iter=6200 {Test error}=0.0234375\n",
            "2023-02-12 14:07:52,934 [nnabla][INFO]: iter=6209 {Training loss}=6.023521109455032e-06\n",
            "2023-02-12 14:07:52,934 [nnabla][INFO]: iter=6209 {Training error}=0.0\n",
            "2023-02-12 14:07:52,977 [nnabla][INFO]: iter=6219 {Training loss}=6.112639766797656e-06\n",
            "2023-02-12 14:07:52,978 [nnabla][INFO]: iter=6219 {Training error}=0.0\n",
            "2023-02-12 14:07:53,023 [nnabla][INFO]: iter=6229 {Training loss}=5.958332621958107e-06\n",
            "2023-02-12 14:07:53,024 [nnabla][INFO]: iter=6229 {Training error}=0.0\n",
            "2023-02-12 14:07:53,065 [nnabla][INFO]: iter=6239 {Training loss}=5.926483026996721e-06\n",
            "2023-02-12 14:07:53,066 [nnabla][INFO]: iter=6239 {Training error}=0.0\n",
            "2023-02-12 14:07:53,111 [nnabla][INFO]: iter=6249 {Training loss}=5.947341833234532e-06\n",
            "2023-02-12 14:07:53,111 [nnabla][INFO]: iter=6249 {Training error}=0.0\n",
            "2023-02-12 14:07:53,154 [nnabla][INFO]: iter=6259 {Training loss}=5.896032689634012e-06\n",
            "2023-02-12 14:07:53,154 [nnabla][INFO]: iter=6259 {Training error}=0.0\n",
            "2023-02-12 14:07:53,209 [nnabla][INFO]: iter=6269 {Training loss}=5.834751391375903e-06\n",
            "2023-02-12 14:07:53,209 [nnabla][INFO]: iter=6269 {Training error}=0.0\n",
            "2023-02-12 14:07:53,252 [nnabla][INFO]: iter=6279 {Training loss}=5.712850907002576e-06\n",
            "2023-02-12 14:07:53,253 [nnabla][INFO]: iter=6279 {Training error}=0.0\n",
            "2023-02-12 14:07:53,297 [nnabla][INFO]: iter=6289 {Training loss}=5.8503060245129745e-06\n",
            "2023-02-12 14:07:53,297 [nnabla][INFO]: iter=6289 {Training error}=0.0\n",
            "2023-02-12 14:07:53,340 [nnabla][INFO]: iter=6299 {Training loss}=5.642443284159526e-06\n",
            "2023-02-12 14:07:53,340 [nnabla][INFO]: iter=6299 {Training error}=0.0\n",
            "2023-02-12 14:07:53,340 [nnabla][INFO]: iter=6299 {Training time}=0.4762427806854248[sec/100iter] 29.81647562980652[sec]\n",
            "2023-02-12 14:07:53,368 [nnabla][INFO]: iter=6300 {Test error}=0.0234375\n",
            "2023-02-12 14:07:53,414 [nnabla][INFO]: iter=6309 {Training loss}=5.6920880524558015e-06\n",
            "2023-02-12 14:07:53,414 [nnabla][INFO]: iter=6309 {Training error}=0.0\n",
            "2023-02-12 14:07:53,457 [nnabla][INFO]: iter=6319 {Training loss}=5.755411166319391e-06\n",
            "2023-02-12 14:07:53,457 [nnabla][INFO]: iter=6319 {Training error}=0.0\n",
            "2023-02-12 14:07:53,501 [nnabla][INFO]: iter=6329 {Training loss}=5.470995347423013e-06\n",
            "2023-02-12 14:07:53,501 [nnabla][INFO]: iter=6329 {Training error}=0.0\n",
            "2023-02-12 14:07:53,543 [nnabla][INFO]: iter=6339 {Training loss}=5.661911472998327e-06\n",
            "2023-02-12 14:07:53,544 [nnabla][INFO]: iter=6339 {Training error}=0.0\n",
            "2023-02-12 14:07:53,586 [nnabla][INFO]: iter=6349 {Training loss}=5.581915957009187e-06\n",
            "2023-02-12 14:07:53,587 [nnabla][INFO]: iter=6349 {Training error}=0.0\n",
            "2023-02-12 14:07:53,635 [nnabla][INFO]: iter=6359 {Training loss}=5.348910235625226e-06\n",
            "2023-02-12 14:07:53,636 [nnabla][INFO]: iter=6359 {Training error}=0.0\n",
            "2023-02-12 14:07:53,678 [nnabla][INFO]: iter=6369 {Training loss}=5.425832114269724e-06\n",
            "2023-02-12 14:07:53,678 [nnabla][INFO]: iter=6369 {Training error}=0.0\n",
            "2023-02-12 14:07:53,723 [nnabla][INFO]: iter=6379 {Training loss}=5.448187494039303e-06\n",
            "2023-02-12 14:07:53,723 [nnabla][INFO]: iter=6379 {Training error}=0.0\n",
            "2023-02-12 14:07:53,770 [nnabla][INFO]: iter=6389 {Training loss}=5.471748863783432e-06\n",
            "2023-02-12 14:07:53,770 [nnabla][INFO]: iter=6389 {Training error}=0.0\n",
            "2023-02-12 14:07:53,814 [nnabla][INFO]: iter=6399 {Training loss}=5.214523753238609e-06\n",
            "2023-02-12 14:07:53,815 [nnabla][INFO]: iter=6399 {Training error}=0.0\n",
            "2023-02-12 14:07:53,815 [nnabla][INFO]: iter=6399 {Training time}=0.4745299816131592[sec/100iter] 30.291005611419678[sec]\n",
            "2023-02-12 14:07:53,841 [nnabla][INFO]: iter=6400 {Test error}=0.02265625\n",
            "2023-02-12 14:07:53,892 [nnabla][INFO]: iter=6409 {Training loss}=5.394450909079751e-06\n",
            "2023-02-12 14:07:53,892 [nnabla][INFO]: iter=6409 {Training error}=0.0\n",
            "2023-02-12 14:07:53,934 [nnabla][INFO]: iter=6419 {Training loss}=5.220949788053986e-06\n",
            "2023-02-12 14:07:53,935 [nnabla][INFO]: iter=6419 {Training error}=0.0\n",
            "2023-02-12 14:07:53,980 [nnabla][INFO]: iter=6429 {Training loss}=5.206240530242212e-06\n",
            "2023-02-12 14:07:53,981 [nnabla][INFO]: iter=6429 {Training error}=0.0\n",
            "2023-02-12 14:07:54,028 [nnabla][INFO]: iter=6439 {Training loss}=5.256900294625666e-06\n",
            "2023-02-12 14:07:54,028 [nnabla][INFO]: iter=6439 {Training error}=0.0\n",
            "2023-02-12 14:07:54,076 [nnabla][INFO]: iter=6449 {Training loss}=5.132759270054521e-06\n",
            "2023-02-12 14:07:54,077 [nnabla][INFO]: iter=6449 {Training error}=0.0\n",
            "2023-02-12 14:07:54,125 [nnabla][INFO]: iter=6459 {Training loss}=5.052765573054785e-06\n",
            "2023-02-12 14:07:54,125 [nnabla][INFO]: iter=6459 {Training error}=0.0\n",
            "2023-02-12 14:07:54,179 [nnabla][INFO]: iter=6469 {Training loss}=5.157907253305893e-06\n",
            "2023-02-12 14:07:54,179 [nnabla][INFO]: iter=6469 {Training error}=0.0\n",
            "2023-02-12 14:07:54,227 [nnabla][INFO]: iter=6479 {Training loss}=5.099884219816886e-06\n",
            "2023-02-12 14:07:54,228 [nnabla][INFO]: iter=6479 {Training error}=0.0\n",
            "2023-02-12 14:07:54,271 [nnabla][INFO]: iter=6489 {Training loss}=5.0978387662325986e-06\n",
            "2023-02-12 14:07:54,271 [nnabla][INFO]: iter=6489 {Training error}=0.0\n",
            "2023-02-12 14:07:54,317 [nnabla][INFO]: iter=6499 {Training loss}=4.991207788407337e-06\n",
            "2023-02-12 14:07:54,318 [nnabla][INFO]: iter=6499 {Training error}=0.0\n",
            "2023-02-12 14:07:54,319 [nnabla][INFO]: iter=6499 {Training time}=0.5040833950042725[sec/100iter] 30.79508900642395[sec]\n",
            "2023-02-12 14:07:54,345 [nnabla][INFO]: iter=6500 {Test error}=0.0234375\n",
            "2023-02-12 14:07:54,390 [nnabla][INFO]: iter=6509 {Training loss}=5.001547833671793e-06\n",
            "2023-02-12 14:07:54,390 [nnabla][INFO]: iter=6509 {Training error}=0.0\n",
            "2023-02-12 14:07:54,436 [nnabla][INFO]: iter=6519 {Training loss}=4.809791789739393e-06\n",
            "2023-02-12 14:07:54,436 [nnabla][INFO]: iter=6519 {Training error}=0.0\n",
            "2023-02-12 14:07:54,482 [nnabla][INFO]: iter=6529 {Training loss}=4.9362606659997255e-06\n",
            "2023-02-12 14:07:54,483 [nnabla][INFO]: iter=6529 {Training error}=0.0\n",
            "2023-02-12 14:07:54,528 [nnabla][INFO]: iter=6539 {Training loss}=4.918475497106556e-06\n",
            "2023-02-12 14:07:54,528 [nnabla][INFO]: iter=6539 {Training error}=0.0\n",
            "2023-02-12 14:07:54,573 [nnabla][INFO]: iter=6549 {Training loss}=4.7595976866432466e-06\n",
            "2023-02-12 14:07:54,573 [nnabla][INFO]: iter=6549 {Training error}=0.0\n",
            "2023-02-12 14:07:54,622 [nnabla][INFO]: iter=6559 {Training loss}=4.934030130243627e-06\n",
            "2023-02-12 14:07:54,622 [nnabla][INFO]: iter=6559 {Training error}=0.0\n",
            "2023-02-12 14:07:54,668 [nnabla][INFO]: iter=6569 {Training loss}=4.7716139306430705e-06\n",
            "2023-02-12 14:07:54,668 [nnabla][INFO]: iter=6569 {Training error}=0.0\n",
            "2023-02-12 14:07:54,713 [nnabla][INFO]: iter=6579 {Training loss}=4.625401743396651e-06\n",
            "2023-02-12 14:07:54,713 [nnabla][INFO]: iter=6579 {Training error}=0.0\n",
            "2023-02-12 14:07:54,756 [nnabla][INFO]: iter=6589 {Training loss}=4.7002749852254055e-06\n",
            "2023-02-12 14:07:54,756 [nnabla][INFO]: iter=6589 {Training error}=0.0\n",
            "2023-02-12 14:07:54,800 [nnabla][INFO]: iter=6599 {Training loss}=4.730450200440828e-06\n",
            "2023-02-12 14:07:54,800 [nnabla][INFO]: iter=6599 {Training error}=0.0\n",
            "2023-02-12 14:07:54,800 [nnabla][INFO]: iter=6599 {Training time}=0.4815692901611328[sec/100iter] 31.276658296585083[sec]\n",
            "2023-02-12 14:07:54,826 [nnabla][INFO]: iter=6600 {Test error}=0.0234375\n",
            "2023-02-12 14:07:54,870 [nnabla][INFO]: iter=6609 {Training loss}=4.6186041799956e-06\n",
            "2023-02-12 14:07:54,871 [nnabla][INFO]: iter=6609 {Training error}=0.0\n",
            "2023-02-12 14:07:54,916 [nnabla][INFO]: iter=6619 {Training loss}=4.72374676974141e-06\n",
            "2023-02-12 14:07:54,916 [nnabla][INFO]: iter=6619 {Training error}=0.0\n",
            "2023-02-12 14:07:54,966 [nnabla][INFO]: iter=6629 {Training loss}=4.479376912058797e-06\n",
            "2023-02-12 14:07:54,966 [nnabla][INFO]: iter=6629 {Training error}=0.0\n",
            "2023-02-12 14:07:55,012 [nnabla][INFO]: iter=6639 {Training loss}=4.460657692106906e-06\n",
            "2023-02-12 14:07:55,012 [nnabla][INFO]: iter=6639 {Training error}=0.0\n",
            "2023-02-12 14:07:55,059 [nnabla][INFO]: iter=6649 {Training loss}=4.660792001232039e-06\n",
            "2023-02-12 14:07:55,059 [nnabla][INFO]: iter=6649 {Training error}=0.0\n",
            "2023-02-12 14:07:55,105 [nnabla][INFO]: iter=6659 {Training loss}=4.392210485093528e-06\n",
            "2023-02-12 14:07:55,105 [nnabla][INFO]: iter=6659 {Training error}=0.0\n",
            "2023-02-12 14:07:55,150 [nnabla][INFO]: iter=6669 {Training loss}=4.576976152748102e-06\n",
            "2023-02-12 14:07:55,151 [nnabla][INFO]: iter=6669 {Training error}=0.0\n",
            "2023-02-12 14:07:55,204 [nnabla][INFO]: iter=6679 {Training loss}=4.393325980345253e-06\n",
            "2023-02-12 14:07:55,205 [nnabla][INFO]: iter=6679 {Training error}=0.0\n",
            "2023-02-12 14:07:55,258 [nnabla][INFO]: iter=6689 {Training loss}=4.371811883174814e-06\n",
            "2023-02-12 14:07:55,259 [nnabla][INFO]: iter=6689 {Training error}=0.0\n",
            "2023-02-12 14:07:55,304 [nnabla][INFO]: iter=6699 {Training loss}=4.382432052807417e-06\n",
            "2023-02-12 14:07:55,304 [nnabla][INFO]: iter=6699 {Training error}=0.0\n",
            "2023-02-12 14:07:55,304 [nnabla][INFO]: iter=6699 {Training time}=0.5036792755126953[sec/100iter] 31.78033757209778[sec]\n",
            "2023-02-12 14:07:55,337 [nnabla][INFO]: iter=6700 {Test error}=0.0234375\n",
            "2023-02-12 14:07:55,382 [nnabla][INFO]: iter=6709 {Training loss}=4.340617124398705e-06\n",
            "2023-02-12 14:07:55,382 [nnabla][INFO]: iter=6709 {Training error}=0.0\n",
            "2023-02-12 14:07:55,433 [nnabla][INFO]: iter=6719 {Training loss}=4.394722964207176e-06\n",
            "2023-02-12 14:07:55,434 [nnabla][INFO]: iter=6719 {Training error}=0.0\n",
            "2023-02-12 14:07:55,479 [nnabla][INFO]: iter=6729 {Training loss}=4.265928055247059e-06\n",
            "2023-02-12 14:07:55,479 [nnabla][INFO]: iter=6729 {Training error}=0.0\n",
            "2023-02-12 14:07:55,522 [nnabla][INFO]: iter=6739 {Training loss}=4.334468030720018e-06\n",
            "2023-02-12 14:07:55,522 [nnabla][INFO]: iter=6739 {Training error}=0.0\n",
            "2023-02-12 14:07:55,562 [nnabla][INFO]: iter=6749 {Training loss}=4.244881438353332e-06\n",
            "2023-02-12 14:07:55,562 [nnabla][INFO]: iter=6749 {Training error}=0.0\n",
            "2023-02-12 14:07:55,605 [nnabla][INFO]: iter=6759 {Training loss}=4.160597200097982e-06\n",
            "2023-02-12 14:07:55,605 [nnabla][INFO]: iter=6759 {Training error}=0.0\n",
            "2023-02-12 14:07:55,646 [nnabla][INFO]: iter=6769 {Training loss}=4.1971989048761316e-06\n",
            "2023-02-12 14:07:55,647 [nnabla][INFO]: iter=6769 {Training error}=0.0\n",
            "2023-02-12 14:07:55,685 [nnabla][INFO]: iter=6779 {Training loss}=4.169817657384556e-06\n",
            "2023-02-12 14:07:55,685 [nnabla][INFO]: iter=6779 {Training error}=0.0\n",
            "2023-02-12 14:07:55,724 [nnabla][INFO]: iter=6789 {Training loss}=4.118784090678673e-06\n",
            "2023-02-12 14:07:55,724 [nnabla][INFO]: iter=6789 {Training error}=0.0\n",
            "2023-02-12 14:07:55,772 [nnabla][INFO]: iter=6799 {Training loss}=4.121205165574793e-06\n",
            "2023-02-12 14:07:55,772 [nnabla][INFO]: iter=6799 {Training error}=0.0\n",
            "2023-02-12 14:07:55,772 [nnabla][INFO]: iter=6799 {Training time}=0.46817636489868164[sec/100iter] 32.24851393699646[sec]\n",
            "2023-02-12 14:07:55,794 [nnabla][INFO]: iter=6800 {Test error}=0.02265625\n",
            "2023-02-12 14:07:55,832 [nnabla][INFO]: iter=6809 {Training loss}=4.071195235155756e-06\n",
            "2023-02-12 14:07:55,832 [nnabla][INFO]: iter=6809 {Training error}=0.0\n",
            "2023-02-12 14:07:55,879 [nnabla][INFO]: iter=6819 {Training loss}=4.126700787310256e-06\n",
            "2023-02-12 14:07:55,879 [nnabla][INFO]: iter=6819 {Training error}=0.0\n",
            "2023-02-12 14:07:55,917 [nnabla][INFO]: iter=6829 {Training loss}=3.954599378630519e-06\n",
            "2023-02-12 14:07:55,917 [nnabla][INFO]: iter=6829 {Training error}=0.0\n",
            "2023-02-12 14:07:55,956 [nnabla][INFO]: iter=6839 {Training loss}=4.032361630379455e-06\n",
            "2023-02-12 14:07:55,956 [nnabla][INFO]: iter=6839 {Training error}=0.0\n",
            "2023-02-12 14:07:55,994 [nnabla][INFO]: iter=6849 {Training loss}=4.043164608447114e-06\n",
            "2023-02-12 14:07:55,994 [nnabla][INFO]: iter=6849 {Training error}=0.0\n",
            "2023-02-12 14:07:56,033 [nnabla][INFO]: iter=6859 {Training loss}=3.922097221220611e-06\n",
            "2023-02-12 14:07:56,033 [nnabla][INFO]: iter=6859 {Training error}=0.0\n",
            "2023-02-12 14:07:56,073 [nnabla][INFO]: iter=6869 {Training loss}=3.917998583347071e-06\n",
            "2023-02-12 14:07:56,073 [nnabla][INFO]: iter=6869 {Training error}=0.0\n",
            "2023-02-12 14:07:56,110 [nnabla][INFO]: iter=6879 {Training loss}=3.8212378967728e-06\n",
            "2023-02-12 14:07:56,110 [nnabla][INFO]: iter=6879 {Training error}=0.0\n",
            "2023-02-12 14:07:56,153 [nnabla][INFO]: iter=6889 {Training loss}=3.81565132556716e-06\n",
            "2023-02-12 14:07:56,153 [nnabla][INFO]: iter=6889 {Training error}=0.0\n",
            "2023-02-12 14:07:56,190 [nnabla][INFO]: iter=6899 {Training loss}=3.893412667821394e-06\n",
            "2023-02-12 14:07:56,191 [nnabla][INFO]: iter=6899 {Training error}=0.0\n",
            "2023-02-12 14:07:56,191 [nnabla][INFO]: iter=6899 {Training time}=0.41835975646972656[sec/100iter] 32.66687369346619[sec]\n",
            "2023-02-12 14:07:56,213 [nnabla][INFO]: iter=6900 {Test error}=0.0234375\n",
            "2023-02-12 14:07:56,251 [nnabla][INFO]: iter=6909 {Training loss}=3.900399860867765e-06\n",
            "2023-02-12 14:07:56,251 [nnabla][INFO]: iter=6909 {Training error}=0.0\n",
            "2023-02-12 14:07:56,292 [nnabla][INFO]: iter=6919 {Training loss}=3.7959994187986013e-06\n",
            "2023-02-12 14:07:56,293 [nnabla][INFO]: iter=6919 {Training error}=0.0\n",
            "2023-02-12 14:07:56,329 [nnabla][INFO]: iter=6929 {Training loss}=3.691696520036203e-06\n",
            "2023-02-12 14:07:56,330 [nnabla][INFO]: iter=6929 {Training error}=0.0\n",
            "2023-02-12 14:07:56,369 [nnabla][INFO]: iter=6939 {Training loss}=3.778492555284174e-06\n",
            "2023-02-12 14:07:56,369 [nnabla][INFO]: iter=6939 {Training error}=0.0\n",
            "2023-02-12 14:07:56,406 [nnabla][INFO]: iter=6949 {Training loss}=3.664315272544627e-06\n",
            "2023-02-12 14:07:56,406 [nnabla][INFO]: iter=6949 {Training error}=0.0\n",
            "2023-02-12 14:07:56,442 [nnabla][INFO]: iter=6959 {Training loss}=3.780450015256065e-06\n",
            "2023-02-12 14:07:56,442 [nnabla][INFO]: iter=6959 {Training error}=0.0\n",
            "2023-02-12 14:07:56,479 [nnabla][INFO]: iter=6969 {Training loss}=3.6294854908192065e-06\n",
            "2023-02-12 14:07:56,479 [nnabla][INFO]: iter=6969 {Training error}=0.0\n",
            "2023-02-12 14:07:56,515 [nnabla][INFO]: iter=6979 {Training loss}=3.6653405004472006e-06\n",
            "2023-02-12 14:07:56,516 [nnabla][INFO]: iter=6979 {Training error}=0.0\n",
            "2023-02-12 14:07:56,555 [nnabla][INFO]: iter=6989 {Training loss}=3.632000925790635e-06\n",
            "2023-02-12 14:07:56,556 [nnabla][INFO]: iter=6989 {Training error}=0.0\n",
            "2023-02-12 14:07:56,591 [nnabla][INFO]: iter=6999 {Training loss}=3.5858095088769915e-06\n",
            "2023-02-12 14:07:56,592 [nnabla][INFO]: iter=6999 {Training error}=0.0\n",
            "2023-02-12 14:07:56,592 [nnabla][INFO]: iter=6999 {Training time}=0.4011270999908447[sec/100iter] 33.06800079345703[sec]\n",
            "2023-02-12 14:07:56,615 [nnabla][INFO]: iter=7000 {Test error}=0.0234375\n",
            "2023-02-12 14:07:56,633 [nnabla][INFO]: Solver state save (.h5): output/states_7000.h5\n",
            "2023-02-12 14:07:56,640 [nnabla][INFO]: Parameter save (.h5): output/params_7000.h5\n",
            "2023-02-12 14:07:56,641 [nnabla][INFO]: Checkpoint save (.json): output/checkpoint_7000.json\n",
            "2023-02-12 14:07:56,678 [nnabla][INFO]: iter=7009 {Training loss}=3.5686716728378087e-06\n",
            "2023-02-12 14:07:56,678 [nnabla][INFO]: iter=7009 {Training error}=0.0\n",
            "2023-02-12 14:07:56,714 [nnabla][INFO]: iter=7019 {Training loss}=3.5379409837332787e-06\n",
            "2023-02-12 14:07:56,714 [nnabla][INFO]: iter=7019 {Training error}=0.0\n",
            "2023-02-12 14:07:56,752 [nnabla][INFO]: iter=7029 {Training loss}=3.589906555134803e-06\n",
            "2023-02-12 14:07:56,752 [nnabla][INFO]: iter=7029 {Training error}=0.0\n",
            "2023-02-12 14:07:56,791 [nnabla][INFO]: iter=7039 {Training loss}=3.50962977790914e-06\n",
            "2023-02-12 14:07:56,791 [nnabla][INFO]: iter=7039 {Training error}=0.0\n",
            "2023-02-12 14:07:56,829 [nnabla][INFO]: iter=7049 {Training loss}=3.499849071886274e-06\n",
            "2023-02-12 14:07:56,829 [nnabla][INFO]: iter=7049 {Training error}=0.0\n",
            "2023-02-12 14:07:56,865 [nnabla][INFO]: iter=7059 {Training loss}=3.4782431157509563e-06\n",
            "2023-02-12 14:07:56,866 [nnabla][INFO]: iter=7059 {Training error}=0.0\n",
            "2023-02-12 14:07:56,902 [nnabla][INFO]: iter=7069 {Training loss}=3.486721197987208e-06\n",
            "2023-02-12 14:07:56,902 [nnabla][INFO]: iter=7069 {Training error}=0.0\n",
            "2023-02-12 14:07:56,940 [nnabla][INFO]: iter=7079 {Training loss}=3.412214482523268e-06\n",
            "2023-02-12 14:07:56,940 [nnabla][INFO]: iter=7079 {Training error}=0.0\n",
            "2023-02-12 14:07:56,981 [nnabla][INFO]: iter=7089 {Training loss}=3.383532657608157e-06\n",
            "2023-02-12 14:07:56,981 [nnabla][INFO]: iter=7089 {Training error}=0.0\n",
            "2023-02-12 14:07:57,018 [nnabla][INFO]: iter=7099 {Training loss}=3.391727659618482e-06\n",
            "2023-02-12 14:07:57,018 [nnabla][INFO]: iter=7099 {Training error}=0.0\n",
            "2023-02-12 14:07:57,018 [nnabla][INFO]: iter=7099 {Training time}=0.42620277404785156[sec/100iter] 33.49420356750488[sec]\n",
            "2023-02-12 14:07:57,040 [nnabla][INFO]: iter=7100 {Test error}=0.0234375\n",
            "2023-02-12 14:07:57,079 [nnabla][INFO]: iter=7109 {Training loss}=3.3055839594453573e-06\n",
            "2023-02-12 14:07:57,079 [nnabla][INFO]: iter=7109 {Training error}=0.0\n",
            "2023-02-12 14:07:57,115 [nnabla][INFO]: iter=7119 {Training loss}=3.43978354067076e-06\n",
            "2023-02-12 14:07:57,115 [nnabla][INFO]: iter=7119 {Training error}=0.0\n",
            "2023-02-12 14:07:57,152 [nnabla][INFO]: iter=7129 {Training loss}=3.2351777008443605e-06\n",
            "2023-02-12 14:07:57,152 [nnabla][INFO]: iter=7129 {Training error}=0.0\n",
            "2023-02-12 14:07:57,188 [nnabla][INFO]: iter=7139 {Training loss}=3.3331507438560948e-06\n",
            "2023-02-12 14:07:57,189 [nnabla][INFO]: iter=7139 {Training error}=0.0\n",
            "2023-02-12 14:07:57,230 [nnabla][INFO]: iter=7149 {Training loss}=3.2805310183903202e-06\n",
            "2023-02-12 14:07:57,230 [nnabla][INFO]: iter=7149 {Training error}=0.0\n",
            "2023-02-12 14:07:57,270 [nnabla][INFO]: iter=7159 {Training loss}=3.225585487598437e-06\n",
            "2023-02-12 14:07:57,271 [nnabla][INFO]: iter=7159 {Training error}=0.0\n",
            "2023-02-12 14:07:57,312 [nnabla][INFO]: iter=7169 {Training loss}=3.2589250622550026e-06\n",
            "2023-02-12 14:07:57,312 [nnabla][INFO]: iter=7169 {Training error}=0.0\n",
            "2023-02-12 14:07:57,348 [nnabla][INFO]: iter=7179 {Training loss}=3.14167482429184e-06\n",
            "2023-02-12 14:07:57,348 [nnabla][INFO]: iter=7179 {Training error}=0.0\n",
            "2023-02-12 14:07:57,385 [nnabla][INFO]: iter=7189 {Training loss}=3.2037939945439575e-06\n",
            "2023-02-12 14:07:57,385 [nnabla][INFO]: iter=7189 {Training error}=0.0\n",
            "2023-02-12 14:07:57,421 [nnabla][INFO]: iter=7199 {Training loss}=3.1982976906874683e-06\n",
            "2023-02-12 14:07:57,421 [nnabla][INFO]: iter=7199 {Training error}=0.0\n",
            "2023-02-12 14:07:57,421 [nnabla][INFO]: iter=7199 {Training time}=0.4029519557952881[sec/100iter] 33.89715552330017[sec]\n",
            "2023-02-12 14:07:57,444 [nnabla][INFO]: iter=7200 {Test error}=0.02265625\n",
            "2023-02-12 14:07:57,482 [nnabla][INFO]: iter=7209 {Training loss}=3.196063062205212e-06\n",
            "2023-02-12 14:07:57,482 [nnabla][INFO]: iter=7209 {Training error}=0.0\n",
            "2023-02-12 14:07:57,518 [nnabla][INFO]: iter=7219 {Training loss}=3.13906866722391e-06\n",
            "2023-02-12 14:07:57,518 [nnabla][INFO]: iter=7219 {Training error}=0.0\n",
            "2023-02-12 14:07:57,560 [nnabla][INFO]: iter=7229 {Training loss}=3.1677514016337227e-06\n",
            "2023-02-12 14:07:57,560 [nnabla][INFO]: iter=7229 {Training error}=0.0\n",
            "2023-02-12 14:07:57,596 [nnabla][INFO]: iter=7239 {Training loss}=3.0058922675380018e-06\n",
            "2023-02-12 14:07:57,597 [nnabla][INFO]: iter=7239 {Training error}=0.0\n",
            "2023-02-12 14:07:57,634 [nnabla][INFO]: iter=7249 {Training loss}=3.056370132981101e-06\n",
            "2023-02-12 14:07:57,634 [nnabla][INFO]: iter=7249 {Training error}=0.0\n",
            "2023-02-12 14:07:57,670 [nnabla][INFO]: iter=7259 {Training loss}=3.1354363727587042e-06\n",
            "2023-02-12 14:07:57,671 [nnabla][INFO]: iter=7259 {Training error}=0.0\n",
            "2023-02-12 14:07:57,707 [nnabla][INFO]: iter=7269 {Training loss}=2.969758952531265e-06\n",
            "2023-02-12 14:07:57,707 [nnabla][INFO]: iter=7269 {Training error}=0.0\n",
            "2023-02-12 14:07:57,748 [nnabla][INFO]: iter=7279 {Training loss}=3.0413752938329708e-06\n",
            "2023-02-12 14:07:57,749 [nnabla][INFO]: iter=7279 {Training error}=0.0\n",
            "2023-02-12 14:07:57,788 [nnabla][INFO]: iter=7289 {Training loss}=3.034017481695628e-06\n",
            "2023-02-12 14:07:57,788 [nnabla][INFO]: iter=7289 {Training error}=0.0\n",
            "2023-02-12 14:07:57,825 [nnabla][INFO]: iter=7299 {Training loss}=2.953833700303221e-06\n",
            "2023-02-12 14:07:57,825 [nnabla][INFO]: iter=7299 {Training error}=0.0\n",
            "2023-02-12 14:07:57,825 [nnabla][INFO]: iter=7299 {Training time}=0.404238224029541[sec/100iter] 34.30139374732971[sec]\n",
            "2023-02-12 14:07:57,846 [nnabla][INFO]: iter=7300 {Test error}=0.0234375\n",
            "2023-02-12 14:07:57,883 [nnabla][INFO]: iter=7309 {Training loss}=2.963611677841982e-06\n",
            "2023-02-12 14:07:57,883 [nnabla][INFO]: iter=7309 {Training error}=0.0\n",
            "2023-02-12 14:07:57,918 [nnabla][INFO]: iter=7319 {Training loss}=2.9158361485315254e-06\n",
            "2023-02-12 14:07:57,918 [nnabla][INFO]: iter=7319 {Training error}=0.0\n",
            "2023-02-12 14:07:57,954 [nnabla][INFO]: iter=7329 {Training loss}=2.950109774246812e-06\n",
            "2023-02-12 14:07:57,954 [nnabla][INFO]: iter=7329 {Training error}=0.0\n",
            "2023-02-12 14:07:57,992 [nnabla][INFO]: iter=7339 {Training loss}=2.8554891287058126e-06\n",
            "2023-02-12 14:07:57,993 [nnabla][INFO]: iter=7339 {Training error}=0.0\n",
            "2023-02-12 14:07:58,032 [nnabla][INFO]: iter=7349 {Training loss}=2.912856416514842e-06\n",
            "2023-02-12 14:07:58,032 [nnabla][INFO]: iter=7349 {Training error}=0.0\n",
            "2023-02-12 14:07:58,071 [nnabla][INFO]: iter=7359 {Training loss}=2.9287818961165613e-06\n",
            "2023-02-12 14:07:58,071 [nnabla][INFO]: iter=7359 {Training error}=0.0\n",
            "2023-02-12 14:07:58,107 [nnabla][INFO]: iter=7369 {Training loss}=2.7534183573152404e-06\n",
            "2023-02-12 14:07:58,107 [nnabla][INFO]: iter=7369 {Training error}=0.0\n",
            "2023-02-12 14:07:58,143 [nnabla][INFO]: iter=7379 {Training loss}=2.8454303446778795e-06\n",
            "2023-02-12 14:07:58,144 [nnabla][INFO]: iter=7379 {Training error}=0.0\n",
            "2023-02-12 14:07:58,180 [nnabla][INFO]: iter=7389 {Training loss}=2.860890617739642e-06\n",
            "2023-02-12 14:07:58,180 [nnabla][INFO]: iter=7389 {Training error}=0.0\n",
            "2023-02-12 14:07:58,217 [nnabla][INFO]: iter=7399 {Training loss}=2.7681330720952246e-06\n",
            "2023-02-12 14:07:58,217 [nnabla][INFO]: iter=7399 {Training error}=0.0\n",
            "2023-02-12 14:07:58,217 [nnabla][INFO]: iter=7399 {Training time}=0.3916432857513428[sec/100iter] 34.693037033081055[sec]\n",
            "2023-02-12 14:07:58,238 [nnabla][INFO]: iter=7400 {Test error}=0.0234375\n",
            "2023-02-12 14:07:58,285 [nnabla][INFO]: iter=7409 {Training loss}=2.7686921839631395e-06\n",
            "2023-02-12 14:07:58,285 [nnabla][INFO]: iter=7409 {Training error}=0.0\n",
            "2023-02-12 14:07:58,324 [nnabla][INFO]: iter=7419 {Training loss}=2.7088096885563573e-06\n",
            "2023-02-12 14:07:58,324 [nnabla][INFO]: iter=7419 {Training error}=0.0\n",
            "2023-02-12 14:07:58,360 [nnabla][INFO]: iter=7429 {Training loss}=2.7840576422022423e-06\n",
            "2023-02-12 14:07:58,361 [nnabla][INFO]: iter=7429 {Training error}=0.0\n",
            "2023-02-12 14:07:58,396 [nnabla][INFO]: iter=7439 {Training loss}=2.665225338205346e-06\n",
            "2023-02-12 14:07:58,396 [nnabla][INFO]: iter=7439 {Training error}=0.0\n",
            "2023-02-12 14:07:58,432 [nnabla][INFO]: iter=7449 {Training loss}=2.739727960943128e-06\n",
            "2023-02-12 14:07:58,432 [nnabla][INFO]: iter=7449 {Training error}=0.0\n",
            "2023-02-12 14:07:58,468 [nnabla][INFO]: iter=7459 {Training loss}=2.718122914302512e-06\n",
            "2023-02-12 14:07:58,468 [nnabla][INFO]: iter=7459 {Training error}=0.0\n",
            "2023-02-12 14:07:58,504 [nnabla][INFO]: iter=7469 {Training loss}=2.6749103199108504e-06\n",
            "2023-02-12 14:07:58,504 [nnabla][INFO]: iter=7469 {Training error}=0.0\n",
            "2023-02-12 14:07:58,547 [nnabla][INFO]: iter=7479 {Training loss}=2.581966555226245e-06\n",
            "2023-02-12 14:07:58,547 [nnabla][INFO]: iter=7479 {Training error}=0.0\n",
            "2023-02-12 14:07:58,586 [nnabla][INFO]: iter=7489 {Training loss}=2.6479028747417033e-06\n",
            "2023-02-12 14:07:58,587 [nnabla][INFO]: iter=7489 {Training error}=0.0\n",
            "2023-02-12 14:07:58,624 [nnabla][INFO]: iter=7499 {Training loss}=2.7131854949402623e-06\n",
            "2023-02-12 14:07:58,624 [nnabla][INFO]: iter=7499 {Training error}=0.0\n",
            "2023-02-12 14:07:58,624 [nnabla][INFO]: iter=7499 {Training time}=0.40709590911865234[sec/100iter] 35.10013294219971[sec]\n",
            "2023-02-12 14:07:58,645 [nnabla][INFO]: iter=7500 {Test error}=0.0234375\n",
            "2023-02-12 14:07:58,681 [nnabla][INFO]: iter=7509 {Training loss}=2.5581255158613203e-06\n",
            "2023-02-12 14:07:58,681 [nnabla][INFO]: iter=7509 {Training error}=0.0\n",
            "2023-02-12 14:07:58,717 [nnabla][INFO]: iter=7519 {Training loss}=2.632443738548318e-06\n",
            "2023-02-12 14:07:58,717 [nnabla][INFO]: iter=7519 {Training error}=0.0\n",
            "2023-02-12 14:07:58,753 [nnabla][INFO]: iter=7529 {Training loss}=2.5632464257796528e-06\n",
            "2023-02-12 14:07:58,753 [nnabla][INFO]: iter=7529 {Training error}=0.0\n",
            "2023-02-12 14:07:58,791 [nnabla][INFO]: iter=7539 {Training loss}=2.532794496801216e-06\n",
            "2023-02-12 14:07:58,791 [nnabla][INFO]: iter=7539 {Training error}=0.0\n",
            "2023-02-12 14:07:58,828 [nnabla][INFO]: iter=7549 {Training loss}=2.5016884137585294e-06\n",
            "2023-02-12 14:07:58,828 [nnabla][INFO]: iter=7549 {Training error}=0.0\n",
            "2023-02-12 14:07:58,863 [nnabla][INFO]: iter=7559 {Training loss}=2.5745162020029966e-06\n",
            "2023-02-12 14:07:58,864 [nnabla][INFO]: iter=7559 {Training error}=0.0\n",
            "2023-02-12 14:07:58,905 [nnabla][INFO]: iter=7569 {Training loss}=2.518452674848959e-06\n",
            "2023-02-12 14:07:58,905 [nnabla][INFO]: iter=7569 {Training error}=0.0\n",
            "2023-02-12 14:07:58,941 [nnabla][INFO]: iter=7579 {Training loss}=2.489767894076067e-06\n",
            "2023-02-12 14:07:58,941 [nnabla][INFO]: iter=7579 {Training error}=0.0\n",
            "2023-02-12 14:07:58,983 [nnabla][INFO]: iter=7589 {Training loss}=2.497404238965828e-06\n",
            "2023-02-12 14:07:58,984 [nnabla][INFO]: iter=7589 {Training error}=0.0\n",
            "2023-02-12 14:07:59,020 [nnabla][INFO]: iter=7599 {Training loss}=2.4200137431762414e-06\n",
            "2023-02-12 14:07:59,020 [nnabla][INFO]: iter=7599 {Training error}=0.0\n",
            "2023-02-12 14:07:59,020 [nnabla][INFO]: iter=7599 {Training time}=0.39604830741882324[sec/100iter] 35.49618124961853[sec]\n",
            "2023-02-12 14:07:59,041 [nnabla][INFO]: iter=7600 {Test error}=0.02265625\n",
            "2023-02-12 14:07:59,078 [nnabla][INFO]: iter=7609 {Training loss}=2.4524219952581916e-06\n",
            "2023-02-12 14:07:59,079 [nnabla][INFO]: iter=7609 {Training error}=0.0\n",
            "2023-02-12 14:07:59,114 [nnabla][INFO]: iter=7619 {Training loss}=2.4993601073219907e-06\n",
            "2023-02-12 14:07:59,115 [nnabla][INFO]: iter=7619 {Training error}=0.0\n",
            "2023-02-12 14:07:59,150 [nnabla][INFO]: iter=7629 {Training loss}=2.3761494958307594e-06\n",
            "2023-02-12 14:07:59,151 [nnabla][INFO]: iter=7629 {Training error}=0.0\n",
            "2023-02-12 14:07:59,191 [nnabla][INFO]: iter=7639 {Training loss}=2.436684098938713e-06\n",
            "2023-02-12 14:07:59,191 [nnabla][INFO]: iter=7639 {Training error}=0.0\n",
            "2023-02-12 14:07:59,228 [nnabla][INFO]: iter=7649 {Training loss}=2.3550082914880477e-06\n",
            "2023-02-12 14:07:59,228 [nnabla][INFO]: iter=7649 {Training error}=0.0\n",
            "2023-02-12 14:07:59,264 [nnabla][INFO]: iter=7659 {Training loss}=2.411724608464283e-06\n",
            "2023-02-12 14:07:59,264 [nnabla][INFO]: iter=7659 {Training error}=0.0\n",
            "2023-02-12 14:07:59,303 [nnabla][INFO]: iter=7669 {Training loss}=2.311703383384156e-06\n",
            "2023-02-12 14:07:59,303 [nnabla][INFO]: iter=7669 {Training error}=0.0\n",
            "2023-02-12 14:07:59,343 [nnabla][INFO]: iter=7679 {Training loss}=2.3272564249054994e-06\n",
            "2023-02-12 14:07:59,343 [nnabla][INFO]: iter=7679 {Training error}=0.0\n",
            "2023-02-12 14:07:59,379 [nnabla][INFO]: iter=7689 {Training loss}=2.3786637939338107e-06\n",
            "2023-02-12 14:07:59,379 [nnabla][INFO]: iter=7689 {Training error}=0.0\n",
            "2023-02-12 14:07:59,416 [nnabla][INFO]: iter=7699 {Training loss}=2.25982921620016e-06\n",
            "2023-02-12 14:07:59,416 [nnabla][INFO]: iter=7699 {Training error}=0.0\n",
            "2023-02-12 14:07:59,417 [nnabla][INFO]: iter=7699 {Training time}=0.39650464057922363[sec/100iter] 35.892685890197754[sec]\n",
            "2023-02-12 14:07:59,442 [nnabla][INFO]: iter=7700 {Test error}=0.0234375\n",
            "2023-02-12 14:07:59,479 [nnabla][INFO]: iter=7709 {Training loss}=2.295591912115924e-06\n",
            "2023-02-12 14:07:59,479 [nnabla][INFO]: iter=7709 {Training error}=0.0\n",
            "2023-02-12 14:07:59,520 [nnabla][INFO]: iter=7719 {Training loss}=2.2814360818301793e-06\n",
            "2023-02-12 14:07:59,520 [nnabla][INFO]: iter=7719 {Training error}=0.0\n",
            "2023-02-12 14:07:59,557 [nnabla][INFO]: iter=7729 {Training loss}=2.31310082199343e-06\n",
            "2023-02-12 14:07:59,557 [nnabla][INFO]: iter=7729 {Training error}=0.0\n",
            "2023-02-12 14:07:59,593 [nnabla][INFO]: iter=7739 {Training loss}=2.213543666584883e-06\n",
            "2023-02-12 14:07:59,594 [nnabla][INFO]: iter=7739 {Training error}=0.0\n",
            "2023-02-12 14:07:59,631 [nnabla][INFO]: iter=7749 {Training loss}=2.263461738039041e-06\n",
            "2023-02-12 14:07:59,631 [nnabla][INFO]: iter=7749 {Training error}=0.0\n",
            "2023-02-12 14:07:59,667 [nnabla][INFO]: iter=7759 {Training loss}=2.1867228952032747e-06\n",
            "2023-02-12 14:07:59,667 [nnabla][INFO]: iter=7759 {Training error}=0.0\n",
            "2023-02-12 14:07:59,713 [nnabla][INFO]: iter=7769 {Training loss}=2.1918444872426335e-06\n",
            "2023-02-12 14:07:59,713 [nnabla][INFO]: iter=7769 {Training error}=0.0\n",
            "2023-02-12 14:07:59,749 [nnabla][INFO]: iter=7779 {Training loss}=2.2910289771971293e-06\n",
            "2023-02-12 14:07:59,749 [nnabla][INFO]: iter=7779 {Training error}=0.0\n",
            "2023-02-12 14:07:59,785 [nnabla][INFO]: iter=7789 {Training loss}=2.1493765416380484e-06\n",
            "2023-02-12 14:07:59,786 [nnabla][INFO]: iter=7789 {Training error}=0.0\n",
            "2023-02-12 14:07:59,822 [nnabla][INFO]: iter=7799 {Training loss}=2.124325419572415e-06\n",
            "2023-02-12 14:07:59,822 [nnabla][INFO]: iter=7799 {Training error}=0.0\n",
            "2023-02-12 14:07:59,823 [nnabla][INFO]: iter=7799 {Training time}=0.4059884548187256[sec/100iter] 36.29867434501648[sec]\n",
            "2023-02-12 14:07:59,843 [nnabla][INFO]: iter=7800 {Test error}=0.0234375\n",
            "2023-02-12 14:07:59,879 [nnabla][INFO]: iter=7809 {Training loss}=2.182251819249359e-06\n",
            "2023-02-12 14:07:59,880 [nnabla][INFO]: iter=7809 {Training error}=0.0\n",
            "2023-02-12 14:07:59,916 [nnabla][INFO]: iter=7819 {Training loss}=2.1944531454209937e-06\n",
            "2023-02-12 14:07:59,916 [nnabla][INFO]: iter=7819 {Training error}=0.0\n",
            "2023-02-12 14:07:59,952 [nnabla][INFO]: iter=7829 {Training loss}=2.1101682250446174e-06\n",
            "2023-02-12 14:07:59,952 [nnabla][INFO]: iter=7829 {Training error}=0.0\n",
            "2023-02-12 14:07:59,990 [nnabla][INFO]: iter=7839 {Training loss}=2.136618377335253e-06\n",
            "2023-02-12 14:07:59,990 [nnabla][INFO]: iter=7839 {Training error}=0.0\n",
            "2023-02-12 14:08:00,029 [nnabla][INFO]: iter=7849 {Training loss}=2.088469500449719e-06\n",
            "2023-02-12 14:08:00,029 [nnabla][INFO]: iter=7849 {Training error}=0.0\n",
            "2023-02-12 14:08:00,071 [nnabla][INFO]: iter=7859 {Training loss}=2.1196688066993374e-06\n",
            "2023-02-12 14:08:00,071 [nnabla][INFO]: iter=7859 {Training error}=0.0\n",
            "2023-02-12 14:08:00,107 [nnabla][INFO]: iter=7869 {Training loss}=2.0518696146609727e-06\n",
            "2023-02-12 14:08:00,107 [nnabla][INFO]: iter=7869 {Training error}=0.0\n",
            "2023-02-12 14:08:00,143 [nnabla][INFO]: iter=7879 {Training loss}=2.055688355540042e-06\n",
            "2023-02-12 14:08:00,144 [nnabla][INFO]: iter=7879 {Training error}=0.0\n",
            "2023-02-12 14:08:00,179 [nnabla][INFO]: iter=7889 {Training loss}=2.0632321593438974e-06\n",
            "2023-02-12 14:08:00,180 [nnabla][INFO]: iter=7889 {Training error}=0.0\n",
            "2023-02-12 14:08:00,220 [nnabla][INFO]: iter=7899 {Training loss}=2.0409731860127067e-06\n",
            "2023-02-12 14:08:00,220 [nnabla][INFO]: iter=7899 {Training error}=0.0\n",
            "2023-02-12 14:08:00,220 [nnabla][INFO]: iter=7899 {Training time}=0.3973860740661621[sec/100iter] 36.69606041908264[sec]\n",
            "2023-02-12 14:08:00,241 [nnabla][INFO]: iter=7900 {Test error}=0.0234375\n",
            "2023-02-12 14:08:00,278 [nnabla][INFO]: iter=7909 {Training loss}=2.0014856545458315e-06\n",
            "2023-02-12 14:08:00,278 [nnabla][INFO]: iter=7909 {Training error}=0.0\n",
            "2023-02-12 14:08:00,314 [nnabla][INFO]: iter=7919 {Training loss}=2.0446061625989387e-06\n",
            "2023-02-12 14:08:00,314 [nnabla][INFO]: iter=7919 {Training error}=0.0\n",
            "2023-02-12 14:08:00,361 [nnabla][INFO]: iter=7929 {Training loss}=2.0050251805514563e-06\n",
            "2023-02-12 14:08:00,361 [nnabla][INFO]: iter=7929 {Training error}=0.0\n",
            "2023-02-12 14:08:00,398 [nnabla][INFO]: iter=7939 {Training loss}=1.929030759129091e-06\n",
            "2023-02-12 14:08:00,398 [nnabla][INFO]: iter=7939 {Training error}=0.0\n",
            "2023-02-12 14:08:00,434 [nnabla][INFO]: iter=7949 {Training loss}=2.0894015051453607e-06\n",
            "2023-02-12 14:08:00,435 [nnabla][INFO]: iter=7949 {Training error}=0.0\n",
            "2023-02-12 14:08:00,471 [nnabla][INFO]: iter=7959 {Training loss}=1.942068820426357e-06\n",
            "2023-02-12 14:08:00,471 [nnabla][INFO]: iter=7959 {Training error}=0.0\n",
            "2023-02-12 14:08:00,513 [nnabla][INFO]: iter=7969 {Training loss}=2.011264996326645e-06\n",
            "2023-02-12 14:08:00,513 [nnabla][INFO]: iter=7969 {Training error}=0.0\n",
            "2023-02-12 14:08:00,550 [nnabla][INFO]: iter=7979 {Training loss}=1.8571338387118885e-06\n",
            "2023-02-12 14:08:00,550 [nnabla][INFO]: iter=7979 {Training error}=0.0\n",
            "2023-02-12 14:08:00,585 [nnabla][INFO]: iter=7989 {Training loss}=1.925212472997373e-06\n",
            "2023-02-12 14:08:00,585 [nnabla][INFO]: iter=7989 {Training error}=0.0\n",
            "2023-02-12 14:08:00,625 [nnabla][INFO]: iter=7999 {Training loss}=1.8600205748953158e-06\n",
            "2023-02-12 14:08:00,625 [nnabla][INFO]: iter=7999 {Training error}=0.0\n",
            "2023-02-12 14:08:00,625 [nnabla][INFO]: iter=7999 {Training time}=0.4053995609283447[sec/100iter] 37.101459980010986[sec]\n",
            "2023-02-12 14:08:00,658 [nnabla][INFO]: iter=8000 {Test error}=0.0234375\n",
            "2023-02-12 14:08:00,672 [nnabla][INFO]: Solver state save (.h5): output/states_8000.h5\n",
            "2023-02-12 14:08:00,680 [nnabla][INFO]: Parameter save (.h5): output/params_8000.h5\n",
            "2023-02-12 14:08:00,680 [nnabla][INFO]: Checkpoint save (.json): output/checkpoint_8000.json\n",
            "2023-02-12 14:08:00,720 [nnabla][INFO]: iter=8009 {Training loss}=1.947750206454657e-06\n",
            "2023-02-12 14:08:00,720 [nnabla][INFO]: iter=8009 {Training error}=0.0\n",
            "2023-02-12 14:08:00,756 [nnabla][INFO]: iter=8019 {Training loss}=1.9201829672965687e-06\n",
            "2023-02-12 14:08:00,756 [nnabla][INFO]: iter=8019 {Training error}=0.0\n",
            "2023-02-12 14:08:00,791 [nnabla][INFO]: iter=8029 {Training loss}=1.862628437265812e-06\n",
            "2023-02-12 14:08:00,791 [nnabla][INFO]: iter=8029 {Training error}=0.0\n",
            "2023-02-12 14:08:00,827 [nnabla][INFO]: iter=8039 {Training loss}=1.9131061890220735e-06\n",
            "2023-02-12 14:08:00,827 [nnabla][INFO]: iter=8039 {Training error}=0.0\n",
            "2023-02-12 14:08:00,862 [nnabla][INFO]: iter=8049 {Training loss}=1.8372966223978437e-06\n",
            "2023-02-12 14:08:00,862 [nnabla][INFO]: iter=8049 {Training error}=0.0\n",
            "2023-02-12 14:08:00,897 [nnabla][INFO]: iter=8059 {Training loss}=1.853502340054547e-06\n",
            "2023-02-12 14:08:00,898 [nnabla][INFO]: iter=8059 {Training error}=0.0\n",
            "2023-02-12 14:08:00,937 [nnabla][INFO]: iter=8069 {Training loss}=1.8359935438638786e-06\n",
            "2023-02-12 14:08:00,937 [nnabla][INFO]: iter=8069 {Training error}=0.0\n",
            "2023-02-12 14:08:00,975 [nnabla][INFO]: iter=8079 {Training loss}=1.8039556834992254e-06\n",
            "2023-02-12 14:08:00,975 [nnabla][INFO]: iter=8079 {Training error}=0.0\n",
            "2023-02-12 14:08:01,012 [nnabla][INFO]: iter=8089 {Training loss}=1.813362132452312e-06\n",
            "2023-02-12 14:08:01,012 [nnabla][INFO]: iter=8089 {Training error}=0.0\n",
            "2023-02-12 14:08:01,048 [nnabla][INFO]: iter=8099 {Training loss}=1.8096374105880386e-06\n",
            "2023-02-12 14:08:01,048 [nnabla][INFO]: iter=8099 {Training error}=0.0\n",
            "2023-02-12 14:08:01,048 [nnabla][INFO]: iter=8099 {Training time}=0.4226090908050537[sec/100iter] 37.52406907081604[sec]\n",
            "2023-02-12 14:08:01,068 [nnabla][INFO]: iter=8100 {Test error}=0.02265625\n",
            "2023-02-12 14:08:01,104 [nnabla][INFO]: iter=8109 {Training loss}=1.7770416889106855e-06\n",
            "2023-02-12 14:08:01,104 [nnabla][INFO]: iter=8109 {Training error}=0.0\n",
            "2023-02-12 14:08:01,139 [nnabla][INFO]: iter=8119 {Training loss}=1.7576700201971107e-06\n",
            "2023-02-12 14:08:01,140 [nnabla][INFO]: iter=8119 {Training error}=0.0\n",
            "2023-02-12 14:08:01,175 [nnabla][INFO]: iter=8129 {Training loss}=1.8337581195737584e-06\n",
            "2023-02-12 14:08:01,175 [nnabla][INFO]: iter=8129 {Training error}=0.0\n",
            "2023-02-12 14:08:01,214 [nnabla][INFO]: iter=8139 {Training loss}=1.7469601516495459e-06\n",
            "2023-02-12 14:08:01,214 [nnabla][INFO]: iter=8139 {Training error}=0.0\n",
            "2023-02-12 14:08:01,249 [nnabla][INFO]: iter=8149 {Training loss}=1.738950913932058e-06\n",
            "2023-02-12 14:08:01,249 [nnabla][INFO]: iter=8149 {Training error}=0.0\n",
            "2023-02-12 14:08:01,285 [nnabla][INFO]: iter=8159 {Training loss}=1.6951796624198323e-06\n",
            "2023-02-12 14:08:01,285 [nnabla][INFO]: iter=8159 {Training error}=0.0\n",
            "2023-02-12 14:08:01,321 [nnabla][INFO]: iter=8169 {Training loss}=1.7459358332416741e-06\n",
            "2023-02-12 14:08:01,321 [nnabla][INFO]: iter=8169 {Training error}=0.0\n",
            "2023-02-12 14:08:01,359 [nnabla][INFO]: iter=8179 {Training loss}=1.721163471302134e-06\n",
            "2023-02-12 14:08:01,359 [nnabla][INFO]: iter=8179 {Training error}=0.0\n",
            "2023-02-12 14:08:01,397 [nnabla][INFO]: iter=8189 {Training loss}=1.6986250557238236e-06\n",
            "2023-02-12 14:08:01,397 [nnabla][INFO]: iter=8189 {Training error}=0.0\n",
            "2023-02-12 14:08:01,433 [nnabla][INFO]: iter=8199 {Training loss}=1.650383637752384e-06\n",
            "2023-02-12 14:08:01,433 [nnabla][INFO]: iter=8199 {Training error}=0.0\n",
            "2023-02-12 14:08:01,433 [nnabla][INFO]: iter=8199 {Training time}=0.38543272018432617[sec/100iter] 37.909501791000366[sec]\n",
            "2023-02-12 14:08:01,455 [nnabla][INFO]: iter=8200 {Test error}=0.0234375\n",
            "2023-02-12 14:08:01,491 [nnabla][INFO]: iter=8209 {Training loss}=1.7743411717674462e-06\n",
            "2023-02-12 14:08:01,491 [nnabla][INFO]: iter=8209 {Training error}=0.0\n",
            "2023-02-12 14:08:01,531 [nnabla][INFO]: iter=8219 {Training loss}=1.6140623984028935e-06\n",
            "2023-02-12 14:08:01,531 [nnabla][INFO]: iter=8219 {Training error}=0.0\n",
            "2023-02-12 14:08:01,567 [nnabla][INFO]: iter=8229 {Training loss}=1.6741321360314032e-06\n",
            "2023-02-12 14:08:01,568 [nnabla][INFO]: iter=8229 {Training error}=0.0\n",
            "2023-02-12 14:08:01,603 [nnabla][INFO]: iter=8239 {Training loss}=1.6616522771073505e-06\n",
            "2023-02-12 14:08:01,604 [nnabla][INFO]: iter=8239 {Training error}=0.0\n",
            "2023-02-12 14:08:01,641 [nnabla][INFO]: iter=8249 {Training loss}=1.6403253084718017e-06\n",
            "2023-02-12 14:08:01,641 [nnabla][INFO]: iter=8249 {Training error}=0.0\n",
            "2023-02-12 14:08:01,676 [nnabla][INFO]: iter=8259 {Training loss}=1.6493595467181876e-06\n",
            "2023-02-12 14:08:01,677 [nnabla][INFO]: iter=8259 {Training error}=0.0\n",
            "2023-02-12 14:08:01,713 [nnabla][INFO]: iter=8269 {Training loss}=1.6279391275020316e-06\n",
            "2023-02-12 14:08:01,713 [nnabla][INFO]: iter=8269 {Training error}=0.0\n",
            "2023-02-12 14:08:01,748 [nnabla][INFO]: iter=8279 {Training loss}=1.6059600511653116e-06\n",
            "2023-02-12 14:08:01,748 [nnabla][INFO]: iter=8279 {Training error}=0.0\n",
            "2023-02-12 14:08:01,789 [nnabla][INFO]: iter=8289 {Training loss}=1.6210476587730227e-06\n",
            "2023-02-12 14:08:01,789 [nnabla][INFO]: iter=8289 {Training error}=0.0\n",
            "2023-02-12 14:08:01,829 [nnabla][INFO]: iter=8299 {Training loss}=1.5821187844267115e-06\n",
            "2023-02-12 14:08:01,829 [nnabla][INFO]: iter=8299 {Training error}=0.0\n",
            "2023-02-12 14:08:01,829 [nnabla][INFO]: iter=8299 {Training time}=0.39607954025268555[sec/100iter] 38.30558133125305[sec]\n",
            "2023-02-12 14:08:01,851 [nnabla][INFO]: iter=8300 {Test error}=0.0234375\n",
            "2023-02-12 14:08:01,887 [nnabla][INFO]: iter=8309 {Training loss}=1.5585567325615557e-06\n",
            "2023-02-12 14:08:01,887 [nnabla][INFO]: iter=8309 {Training error}=0.0\n",
            "2023-02-12 14:08:01,924 [nnabla][INFO]: iter=8319 {Training loss}=1.5776483905938221e-06\n",
            "2023-02-12 14:08:01,924 [nnabla][INFO]: iter=8319 {Training error}=0.0\n",
            "2023-02-12 14:08:01,960 [nnabla][INFO]: iter=8329 {Training loss}=1.5750414377180277e-06\n",
            "2023-02-12 14:08:01,960 [nnabla][INFO]: iter=8329 {Training error}=0.0\n",
            "2023-02-12 14:08:01,996 [nnabla][INFO]: iter=8339 {Training loss}=1.5604193777107866e-06\n",
            "2023-02-12 14:08:01,996 [nnabla][INFO]: iter=8339 {Training error}=0.0\n",
            "2023-02-12 14:08:02,036 [nnabla][INFO]: iter=8349 {Training loss}=1.5527828054473503e-06\n",
            "2023-02-12 14:08:02,036 [nnabla][INFO]: iter=8349 {Training error}=0.0\n",
            "2023-02-12 14:08:02,080 [nnabla][INFO]: iter=8359 {Training loss}=1.5952504099914222e-06\n",
            "2023-02-12 14:08:02,080 [nnabla][INFO]: iter=8359 {Training error}=0.0\n",
            "2023-02-12 14:08:02,117 [nnabla][INFO]: iter=8369 {Training loss}=1.4446570730797248e-06\n",
            "2023-02-12 14:08:02,117 [nnabla][INFO]: iter=8369 {Training error}=0.0\n",
            "2023-02-12 14:08:02,153 [nnabla][INFO]: iter=8379 {Training loss}=1.5070556855789619e-06\n",
            "2023-02-12 14:08:02,153 [nnabla][INFO]: iter=8379 {Training error}=0.0\n",
            "2023-02-12 14:08:02,188 [nnabla][INFO]: iter=8389 {Training loss}=1.5416073892993154e-06\n",
            "2023-02-12 14:08:02,188 [nnabla][INFO]: iter=8389 {Training error}=0.0\n",
            "2023-02-12 14:08:02,224 [nnabla][INFO]: iter=8399 {Training loss}=1.4971833479648922e-06\n",
            "2023-02-12 14:08:02,224 [nnabla][INFO]: iter=8399 {Training error}=0.0\n",
            "2023-02-12 14:08:02,224 [nnabla][INFO]: iter=8399 {Training time}=0.3950209617614746[sec/100iter] 38.700602293014526[sec]\n",
            "2023-02-12 14:08:02,245 [nnabla][INFO]: iter=8400 {Test error}=0.0234375\n",
            "2023-02-12 14:08:02,280 [nnabla][INFO]: iter=8409 {Training loss}=1.4716656551172491e-06\n",
            "2023-02-12 14:08:02,280 [nnabla][INFO]: iter=8409 {Training error}=0.0\n",
            "2023-02-12 14:08:02,318 [nnabla][INFO]: iter=8419 {Training loss}=1.5014675227575935e-06\n",
            "2023-02-12 14:08:02,319 [nnabla][INFO]: iter=8419 {Training error}=0.0\n",
            "2023-02-12 14:08:02,358 [nnabla][INFO]: iter=8429 {Training loss}=1.430781026101613e-06\n",
            "2023-02-12 14:08:02,358 [nnabla][INFO]: iter=8429 {Training error}=0.0\n",
            "2023-02-12 14:08:02,398 [nnabla][INFO]: iter=8439 {Training loss}=1.4664504988104454e-06\n",
            "2023-02-12 14:08:02,398 [nnabla][INFO]: iter=8439 {Training error}=0.0\n",
            "2023-02-12 14:08:02,434 [nnabla][INFO]: iter=8449 {Training loss}=1.4521081084239995e-06\n",
            "2023-02-12 14:08:02,434 [nnabla][INFO]: iter=8449 {Training error}=0.0\n",
            "2023-02-12 14:08:02,469 [nnabla][INFO]: iter=8459 {Training loss}=1.4269623989093816e-06\n",
            "2023-02-12 14:08:02,469 [nnabla][INFO]: iter=8459 {Training error}=0.0\n",
            "2023-02-12 14:08:02,505 [nnabla][INFO]: iter=8469 {Training loss}=1.474180521654489e-06\n",
            "2023-02-12 14:08:02,505 [nnabla][INFO]: iter=8469 {Training error}=0.0\n",
            "2023-02-12 14:08:02,543 [nnabla][INFO]: iter=8479 {Training loss}=1.4244483281800058e-06\n",
            "2023-02-12 14:08:02,543 [nnabla][INFO]: iter=8479 {Training error}=0.0\n",
            "2023-02-12 14:08:02,579 [nnabla][INFO]: iter=8489 {Training loss}=1.4071256373426877e-06\n",
            "2023-02-12 14:08:02,579 [nnabla][INFO]: iter=8489 {Training error}=0.0\n",
            "2023-02-12 14:08:02,616 [nnabla][INFO]: iter=8499 {Training loss}=1.4272425232775277e-06\n",
            "2023-02-12 14:08:02,616 [nnabla][INFO]: iter=8499 {Training error}=0.0\n",
            "2023-02-12 14:08:02,616 [nnabla][INFO]: iter=8499 {Training time}=0.39145350456237793[sec/100iter] 39.092055797576904[sec]\n",
            "2023-02-12 14:08:02,638 [nnabla][INFO]: iter=8500 {Test error}=0.02265625\n",
            "2023-02-12 14:08:02,680 [nnabla][INFO]: iter=8509 {Training loss}=1.3933423588241567e-06\n",
            "2023-02-12 14:08:02,680 [nnabla][INFO]: iter=8509 {Training error}=0.0\n",
            "2023-02-12 14:08:02,716 [nnabla][INFO]: iter=8519 {Training loss}=1.3545997035180335e-06\n",
            "2023-02-12 14:08:02,716 [nnabla][INFO]: iter=8519 {Training error}=0.0\n",
            "2023-02-12 14:08:02,751 [nnabla][INFO]: iter=8529 {Training loss}=1.3912007261751569e-06\n",
            "2023-02-12 14:08:02,751 [nnabla][INFO]: iter=8529 {Training error}=0.0\n",
            "2023-02-12 14:08:02,787 [nnabla][INFO]: iter=8539 {Training loss}=1.3716428384213941e-06\n",
            "2023-02-12 14:08:02,787 [nnabla][INFO]: iter=8539 {Training error}=0.0\n",
            "2023-02-12 14:08:02,827 [nnabla][INFO]: iter=8549 {Training loss}=1.3953912230135757e-06\n",
            "2023-02-12 14:08:02,827 [nnabla][INFO]: iter=8549 {Training error}=0.0\n",
            "2023-02-12 14:08:02,862 [nnabla][INFO]: iter=8559 {Training loss}=1.3503158697858453e-06\n",
            "2023-02-12 14:08:02,862 [nnabla][INFO]: iter=8559 {Training error}=0.0\n",
            "2023-02-12 14:08:02,897 [nnabla][INFO]: iter=8569 {Training loss}=1.322749426435621e-06\n",
            "2023-02-12 14:08:02,898 [nnabla][INFO]: iter=8569 {Training error}=0.0\n",
            "2023-02-12 14:08:02,932 [nnabla][INFO]: iter=8579 {Training loss}=1.3633541584567865e-06\n",
            "2023-02-12 14:08:02,933 [nnabla][INFO]: iter=8579 {Training error}=0.0\n",
            "2023-02-12 14:08:02,973 [nnabla][INFO]: iter=8589 {Training loss}=1.3055196177447215e-06\n",
            "2023-02-12 14:08:02,973 [nnabla][INFO]: iter=8589 {Training error}=0.0\n",
            "2023-02-12 14:08:03,008 [nnabla][INFO]: iter=8599 {Training loss}=1.3462181414070074e-06\n",
            "2023-02-12 14:08:03,008 [nnabla][INFO]: iter=8599 {Training error}=0.0\n",
            "2023-02-12 14:08:03,008 [nnabla][INFO]: iter=8599 {Training time}=0.3925471305847168[sec/100iter] 39.48460292816162[sec]\n",
            "2023-02-12 14:08:03,035 [nnabla][INFO]: iter=8600 {Test error}=0.0234375\n",
            "2023-02-12 14:08:03,070 [nnabla][INFO]: iter=8609 {Training loss}=1.2963930657861056e-06\n",
            "2023-02-12 14:08:03,070 [nnabla][INFO]: iter=8609 {Training error}=0.0\n",
            "2023-02-12 14:08:03,106 [nnabla][INFO]: iter=8619 {Training loss}=1.3400716625255882e-06\n",
            "2023-02-12 14:08:03,106 [nnabla][INFO]: iter=8619 {Training error}=0.0\n",
            "2023-02-12 14:08:03,141 [nnabla][INFO]: iter=8629 {Training loss}=1.2683601653407095e-06\n",
            "2023-02-12 14:08:03,142 [nnabla][INFO]: iter=8629 {Training error}=0.0\n",
            "2023-02-12 14:08:03,177 [nnabla][INFO]: iter=8639 {Training loss}=1.2985350394956185e-06\n",
            "2023-02-12 14:08:03,177 [nnabla][INFO]: iter=8639 {Training error}=0.0\n",
            "2023-02-12 14:08:03,212 [nnabla][INFO]: iter=8649 {Training loss}=1.322655862168176e-06\n",
            "2023-02-12 14:08:03,213 [nnabla][INFO]: iter=8649 {Training error}=0.0\n",
            "2023-02-12 14:08:03,248 [nnabla][INFO]: iter=8659 {Training loss}=1.2413522654242115e-06\n",
            "2023-02-12 14:08:03,249 [nnabla][INFO]: iter=8659 {Training error}=0.0\n",
            "2023-02-12 14:08:03,284 [nnabla][INFO]: iter=8669 {Training loss}=1.3015155673201662e-06\n",
            "2023-02-12 14:08:03,284 [nnabla][INFO]: iter=8669 {Training error}=0.0\n",
            "2023-02-12 14:08:03,319 [nnabla][INFO]: iter=8679 {Training loss}=1.2650077678699745e-06\n",
            "2023-02-12 14:08:03,320 [nnabla][INFO]: iter=8679 {Training error}=0.0\n",
            "2023-02-12 14:08:03,358 [nnabla][INFO]: iter=8689 {Training loss}=1.2412592695909552e-06\n",
            "2023-02-12 14:08:03,359 [nnabla][INFO]: iter=8689 {Training error}=0.0\n",
            "2023-02-12 14:08:03,401 [nnabla][INFO]: iter=8699 {Training loss}=1.2164863392172265e-06\n",
            "2023-02-12 14:08:03,401 [nnabla][INFO]: iter=8699 {Training error}=0.0\n",
            "2023-02-12 14:08:03,401 [nnabla][INFO]: iter=8699 {Training time}=0.39297008514404297[sec/100iter] 39.877573013305664[sec]\n",
            "2023-02-12 14:08:03,425 [nnabla][INFO]: iter=8700 {Test error}=0.0234375\n",
            "2023-02-12 14:08:03,461 [nnabla][INFO]: iter=8709 {Training loss}=1.2537387874544947e-06\n",
            "2023-02-12 14:08:03,462 [nnabla][INFO]: iter=8709 {Training error}=0.0\n",
            "2023-02-12 14:08:03,497 [nnabla][INFO]: iter=8719 {Training loss}=1.252155811926059e-06\n",
            "2023-02-12 14:08:03,498 [nnabla][INFO]: iter=8719 {Training error}=0.0\n",
            "2023-02-12 14:08:03,534 [nnabla][INFO]: iter=8729 {Training loss}=1.2123883834647131e-06\n",
            "2023-02-12 14:08:03,534 [nnabla][INFO]: iter=8729 {Training error}=0.0\n",
            "2023-02-12 14:08:03,575 [nnabla][INFO]: iter=8739 {Training loss}=1.223471485900518e-06\n",
            "2023-02-12 14:08:03,575 [nnabla][INFO]: iter=8739 {Training error}=0.0\n",
            "2023-02-12 14:08:03,611 [nnabla][INFO]: iter=8749 {Training loss}=1.21667278563109e-06\n",
            "2023-02-12 14:08:03,611 [nnabla][INFO]: iter=8749 {Training error}=0.0\n",
            "2023-02-12 14:08:03,649 [nnabla][INFO]: iter=8759 {Training loss}=1.2084771014997386e-06\n",
            "2023-02-12 14:08:03,649 [nnabla][INFO]: iter=8759 {Training error}=0.0\n",
            "2023-02-12 14:08:03,691 [nnabla][INFO]: iter=8769 {Training loss}=1.170293330687855e-06\n",
            "2023-02-12 14:08:03,691 [nnabla][INFO]: iter=8769 {Training error}=0.0\n",
            "2023-02-12 14:08:03,726 [nnabla][INFO]: iter=8779 {Training loss}=1.2081978866262943e-06\n",
            "2023-02-12 14:08:03,726 [nnabla][INFO]: iter=8779 {Training error}=0.0\n",
            "2023-02-12 14:08:03,763 [nnabla][INFO]: iter=8789 {Training loss}=1.1579070360312471e-06\n",
            "2023-02-12 14:08:03,763 [nnabla][INFO]: iter=8789 {Training error}=0.0\n",
            "2023-02-12 14:08:03,799 [nnabla][INFO]: iter=8799 {Training loss}=1.1950663747484214e-06\n",
            "2023-02-12 14:08:03,799 [nnabla][INFO]: iter=8799 {Training error}=0.0\n",
            "2023-02-12 14:08:03,799 [nnabla][INFO]: iter=8799 {Training time}=0.39754796028137207[sec/100iter] 40.275120973587036[sec]\n",
            "2023-02-12 14:08:03,820 [nnabla][INFO]: iter=8800 {Test error}=0.0234375\n",
            "2023-02-12 14:08:03,859 [nnabla][INFO]: iter=8809 {Training loss}=1.156696271209512e-06\n",
            "2023-02-12 14:08:03,859 [nnabla][INFO]: iter=8809 {Training error}=0.0\n",
            "2023-02-12 14:08:03,894 [nnabla][INFO]: iter=8819 {Training loss}=1.14058479994128e-06\n",
            "2023-02-12 14:08:03,894 [nnabla][INFO]: iter=8819 {Training error}=0.0\n",
            "2023-02-12 14:08:03,931 [nnabla][INFO]: iter=8829 {Training loss}=1.166381821349205e-06\n",
            "2023-02-12 14:08:03,931 [nnabla][INFO]: iter=8829 {Training error}=0.0\n",
            "2023-02-12 14:08:03,966 [nnabla][INFO]: iter=8839 {Training loss}=1.1335066574247321e-06\n",
            "2023-02-12 14:08:03,966 [nnabla][INFO]: iter=8839 {Training error}=0.0\n",
            "2023-02-12 14:08:04,001 [nnabla][INFO]: iter=8849 {Training loss}=1.156510052169324e-06\n",
            "2023-02-12 14:08:04,002 [nnabla][INFO]: iter=8849 {Training error}=0.0\n",
            "2023-02-12 14:08:04,037 [nnabla][INFO]: iter=8859 {Training loss}=1.1107825912404223e-06\n",
            "2023-02-12 14:08:04,037 [nnabla][INFO]: iter=8859 {Training error}=0.0\n",
            "2023-02-12 14:08:04,075 [nnabla][INFO]: iter=8869 {Training loss}=1.13099190457433e-06\n",
            "2023-02-12 14:08:04,075 [nnabla][INFO]: iter=8869 {Training error}=0.0\n",
            "2023-02-12 14:08:04,111 [nnabla][INFO]: iter=8879 {Training loss}=1.091131821340241e-06\n",
            "2023-02-12 14:08:04,111 [nnabla][INFO]: iter=8879 {Training error}=0.0\n",
            "2023-02-12 14:08:04,151 [nnabla][INFO]: iter=8889 {Training loss}=1.1198164884262951e-06\n",
            "2023-02-12 14:08:04,151 [nnabla][INFO]: iter=8889 {Training error}=0.0\n",
            "2023-02-12 14:08:04,186 [nnabla][INFO]: iter=8899 {Training loss}=1.1174880683029187e-06\n",
            "2023-02-12 14:08:04,187 [nnabla][INFO]: iter=8899 {Training error}=0.0\n",
            "2023-02-12 14:08:04,187 [nnabla][INFO]: iter=8899 {Training time}=0.38787841796875[sec/100iter] 40.662999391555786[sec]\n",
            "2023-02-12 14:08:04,209 [nnabla][INFO]: iter=8900 {Test error}=0.02265625\n",
            "2023-02-12 14:08:04,246 [nnabla][INFO]: iter=8909 {Training loss}=1.1028663493561908e-06\n",
            "2023-02-12 14:08:04,246 [nnabla][INFO]: iter=8909 {Training error}=0.0\n",
            "2023-02-12 14:08:04,287 [nnabla][INFO]: iter=8919 {Training loss}=1.0933672456303611e-06\n",
            "2023-02-12 14:08:04,288 [nnabla][INFO]: iter=8919 {Training error}=0.0\n",
            "2023-02-12 14:08:04,322 [nnabla][INFO]: iter=8929 {Training loss}=1.0872205393752665e-06\n",
            "2023-02-12 14:08:04,323 [nnabla][INFO]: iter=8929 {Training error}=0.0\n",
            "2023-02-12 14:08:04,358 [nnabla][INFO]: iter=8939 {Training loss}=1.0613297263262211e-06\n",
            "2023-02-12 14:08:04,358 [nnabla][INFO]: iter=8939 {Training error}=0.0\n",
            "2023-02-12 14:08:04,393 [nnabla][INFO]: iter=8949 {Training loss}=1.0596535275908536e-06\n",
            "2023-02-12 14:08:04,393 [nnabla][INFO]: iter=8949 {Training error}=0.0\n",
            "2023-02-12 14:08:04,430 [nnabla][INFO]: iter=8959 {Training loss}=1.0902936082857195e-06\n",
            "2023-02-12 14:08:04,431 [nnabla][INFO]: iter=8959 {Training error}=0.0\n",
            "2023-02-12 14:08:04,469 [nnabla][INFO]: iter=8969 {Training loss}=1.0330178383810562e-06\n",
            "2023-02-12 14:08:04,469 [nnabla][INFO]: iter=8969 {Training error}=0.0\n",
            "2023-02-12 14:08:04,505 [nnabla][INFO]: iter=8979 {Training loss}=1.0842402389243944e-06\n",
            "2023-02-12 14:08:04,505 [nnabla][INFO]: iter=8979 {Training error}=0.0\n",
            "2023-02-12 14:08:04,541 [nnabla][INFO]: iter=8989 {Training loss}=1.0161610362047213e-06\n",
            "2023-02-12 14:08:04,541 [nnabla][INFO]: iter=8989 {Training error}=0.0\n",
            "2023-02-12 14:08:04,582 [nnabla][INFO]: iter=8999 {Training loss}=1.0494089792700834e-06\n",
            "2023-02-12 14:08:04,583 [nnabla][INFO]: iter=8999 {Training error}=0.0\n",
            "2023-02-12 14:08:04,583 [nnabla][INFO]: iter=8999 {Training time}=0.39595603942871094[sec/100iter] 41.0589554309845[sec]\n",
            "2023-02-12 14:08:04,605 [nnabla][INFO]: iter=9000 {Test error}=0.0234375\n",
            "2023-02-12 14:08:04,622 [nnabla][INFO]: Solver state save (.h5): output/states_9000.h5\n",
            "2023-02-12 14:08:04,630 [nnabla][INFO]: Parameter save (.h5): output/params_9000.h5\n",
            "2023-02-12 14:08:04,630 [nnabla][INFO]: Checkpoint save (.json): output/checkpoint_9000.json\n",
            "2023-02-12 14:08:04,666 [nnabla][INFO]: iter=9009 {Training loss}=1.014391614262422e-06\n",
            "2023-02-12 14:08:04,666 [nnabla][INFO]: iter=9009 {Training error}=0.0\n",
            "2023-02-12 14:08:04,702 [nnabla][INFO]: iter=9019 {Training loss}=1.046056354425673e-06\n",
            "2023-02-12 14:08:04,702 [nnabla][INFO]: iter=9019 {Training error}=0.0\n",
            "2023-02-12 14:08:04,738 [nnabla][INFO]: iter=9029 {Training loss}=1.017930685520696e-06\n",
            "2023-02-12 14:08:04,738 [nnabla][INFO]: iter=9029 {Training error}=0.0\n",
            "2023-02-12 14:08:04,777 [nnabla][INFO]: iter=9039 {Training loss}=9.977210311262752e-07\n",
            "2023-02-12 14:08:04,777 [nnabla][INFO]: iter=9039 {Training error}=0.0\n",
            "2023-02-12 14:08:04,813 [nnabla][INFO]: iter=9049 {Training loss}=1.0317140777260647e-06\n",
            "2023-02-12 14:08:04,813 [nnabla][INFO]: iter=9049 {Training error}=0.0\n",
            "2023-02-12 14:08:04,849 [nnabla][INFO]: iter=9059 {Training loss}=9.925988706527278e-07\n",
            "2023-02-12 14:08:04,849 [nnabla][INFO]: iter=9059 {Training error}=0.0\n",
            "2023-02-12 14:08:04,883 [nnabla][INFO]: iter=9069 {Training loss}=1.0134601780009689e-06\n",
            "2023-02-12 14:08:04,883 [nnabla][INFO]: iter=9069 {Training error}=0.0\n",
            "2023-02-12 14:08:04,920 [nnabla][INFO]: iter=9079 {Training loss}=9.997700090025319e-07\n",
            "2023-02-12 14:08:04,921 [nnabla][INFO]: iter=9079 {Training error}=0.0\n",
            "2023-02-12 14:08:04,956 [nnabla][INFO]: iter=9089 {Training loss}=9.686642670203582e-07\n",
            "2023-02-12 14:08:04,956 [nnabla][INFO]: iter=9089 {Training error}=0.0\n",
            "2023-02-12 14:08:04,992 [nnabla][INFO]: iter=9099 {Training loss}=9.734137620398542e-07\n",
            "2023-02-12 14:08:04,993 [nnabla][INFO]: iter=9099 {Training error}=0.0\n",
            "2023-02-12 14:08:04,993 [nnabla][INFO]: iter=9099 {Training time}=0.4100513458251953[sec/100iter] 41.46900677680969[sec]\n",
            "2023-02-12 14:08:05,014 [nnabla][INFO]: iter=9100 {Test error}=0.0234375\n",
            "2023-02-12 14:08:05,050 [nnabla][INFO]: iter=9109 {Training loss}=9.895254606817616e-07\n",
            "2023-02-12 14:08:05,050 [nnabla][INFO]: iter=9109 {Training error}=0.0\n",
            "2023-02-12 14:08:05,091 [nnabla][INFO]: iter=9119 {Training loss}=9.511554708296899e-07\n",
            "2023-02-12 14:08:05,091 [nnabla][INFO]: iter=9119 {Training error}=0.0\n",
            "2023-02-12 14:08:05,127 [nnabla][INFO]: iter=9129 {Training loss}=9.440774419999798e-07\n",
            "2023-02-12 14:08:05,127 [nnabla][INFO]: iter=9129 {Training error}=0.0\n",
            "2023-02-12 14:08:05,167 [nnabla][INFO]: iter=9139 {Training loss}=9.554394182487158e-07\n",
            "2023-02-12 14:08:05,167 [nnabla][INFO]: iter=9139 {Training error}=0.0\n",
            "2023-02-12 14:08:05,203 [nnabla][INFO]: iter=9149 {Training loss}=9.404453749084496e-07\n",
            "2023-02-12 14:08:05,204 [nnabla][INFO]: iter=9149 {Training error}=0.0\n",
            "2023-02-12 14:08:05,240 [nnabla][INFO]: iter=9159 {Training loss}=9.555326414556475e-07\n",
            "2023-02-12 14:08:05,240 [nnabla][INFO]: iter=9159 {Training error}=0.0\n",
            "2023-02-12 14:08:05,275 [nnabla][INFO]: iter=9169 {Training loss}=9.499447628513735e-07\n",
            "2023-02-12 14:08:05,275 [nnabla][INFO]: iter=9169 {Training error}=0.0\n",
            "2023-02-12 14:08:05,311 [nnabla][INFO]: iter=9179 {Training loss}=9.210738198817126e-07\n",
            "2023-02-12 14:08:05,312 [nnabla][INFO]: iter=9179 {Training error}=0.0\n",
            "2023-02-12 14:08:05,348 [nnabla][INFO]: iter=9189 {Training loss}=9.326224130745686e-07\n",
            "2023-02-12 14:08:05,348 [nnabla][INFO]: iter=9189 {Training error}=0.0\n",
            "2023-02-12 14:08:05,388 [nnabla][INFO]: iter=9199 {Training loss}=9.299213843405596e-07\n",
            "2023-02-12 14:08:05,388 [nnabla][INFO]: iter=9199 {Training error}=0.0\n",
            "2023-02-12 14:08:05,388 [nnabla][INFO]: iter=9199 {Training time}=0.39562034606933594[sec/100iter] 41.86462712287903[sec]\n",
            "2023-02-12 14:08:05,410 [nnabla][INFO]: iter=9200 {Test error}=0.0234375\n",
            "2023-02-12 14:08:05,451 [nnabla][INFO]: iter=9209 {Training loss}=9.140891847891908e-07\n",
            "2023-02-12 14:08:05,451 [nnabla][INFO]: iter=9209 {Training error}=0.0\n",
            "2023-02-12 14:08:05,490 [nnabla][INFO]: iter=9219 {Training loss}=9.140890142589342e-07\n",
            "2023-02-12 14:08:05,490 [nnabla][INFO]: iter=9219 {Training error}=0.0\n",
            "2023-02-12 14:08:05,530 [nnabla][INFO]: iter=9229 {Training loss}=8.978842060969328e-07\n",
            "2023-02-12 14:08:05,530 [nnabla][INFO]: iter=9229 {Training error}=0.0\n",
            "2023-02-12 14:08:05,582 [nnabla][INFO]: iter=9239 {Training loss}=9.092462960325065e-07\n",
            "2023-02-12 14:08:05,582 [nnabla][INFO]: iter=9239 {Training error}=0.0\n",
            "2023-02-12 14:08:05,624 [nnabla][INFO]: iter=9249 {Training loss}=8.860564548740513e-07\n",
            "2023-02-12 14:08:05,624 [nnabla][INFO]: iter=9249 {Training error}=0.0\n",
            "2023-02-12 14:08:05,672 [nnabla][INFO]: iter=9259 {Training loss}=8.923894938561716e-07\n",
            "2023-02-12 14:08:05,673 [nnabla][INFO]: iter=9259 {Training error}=0.0\n",
            "2023-02-12 14:08:05,712 [nnabla][INFO]: iter=9269 {Training loss}=8.819588401820511e-07\n",
            "2023-02-12 14:08:05,713 [nnabla][INFO]: iter=9269 {Training error}=0.0\n",
            "2023-02-12 14:08:05,758 [nnabla][INFO]: iter=9279 {Training loss}=8.739494887777255e-07\n",
            "2023-02-12 14:08:05,758 [nnabla][INFO]: iter=9279 {Training error}=0.0\n",
            "2023-02-12 14:08:05,805 [nnabla][INFO]: iter=9289 {Training loss}=8.711556915841356e-07\n",
            "2023-02-12 14:08:05,805 [nnabla][INFO]: iter=9289 {Training error}=0.0\n",
            "2023-02-12 14:08:05,846 [nnabla][INFO]: iter=9299 {Training loss}=8.705966365596396e-07\n",
            "2023-02-12 14:08:05,846 [nnabla][INFO]: iter=9299 {Training error}=0.0\n",
            "2023-02-12 14:08:05,846 [nnabla][INFO]: iter=9299 {Training time}=0.4580206871032715[sec/100iter] 42.3226478099823[sec]\n",
            "2023-02-12 14:08:05,879 [nnabla][INFO]: iter=9300 {Test error}=0.0234375\n",
            "2023-02-12 14:08:05,919 [nnabla][INFO]: iter=9309 {Training loss}=8.393045618504402e-07\n",
            "2023-02-12 14:08:05,919 [nnabla][INFO]: iter=9309 {Training error}=0.0\n",
            "2023-02-12 14:08:05,962 [nnabla][INFO]: iter=9319 {Training loss}=8.799099191492132e-07\n",
            "2023-02-12 14:08:05,963 [nnabla][INFO]: iter=9319 {Training error}=0.0\n",
            "2023-02-12 14:08:06,004 [nnabla][INFO]: iter=9329 {Training loss}=8.387457341996196e-07\n",
            "2023-02-12 14:08:06,004 [nnabla][INFO]: iter=9329 {Training error}=0.0\n",
            "2023-02-12 14:08:06,045 [nnabla][INFO]: iter=9339 {Training loss}=8.443336696473125e-07\n",
            "2023-02-12 14:08:06,045 [nnabla][INFO]: iter=9339 {Training error}=0.0\n",
            "2023-02-12 14:08:06,085 [nnabla][INFO]: iter=9349 {Training loss}=8.440541705567739e-07\n",
            "2023-02-12 14:08:06,085 [nnabla][INFO]: iter=9349 {Training error}=0.0\n",
            "2023-02-12 14:08:06,141 [nnabla][INFO]: iter=9359 {Training loss}=8.472206900478341e-07\n",
            "2023-02-12 14:08:06,141 [nnabla][INFO]: iter=9359 {Training error}=0.0\n",
            "2023-02-12 14:08:06,183 [nnabla][INFO]: iter=9369 {Training loss}=8.335305210493971e-07\n",
            "2023-02-12 14:08:06,183 [nnabla][INFO]: iter=9369 {Training error}=0.0\n",
            "2023-02-12 14:08:06,229 [nnabla][INFO]: iter=9379 {Training loss}=8.302708920382429e-07\n",
            "2023-02-12 14:08:06,229 [nnabla][INFO]: iter=9379 {Training error}=0.0\n",
            "2023-02-12 14:08:06,269 [nnabla][INFO]: iter=9389 {Training loss}=8.227270882343873e-07\n",
            "2023-02-12 14:08:06,270 [nnabla][INFO]: iter=9389 {Training error}=0.0\n",
            "2023-02-12 14:08:06,309 [nnabla][INFO]: iter=9399 {Training loss}=8.179775932148914e-07\n",
            "2023-02-12 14:08:06,310 [nnabla][INFO]: iter=9399 {Training error}=0.0\n",
            "2023-02-12 14:08:06,310 [nnabla][INFO]: iter=9399 {Training time}=0.4633595943450928[sec/100iter] 42.78600740432739[sec]\n",
            "2023-02-12 14:08:06,335 [nnabla][INFO]: iter=9400 {Test error}=0.02265625\n",
            "2023-02-12 14:08:06,375 [nnabla][INFO]: iter=9409 {Training loss}=8.148108463501558e-07\n",
            "2023-02-12 14:08:06,375 [nnabla][INFO]: iter=9409 {Training error}=0.0\n",
            "2023-02-12 14:08:06,422 [nnabla][INFO]: iter=9419 {Training loss}=7.924594456198975e-07\n",
            "2023-02-12 14:08:06,422 [nnabla][INFO]: iter=9419 {Training error}=0.0\n",
            "2023-02-12 14:08:06,463 [nnabla][INFO]: iter=9429 {Training loss}=8.122964345602668e-07\n",
            "2023-02-12 14:08:06,463 [nnabla][INFO]: iter=9429 {Training error}=0.0\n",
            "2023-02-12 14:08:06,518 [nnabla][INFO]: iter=9439 {Training loss}=7.954396323839319e-07\n",
            "2023-02-12 14:08:06,518 [nnabla][INFO]: iter=9439 {Training error}=0.0\n",
            "2023-02-12 14:08:06,559 [nnabla][INFO]: iter=9449 {Training loss}=7.773720653858618e-07\n",
            "2023-02-12 14:08:06,559 [nnabla][INFO]: iter=9449 {Training error}=0.0\n",
            "2023-02-12 14:08:06,606 [nnabla][INFO]: iter=9459 {Training loss}=7.982334864209406e-07\n",
            "2023-02-12 14:08:06,607 [nnabla][INFO]: iter=9459 {Training error}=0.0\n",
            "2023-02-12 14:08:06,649 [nnabla][INFO]: iter=9469 {Training loss}=7.954395186970942e-07\n",
            "2023-02-12 14:08:06,649 [nnabla][INFO]: iter=9469 {Training error}=0.0\n",
            "2023-02-12 14:08:06,689 [nnabla][INFO]: iter=9479 {Training loss}=7.714117487012118e-07\n",
            "2023-02-12 14:08:06,689 [nnabla][INFO]: iter=9479 {Training error}=0.0\n",
            "2023-02-12 14:08:06,732 [nnabla][INFO]: iter=9489 {Training loss}=7.817492360118194e-07\n",
            "2023-02-12 14:08:06,733 [nnabla][INFO]: iter=9489 {Training error}=0.0\n",
            "2023-02-12 14:08:06,772 [nnabla][INFO]: iter=9499 {Training loss}=7.485011792596197e-07\n",
            "2023-02-12 14:08:06,773 [nnabla][INFO]: iter=9499 {Training error}=0.0\n",
            "2023-02-12 14:08:06,773 [nnabla][INFO]: iter=9499 {Training time}=0.46297216415405273[sec/100iter] 43.248979568481445[sec]\n",
            "2023-02-12 14:08:06,798 [nnabla][INFO]: iter=9500 {Test error}=0.0234375\n",
            "2023-02-12 14:08:06,844 [nnabla][INFO]: iter=9509 {Training loss}=7.559518735433812e-07\n",
            "2023-02-12 14:08:06,844 [nnabla][INFO]: iter=9509 {Training error}=0.0\n",
            "2023-02-12 14:08:06,887 [nnabla][INFO]: iter=9519 {Training loss}=7.518539746342867e-07\n",
            "2023-02-12 14:08:06,887 [nnabla][INFO]: iter=9519 {Training error}=0.0\n",
            "2023-02-12 14:08:06,930 [nnabla][INFO]: iter=9529 {Training loss}=7.621916893185698e-07\n",
            "2023-02-12 14:08:06,930 [nnabla][INFO]: iter=9529 {Training error}=0.0\n",
            "2023-02-12 14:08:06,972 [nnabla][INFO]: iter=9539 {Training loss}=7.615396953042364e-07\n",
            "2023-02-12 14:08:06,972 [nnabla][INFO]: iter=9539 {Training error}=0.0\n",
            "2023-02-12 14:08:07,012 [nnabla][INFO]: iter=9549 {Training loss}=7.265222734531562e-07\n",
            "2023-02-12 14:08:07,012 [nnabla][INFO]: iter=9549 {Training error}=0.0\n",
            "2023-02-12 14:08:07,054 [nnabla][INFO]: iter=9559 {Training loss}=7.655443141629803e-07\n",
            "2023-02-12 14:08:07,054 [nnabla][INFO]: iter=9559 {Training error}=0.0\n",
            "2023-02-12 14:08:07,095 [nnabla][INFO]: iter=9569 {Training loss}=7.183266461652238e-07\n",
            "2023-02-12 14:08:07,096 [nnabla][INFO]: iter=9569 {Training error}=0.0\n",
            "2023-02-12 14:08:07,139 [nnabla][INFO]: iter=9579 {Training loss}=7.607015959365526e-07\n",
            "2023-02-12 14:08:07,139 [nnabla][INFO]: iter=9579 {Training error}=0.0\n",
            "2023-02-12 14:08:07,180 [nnabla][INFO]: iter=9589 {Training loss}=7.093859721862827e-07\n",
            "2023-02-12 14:08:07,180 [nnabla][INFO]: iter=9589 {Training error}=0.0\n",
            "2023-02-12 14:08:07,225 [nnabla][INFO]: iter=9599 {Training loss}=7.230763685583952e-07\n",
            "2023-02-12 14:08:07,226 [nnabla][INFO]: iter=9599 {Training error}=0.0\n",
            "2023-02-12 14:08:07,226 [nnabla][INFO]: iter=9599 {Training time}=0.4529268741607666[sec/100iter] 43.70190644264221[sec]\n",
            "2023-02-12 14:08:07,251 [nnabla][INFO]: iter=9600 {Test error}=0.0234375\n",
            "2023-02-12 14:08:07,292 [nnabla][INFO]: iter=9609 {Training loss}=7.193510782599333e-07\n",
            "2023-02-12 14:08:07,293 [nnabla][INFO]: iter=9609 {Training error}=0.0\n",
            "2023-02-12 14:08:07,334 [nnabla][INFO]: iter=9619 {Training loss}=7.152532361942576e-07\n",
            "2023-02-12 14:08:07,334 [nnabla][INFO]: iter=9619 {Training error}=0.0\n",
            "2023-02-12 14:08:07,377 [nnabla][INFO]: iter=9629 {Training loss}=7.22051822776848e-07\n",
            "2023-02-12 14:08:07,377 [nnabla][INFO]: iter=9629 {Training error}=0.0\n",
            "2023-02-12 14:08:07,419 [nnabla][INFO]: iter=9639 {Training loss}=7.006316877777863e-07\n",
            "2023-02-12 14:08:07,419 [nnabla][INFO]: iter=9639 {Training error}=0.0\n",
            "2023-02-12 14:08:07,460 [nnabla][INFO]: iter=9649 {Training loss}=6.844267090855283e-07\n",
            "2023-02-12 14:08:07,461 [nnabla][INFO]: iter=9649 {Training error}=0.0\n",
            "2023-02-12 14:08:07,510 [nnabla][INFO]: iter=9659 {Training loss}=7.199099627541727e-07\n",
            "2023-02-12 14:08:07,510 [nnabla][INFO]: iter=9659 {Training error}=0.0\n",
            "2023-02-12 14:08:07,559 [nnabla][INFO]: iter=9669 {Training loss}=6.895489832459134e-07\n",
            "2023-02-12 14:08:07,559 [nnabla][INFO]: iter=9669 {Training error}=0.0\n",
            "2023-02-12 14:08:07,601 [nnabla][INFO]: iter=9679 {Training loss}=6.766968567717413e-07\n",
            "2023-02-12 14:08:07,601 [nnabla][INFO]: iter=9679 {Training error}=0.0\n",
            "2023-02-12 14:08:07,644 [nnabla][INFO]: iter=9689 {Training loss}=6.983033244978287e-07\n",
            "2023-02-12 14:08:07,644 [nnabla][INFO]: iter=9689 {Training error}=0.0\n",
            "2023-02-12 14:08:07,691 [nnabla][INFO]: iter=9699 {Training loss}=6.857305265839386e-07\n",
            "2023-02-12 14:08:07,691 [nnabla][INFO]: iter=9699 {Training error}=0.0\n",
            "2023-02-12 14:08:07,691 [nnabla][INFO]: iter=9699 {Training time}=0.4655416011810303[sec/100iter] 44.16744804382324[sec]\n",
            "2023-02-12 14:08:07,717 [nnabla][INFO]: iter=9700 {Test error}=0.0234375\n",
            "2023-02-12 14:08:07,760 [nnabla][INFO]: iter=9709 {Training loss}=6.687806717309286e-07\n",
            "2023-02-12 14:08:07,761 [nnabla][INFO]: iter=9709 {Training error}=0.0\n",
            "2023-02-12 14:08:07,808 [nnabla][INFO]: iter=9719 {Training loss}=6.910391334713495e-07\n",
            "2023-02-12 14:08:07,808 [nnabla][INFO]: iter=9719 {Training error}=0.0\n",
            "2023-02-12 14:08:07,848 [nnabla][INFO]: iter=9729 {Training loss}=6.488504595836275e-07\n",
            "2023-02-12 14:08:07,849 [nnabla][INFO]: iter=9729 {Training error}=0.0\n",
            "2023-02-12 14:08:07,892 [nnabla][INFO]: iter=9739 {Training loss}=6.598398840651498e-07\n",
            "2023-02-12 14:08:07,892 [nnabla][INFO]: iter=9739 {Training error}=0.0\n",
            "2023-02-12 14:08:07,938 [nnabla][INFO]: iter=9749 {Training loss}=6.793044349251431e-07\n",
            "2023-02-12 14:08:07,939 [nnabla][INFO]: iter=9749 {Training error}=0.0\n",
            "2023-02-12 14:08:07,984 [nnabla][INFO]: iter=9759 {Training loss}=6.602125495192013e-07\n",
            "2023-02-12 14:08:07,984 [nnabla][INFO]: iter=9759 {Training error}=0.0\n",
            "2023-02-12 14:08:08,028 [nnabla][INFO]: iter=9769 {Training loss}=6.590948942175601e-07\n",
            "2023-02-12 14:08:08,028 [nnabla][INFO]: iter=9769 {Training error}=0.0\n",
            "2023-02-12 14:08:08,073 [nnabla][INFO]: iter=9779 {Training loss}=6.475466420852172e-07\n",
            "2023-02-12 14:08:08,074 [nnabla][INFO]: iter=9779 {Training error}=0.0\n",
            "2023-02-12 14:08:08,120 [nnabla][INFO]: iter=9789 {Training loss}=6.355326149787288e-07\n",
            "2023-02-12 14:08:08,120 [nnabla][INFO]: iter=9789 {Training error}=0.0\n",
            "2023-02-12 14:08:08,163 [nnabla][INFO]: iter=9799 {Training loss}=6.574185817953548e-07\n",
            "2023-02-12 14:08:08,163 [nnabla][INFO]: iter=9799 {Training error}=0.0\n",
            "2023-02-12 14:08:08,163 [nnabla][INFO]: iter=9799 {Training time}=0.4720451831817627[sec/100iter] 44.639493227005005[sec]\n",
            "2023-02-12 14:08:08,190 [nnabla][INFO]: iter=9800 {Test error}=0.02265625\n",
            "2023-02-12 14:08:08,234 [nnabla][INFO]: iter=9809 {Training loss}=6.535070156132861e-07\n",
            "2023-02-12 14:08:08,235 [nnabla][INFO]: iter=9809 {Training error}=0.0\n",
            "2023-02-12 14:08:08,277 [nnabla][INFO]: iter=9819 {Training loss}=6.220285513336421e-07\n",
            "2023-02-12 14:08:08,278 [nnabla][INFO]: iter=9819 {Training error}=0.0\n",
            "2023-02-12 14:08:08,322 [nnabla][INFO]: iter=9829 {Training loss}=6.210040623955138e-07\n",
            "2023-02-12 14:08:08,322 [nnabla][INFO]: iter=9829 {Training error}=0.0\n",
            "2023-02-12 14:08:08,367 [nnabla][INFO]: iter=9839 {Training loss}=6.380471972988744e-07\n",
            "2023-02-12 14:08:08,367 [nnabla][INFO]: iter=9839 {Training error}=0.0\n",
            "2023-02-12 14:08:08,414 [nnabla][INFO]: iter=9849 {Training loss}=6.294791319305659e-07\n",
            "2023-02-12 14:08:08,414 [nnabla][INFO]: iter=9849 {Training error}=0.0\n",
            "2023-02-12 14:08:08,457 [nnabla][INFO]: iter=9859 {Training loss}=6.144848043732054e-07\n",
            "2023-02-12 14:08:08,458 [nnabla][INFO]: iter=9859 {Training error}=0.0\n",
            "2023-02-12 14:08:08,503 [nnabla][INFO]: iter=9869 {Training loss}=6.229598170648387e-07\n",
            "2023-02-12 14:08:08,503 [nnabla][INFO]: iter=9869 {Training error}=0.0\n",
            "2023-02-12 14:08:08,551 [nnabla][INFO]: iter=9879 {Training loss}=6.245430768103688e-07\n",
            "2023-02-12 14:08:08,551 [nnabla][INFO]: iter=9879 {Training error}=0.0\n",
            "2023-02-12 14:08:08,593 [nnabla][INFO]: iter=9889 {Training loss}=6.03774765295384e-07\n",
            "2023-02-12 14:08:08,594 [nnabla][INFO]: iter=9889 {Training error}=0.0\n",
            "2023-02-12 14:08:08,643 [nnabla][INFO]: iter=9899 {Training loss}=6.030296617609565e-07\n",
            "2023-02-12 14:08:08,643 [nnabla][INFO]: iter=9899 {Training error}=0.0\n",
            "2023-02-12 14:08:08,644 [nnabla][INFO]: iter=9899 {Training time}=0.48021697998046875[sec/100iter] 45.119710206985474[sec]\n",
            "2023-02-12 14:08:08,669 [nnabla][INFO]: iter=9900 {Test error}=0.0234375\n",
            "2023-02-12 14:08:08,714 [nnabla][INFO]: iter=9909 {Training loss}=6.03960984335572e-07\n",
            "2023-02-12 14:08:08,714 [nnabla][INFO]: iter=9909 {Training error}=0.0\n",
            "2023-02-12 14:08:08,757 [nnabla][INFO]: iter=9919 {Training loss}=6.224942126209498e-07\n",
            "2023-02-12 14:08:08,757 [nnabla][INFO]: iter=9919 {Training error}=0.0\n",
            "2023-02-12 14:08:08,798 [nnabla][INFO]: iter=9929 {Training loss}=5.784428367405781e-07\n",
            "2023-02-12 14:08:08,798 [nnabla][INFO]: iter=9929 {Training error}=0.0\n",
            "2023-02-12 14:08:08,842 [nnabla][INFO]: iter=9939 {Training loss}=5.982798825243663e-07\n",
            "2023-02-12 14:08:08,843 [nnabla][INFO]: iter=9939 {Training error}=0.0\n",
            "2023-02-12 14:08:08,885 [nnabla][INFO]: iter=9949 {Training loss}=5.847758757226984e-07\n",
            "2023-02-12 14:08:08,885 [nnabla][INFO]: iter=9949 {Training error}=0.0\n",
            "2023-02-12 14:08:08,934 [nnabla][INFO]: iter=9959 {Training loss}=5.80119262849621e-07\n",
            "2023-02-12 14:08:08,935 [nnabla][INFO]: iter=9959 {Training error}=0.0\n",
            "2023-02-12 14:08:08,977 [nnabla][INFO]: iter=9969 {Training loss}=5.947408681095112e-07\n",
            "2023-02-12 14:08:08,977 [nnabla][INFO]: iter=9969 {Training error}=0.0\n",
            "2023-02-12 14:08:09,021 [nnabla][INFO]: iter=9979 {Training loss}=5.790947739114927e-07\n",
            "2023-02-12 14:08:09,021 [nnabla][INFO]: iter=9979 {Training error}=0.0\n",
            "2023-02-12 14:08:09,066 [nnabla][INFO]: iter=9989 {Training loss}=5.774184614892874e-07\n",
            "2023-02-12 14:08:09,066 [nnabla][INFO]: iter=9989 {Training error}=0.0\n",
            "2023-02-12 14:08:09,112 [nnabla][INFO]: iter=9999 {Training loss}=5.726686822526972e-07\n",
            "2023-02-12 14:08:09,112 [nnabla][INFO]: iter=9999 {Training error}=0.0\n",
            "2023-02-12 14:08:09,113 [nnabla][INFO]: iter=9999 {Training time}=0.4691047668457031[sec/100iter] 45.58881497383118[sec]\n",
            "2023-02-12 14:08:09,139 [nnabla][INFO]: iter=9999 {Test error}=0.0234375\n",
            "2023-02-12 14:08:09,151 [nnabla][INFO]: Parameter save (.h5): output/lenet_params_010000.h5\n",
            "2023-02-12 14:08:09,156 [nnabla][INFO]: Saving output/lenet_result.nnp as nnp\n",
            "2023-02-12 14:08:09,156 [nnabla][INFO]: Saving <_io.StringIO object at 0x7f369191d3a0> as prototxt\n",
            "2023-02-12 14:08:09,168 [nnabla][INFO]: Parameter save (.h5): <_io.BytesIO object at 0x7f36918cd720>\n",
            "2023-02-12 14:08:09,169 [nnabla][INFO]: Model file is saved as (.nnp): output/lenet_result.nnp\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "outputフォルダが作られていることを確認。"
      ],
      "metadata": {
        "id": "oaaEWUuI2mq7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!ls"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x6yXUm1t2rVN",
        "outputId": "e881be16-c163-485f-fd9a-e24e73a89281"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "args.py\t\t\t  converted_datasets  README.md\t\ttrain.csv\n",
            "_checkpoint_nnp_util.py   dcgan.py\t      requirements.txt\tvae.py\n",
            "classification_bnn.py\t  mnist_data.py       siamese.py\tvat.py\n",
            "classification_mydata.py  output\t      test.csv\n",
            "classification.py\t  __pycache__\t      tmp.monitor\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Cソースコードの出力先フォルダを作成。"
      ],
      "metadata": {
        "id": "iUUn93sd2suQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!mkdir ./output_csrc"
      ],
      "metadata": {
        "id": "93786eh_McKj"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "学習済みモデルファイル(.nnp)をCソースコードに変換。"
      ],
      "metadata": {
        "id": "8IlLx0RZ210H"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!nnabla_cli convert -O CSRC -b 1 ./output/lenet_result.nnp ./output_csrc"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-THulbzMMwhk",
        "outputId": "3a796fcb-19df-4a59-863c-db3a43f95db9"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2023-02-12 14:08:28,118 [nnabla][INFO]: Initializing CPU extension...\n",
            "NNabla command line interface (Version:1.33.1, Build:230206062328)\n",
            "2023-02-12 14:08:29,198 [nnabla][WARNING]: The export file format is 'CSRC' or 'SAVED_MODEL' that argument '--export-format' will have to be set!!!\n",
            "Importing ./output/lenet_result.nnp\n",
            " Expanding Validation.\n",
            "Using network [Validation].\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "変換の結果、次の４つのファイルが出来ていることを確認（その他のファイルは使わない）。\n",
        "Validation_inference.c\n",
        "Validation_inference.h\n",
        "Validation_parameters.c\n",
        "Validation_parameters.h"
      ],
      "metadata": {
        "id": "cJXpB1Ip3JqI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!ls -all output_csrc"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-DlV0KqDM1Y3",
        "outputId": "64eb864c-74be-4380-d131-dda1b90dca6a"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "total 568\n",
            "drwxr-xr-x 2 root root   4096 Feb 12 14:08 .\n",
            "drwxr-xr-x 7 root root   4096 Feb 12 14:08 ..\n",
            "-rw-r--r-- 1 root root    902 Feb 12 14:08 GNUmakefile\n",
            "-rw-r--r-- 1 root root   2124 Feb 12 14:08 Validation_example.c\n",
            "-rw-r--r-- 1 root root  23761 Feb 12 14:08 Validation_inference.c\n",
            "-rw-r--r-- 1 root root   2417 Feb 12 14:08 Validation_inference.h\n",
            "-rw-r--r-- 1 root root 529580 Feb 12 14:08 Validation_parameters.c\n",
            "-rw-r--r-- 1 root root    983 Feb 12 14:08 Validation_parameters.h\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "変換後のファイルをPCにダウンロードするために、Google Driveに一旦コピーする。"
      ],
      "metadata": {
        "id": "dFab70nk3Ovz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qgY1Ml1SHEp8",
        "outputId": "79160907-8d4c-4653-9512-9774eb847272"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!cp -r output_csrc /content/drive/MyDrive/"
      ],
      "metadata": {
        "id": "XDGf0RyDM-Fo"
      },
      "execution_count": 19,
      "outputs": []
    }
  ]
}