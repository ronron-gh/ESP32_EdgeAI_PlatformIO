{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPeGQ4XJAY7jy1bms12lwQ3",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ronron-gh/ESP32_EdgeAI/blob/main/nnabla_convert_mnist_model_to_csrc.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "nnablaをインストールする。"
      ],
      "metadata": {
        "id": "c9qrvCSz6jRv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install nnabla-ext-cuda114"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fW7hhP2HMdSp",
        "outputId": "2f5d2ea9-b6c4-46c7-a5d7-4323aa034aa3"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: nnabla-ext-cuda114 in /usr/local/lib/python3.8/dist-packages (1.33.0)\n",
            "Requirement already satisfied: nnabla==1.33.0 in /usr/local/lib/python3.8/dist-packages (from nnabla-ext-cuda114) (1.33.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.8/dist-packages (from nnabla-ext-cuda114) (57.4.0)\n",
            "Requirement already satisfied: ply in /usr/local/lib/python3.8/dist-packages (from nnabla==1.33.0->nnabla-ext-cuda114) (3.11)\n",
            "Requirement already satisfied: configparser in /usr/local/lib/python3.8/dist-packages (from nnabla==1.33.0->nnabla-ext-cuda114) (5.3.0)\n",
            "Requirement already satisfied: boto3 in /usr/local/lib/python3.8/dist-packages (from nnabla==1.33.0->nnabla-ext-cuda114) (1.26.64)\n",
            "Requirement already satisfied: imageio in /usr/local/lib/python3.8/dist-packages (from nnabla==1.33.0->nnabla-ext-cuda114) (2.9.0)\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.8/dist-packages (from nnabla==1.33.0->nnabla-ext-cuda114) (7.1.2)\n",
            "Requirement already satisfied: numpy~=1.23.0 in /usr/local/lib/python3.8/dist-packages (from nnabla==1.33.0->nnabla-ext-cuda114) (1.23.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.8/dist-packages (from nnabla==1.33.0->nnabla-ext-cuda114) (1.15.0)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.8/dist-packages (from nnabla==1.33.0->nnabla-ext-cuda114) (3.1.0)\n",
            "Requirement already satisfied: Cython in /usr/local/lib/python3.8/dist-packages (from nnabla==1.33.0->nnabla-ext-cuda114) (0.29.33)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.8/dist-packages (from nnabla==1.33.0->nnabla-ext-cuda114) (4.64.1)\n",
            "Requirement already satisfied: protobuf<=3.19.4 in /usr/local/lib/python3.8/dist-packages (from nnabla==1.33.0->nnabla-ext-cuda114) (3.19.4)\n",
            "Requirement already satisfied: contextlib2 in /usr/local/lib/python3.8/dist-packages (from nnabla==1.33.0->nnabla-ext-cuda114) (0.5.5)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.8/dist-packages (from nnabla==1.33.0->nnabla-ext-cuda114) (1.10.0)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.8/dist-packages (from nnabla==1.33.0->nnabla-ext-cuda114) (6.0)\n",
            "Requirement already satisfied: jmespath<2.0.0,>=0.7.1 in /usr/local/lib/python3.8/dist-packages (from boto3->nnabla==1.33.0->nnabla-ext-cuda114) (1.0.1)\n",
            "Requirement already satisfied: botocore<1.30.0,>=1.29.64 in /usr/local/lib/python3.8/dist-packages (from boto3->nnabla==1.33.0->nnabla-ext-cuda114) (1.29.64)\n",
            "Requirement already satisfied: s3transfer<0.7.0,>=0.6.0 in /usr/local/lib/python3.8/dist-packages (from boto3->nnabla==1.33.0->nnabla-ext-cuda114) (0.6.0)\n",
            "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /usr/local/lib/python3.8/dist-packages (from botocore<1.30.0,>=1.29.64->boto3->nnabla==1.33.0->nnabla-ext-cuda114) (2.8.2)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.25.4 in /usr/local/lib/python3.8/dist-packages (from botocore<1.30.0,>=1.29.64->boto3->nnabla==1.33.0->nnabla-ext-cuda114) (1.26.14)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "numpyを1.23.0に下げて、pillowを9.1.0に上げる（そうしないと、この後のclassification.pyの実行でエラーになった。数カ月前まではnumpyの変更だけで動いていたので、今後もこのようなバージョンの不整合は発生するかもしれない）。"
      ],
      "metadata": {
        "id": "dKQcl3sV6oHD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install numpy==1.23.0"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VwgVZb7bMgm7",
        "outputId": "c7854aea-1d02-4245-a47e-d5dc3b68b497"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: numpy==1.23.0 in /usr/local/lib/python3.8/dist-packages (1.23.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pillow==9.1.0"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 292
        },
        "id": "fkA6de2Dr8Mm",
        "outputId": "dd283e1c-c35b-4cd1-fda3-87be5d35962d"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting pillow==9.1.0\n",
            "  Downloading Pillow-9.1.0-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.3/4.3 MB\u001b[0m \u001b[31m75.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: pillow\n",
            "  Attempting uninstall: pillow\n",
            "    Found existing installation: Pillow 7.1.2\n",
            "    Uninstalling Pillow-7.1.2:\n",
            "      Successfully uninstalled Pillow-7.1.2\n",
            "Successfully installed pillow-9.1.0\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "PIL"
                ]
              }
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "nnablaのサンプル群をダウンロードする。"
      ],
      "metadata": {
        "id": "teQu2uGZ6E3K"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/sony/nnabla-examples.git"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sVYxgG67MUz8",
        "outputId": "8989ed4a-1c4b-48a8-df2a-07a2f93e1d69"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fatal: destination path 'nnabla-examples' already exists and is not an empty directory.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "mnistサンプルプログラムのフォルダに移動し、学習を実行。"
      ],
      "metadata": {
        "id": "xWi3JyoC9Fjn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%cd nnabla-examples/image-classification/mnist-collection/"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LuWPe4lBNAd6",
        "outputId": "ccba8709-6512-4425-88dd-488d4ad63a88"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/nnabla-examples/image-classification/mnist-collection\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python classification.py -c cudnn -n lenet -o output"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mnLvz8JfNuHG",
        "outputId": "a769584e-db93-4507-dd56-2323bcf0de19"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2023-02-03 23:50:40,784 [nnabla][INFO]: Initializing CPU extension...\n",
            "2023-02-03 23:50:42,060 [nnabla][INFO]: Running in cudnn\n",
            "2023-02-03 23:50:42,220 [nnabla][INFO]: Initializing CUDA extension...\n",
            "2023-02-03 23:50:42,335 [nnabla][INFO]: Initializing cuDNN extension...\n",
            "2023-02-03 23:50:49,631 [nnabla][INFO]: Saving output/lenet_result_epoch0.nnp as nnp\n",
            "2023-02-03 23:50:49,631 [nnabla][INFO]: Saving <_io.StringIO object at 0x7fc0478e3dc0> as prototxt\n",
            "2023-02-03 23:50:49,640 [nnabla][INFO]: Parameter save (.h5): <_io.BytesIO object at 0x7fc047895ef0>\n",
            "2023-02-03 23:50:49,641 [nnabla][INFO]: Model file is saved as (.nnp): output/lenet_result_epoch0.nnp\n",
            "2023-02-03 23:50:49,641 [nnabla][INFO]: DataSource with shuffle(True)\n",
            "2023-02-03 23:50:49,642 [nnabla][INFO]: Getting label data from http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz.\n",
            "train-labels-idx1-ubyte.gz: 100% 28.2k/28.2k [00:00<00:00, 38.4MB/s]\n",
            "2023-02-03 23:50:49,841 [nnabla][INFO]: Getting label data done.\n",
            "2023-02-03 23:50:49,841 [nnabla][INFO]: Getting image data from http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz.\n",
            "train-images-idx3-ubyte.gz: 100% 9.45M/9.45M [00:00<00:00, 226MB/s]\n",
            "2023-02-03 23:50:50,151 [nnabla][INFO]: Getting image data done.\n",
            "2023-02-03 23:50:50,153 [nnabla][INFO]: Using DataIterator\n",
            "2023-02-03 23:50:50,154 [nnabla][INFO]: DataSource with shuffle(True)\n",
            "2023-02-03 23:50:50,154 [nnabla][INFO]: Getting label data from http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz.\n",
            "t10k-labels-idx1-ubyte.gz: 100% 4.44k/4.44k [00:00<00:00, 16.2MB/s]\n",
            "2023-02-03 23:50:50,174 [nnabla][INFO]: Getting label data done.\n",
            "2023-02-03 23:50:50,174 [nnabla][INFO]: Getting image data from http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz.\n",
            "t10k-images-idx3-ubyte.gz: 100% 1.57M/1.57M [00:00<00:00, 66.1MB/s]\n",
            "2023-02-03 23:50:50,406 [nnabla][INFO]: Getting image data done.\n",
            "2023-02-03 23:50:50,407 [nnabla][INFO]: Using DataIterator\n",
            "2023-02-03 23:50:52,236 [nnabla][INFO]: Solver state save (.h5): output/states_0.h5\n",
            "2023-02-03 23:50:52,243 [nnabla][INFO]: Parameter save (.h5): output/params_0.h5\n",
            "2023-02-03 23:50:52,243 [nnabla][INFO]: Checkpoint save (.json): output/checkpoint_0.json\n",
            "2023-02-03 23:50:52,570 [nnabla][INFO]: iter=9 {Training loss}=2.2103335857391357\n",
            "2023-02-03 23:50:52,571 [nnabla][INFO]: iter=9 {Training error}=0.79453125\n",
            "2023-02-03 23:50:52,613 [nnabla][INFO]: iter=19 {Training loss}=1.8591934442520142\n",
            "2023-02-03 23:50:52,614 [nnabla][INFO]: iter=19 {Training error}=0.521875\n",
            "2023-02-03 23:50:52,656 [nnabla][INFO]: iter=29 {Training loss}=1.3404734134674072\n",
            "2023-02-03 23:50:52,657 [nnabla][INFO]: iter=29 {Training error}=0.35078125\n",
            "2023-02-03 23:50:52,699 [nnabla][INFO]: iter=39 {Training loss}=0.8571466207504272\n",
            "2023-02-03 23:50:52,699 [nnabla][INFO]: iter=39 {Training error}=0.2484375\n",
            "2023-02-03 23:50:52,734 [nnabla][INFO]: iter=49 {Training loss}=0.6059712171554565\n",
            "2023-02-03 23:50:52,734 [nnabla][INFO]: iter=49 {Training error}=0.18046875\n",
            "2023-02-03 23:50:52,768 [nnabla][INFO]: iter=59 {Training loss}=0.45011669397354126\n",
            "2023-02-03 23:50:52,768 [nnabla][INFO]: iter=59 {Training error}=0.140625\n",
            "2023-02-03 23:50:52,803 [nnabla][INFO]: iter=69 {Training loss}=0.4164794981479645\n",
            "2023-02-03 23:50:52,803 [nnabla][INFO]: iter=69 {Training error}=0.12109375\n",
            "2023-02-03 23:50:52,837 [nnabla][INFO]: iter=79 {Training loss}=0.38167357444763184\n",
            "2023-02-03 23:50:52,837 [nnabla][INFO]: iter=79 {Training error}=0.11328125\n",
            "2023-02-03 23:50:52,873 [nnabla][INFO]: iter=89 {Training loss}=0.30672886967658997\n",
            "2023-02-03 23:50:52,873 [nnabla][INFO]: iter=89 {Training error}=0.096875\n",
            "2023-02-03 23:50:52,906 [nnabla][INFO]: iter=99 {Training loss}=0.3035191297531128\n",
            "2023-02-03 23:50:52,907 [nnabla][INFO]: iter=99 {Training error}=0.08828125\n",
            "2023-02-03 23:50:52,907 [nnabla][INFO]: iter=99 {Training time}=3.2791800498962402[sec/100iter] 3.2791800498962402[sec]\n",
            "2023-02-03 23:50:52,918 [nnabla][INFO]: iter=100 {Test error}=0.499609375\n",
            "2023-02-03 23:50:52,945 [nnabla][INFO]: iter=109 {Training loss}=0.356850266456604\n",
            "2023-02-03 23:50:52,945 [nnabla][INFO]: iter=109 {Training error}=0.10859375\n",
            "2023-02-03 23:50:52,973 [nnabla][INFO]: iter=119 {Training loss}=0.3294120132923126\n",
            "2023-02-03 23:50:52,973 [nnabla][INFO]: iter=119 {Training error}=0.0890625\n",
            "2023-02-03 23:50:53,000 [nnabla][INFO]: iter=129 {Training loss}=0.2886124551296234\n",
            "2023-02-03 23:50:53,000 [nnabla][INFO]: iter=129 {Training error}=0.0875\n",
            "2023-02-03 23:50:53,027 [nnabla][INFO]: iter=139 {Training loss}=0.2834710478782654\n",
            "2023-02-03 23:50:53,027 [nnabla][INFO]: iter=139 {Training error}=0.0875\n",
            "2023-02-03 23:50:53,057 [nnabla][INFO]: iter=149 {Training loss}=0.2908894419670105\n",
            "2023-02-03 23:50:53,057 [nnabla][INFO]: iter=149 {Training error}=0.0875\n",
            "2023-02-03 23:50:53,088 [nnabla][INFO]: iter=159 {Training loss}=0.221001535654068\n",
            "2023-02-03 23:50:53,089 [nnabla][INFO]: iter=159 {Training error}=0.06171875\n",
            "2023-02-03 23:50:53,115 [nnabla][INFO]: iter=169 {Training loss}=0.2263077050447464\n",
            "2023-02-03 23:50:53,116 [nnabla][INFO]: iter=169 {Training error}=0.065625\n",
            "2023-02-03 23:50:53,143 [nnabla][INFO]: iter=179 {Training loss}=0.1903238594532013\n",
            "2023-02-03 23:50:53,143 [nnabla][INFO]: iter=179 {Training error}=0.059375\n",
            "2023-02-03 23:50:53,170 [nnabla][INFO]: iter=189 {Training loss}=0.2100035846233368\n",
            "2023-02-03 23:50:53,170 [nnabla][INFO]: iter=189 {Training error}=0.06953125\n",
            "2023-02-03 23:50:53,199 [nnabla][INFO]: iter=199 {Training loss}=0.24220561981201172\n",
            "2023-02-03 23:50:53,199 [nnabla][INFO]: iter=199 {Training error}=0.06328125\n",
            "2023-02-03 23:50:53,199 [nnabla][INFO]: iter=199 {Training time}=0.29221534729003906[sec/100iter] 3.5713953971862793[sec]\n",
            "2023-02-03 23:50:53,210 [nnabla][INFO]: iter=200 {Test error}=0.04609375\n",
            "2023-02-03 23:50:53,237 [nnabla][INFO]: iter=209 {Training loss}=0.23690828680992126\n",
            "2023-02-03 23:50:53,237 [nnabla][INFO]: iter=209 {Training error}=0.06171875\n",
            "2023-02-03 23:50:53,264 [nnabla][INFO]: iter=219 {Training loss}=0.19883140921592712\n",
            "2023-02-03 23:50:53,264 [nnabla][INFO]: iter=219 {Training error}=0.06875\n",
            "2023-02-03 23:50:53,291 [nnabla][INFO]: iter=229 {Training loss}=0.22125062346458435\n",
            "2023-02-03 23:50:53,291 [nnabla][INFO]: iter=229 {Training error}=0.0625\n",
            "2023-02-03 23:50:53,318 [nnabla][INFO]: iter=239 {Training loss}=0.20737719535827637\n",
            "2023-02-03 23:50:53,319 [nnabla][INFO]: iter=239 {Training error}=0.06953125\n",
            "2023-02-03 23:50:53,346 [nnabla][INFO]: iter=249 {Training loss}=0.1714411824941635\n",
            "2023-02-03 23:50:53,346 [nnabla][INFO]: iter=249 {Training error}=0.04609375\n",
            "2023-02-03 23:50:53,374 [nnabla][INFO]: iter=259 {Training loss}=0.14104554057121277\n",
            "2023-02-03 23:50:53,374 [nnabla][INFO]: iter=259 {Training error}=0.03984375\n",
            "2023-02-03 23:50:53,405 [nnabla][INFO]: iter=269 {Training loss}=0.16585871577262878\n",
            "2023-02-03 23:50:53,405 [nnabla][INFO]: iter=269 {Training error}=0.04765625\n",
            "2023-02-03 23:50:53,433 [nnabla][INFO]: iter=279 {Training loss}=0.18362471461296082\n",
            "2023-02-03 23:50:53,433 [nnabla][INFO]: iter=279 {Training error}=0.053125\n",
            "2023-02-03 23:50:53,460 [nnabla][INFO]: iter=289 {Training loss}=0.1696622669696808\n",
            "2023-02-03 23:50:53,461 [nnabla][INFO]: iter=289 {Training error}=0.04609375\n",
            "2023-02-03 23:50:53,488 [nnabla][INFO]: iter=299 {Training loss}=0.1445866972208023\n",
            "2023-02-03 23:50:53,488 [nnabla][INFO]: iter=299 {Training error}=0.04609375\n",
            "2023-02-03 23:50:53,488 [nnabla][INFO]: iter=299 {Training time}=0.28902506828308105[sec/100iter] 3.8604204654693604[sec]\n",
            "2023-02-03 23:50:53,499 [nnabla][INFO]: iter=300 {Test error}=0.04375\n",
            "2023-02-03 23:50:53,526 [nnabla][INFO]: iter=309 {Training loss}=0.17539171874523163\n",
            "2023-02-03 23:50:53,526 [nnabla][INFO]: iter=309 {Training error}=0.05546875\n",
            "2023-02-03 23:50:53,553 [nnabla][INFO]: iter=319 {Training loss}=0.1331419050693512\n",
            "2023-02-03 23:50:53,553 [nnabla][INFO]: iter=319 {Training error}=0.04140625\n",
            "2023-02-03 23:50:53,580 [nnabla][INFO]: iter=329 {Training loss}=0.14711689949035645\n",
            "2023-02-03 23:50:53,580 [nnabla][INFO]: iter=329 {Training error}=0.04140625\n",
            "2023-02-03 23:50:53,608 [nnabla][INFO]: iter=339 {Training loss}=0.13480409979820251\n",
            "2023-02-03 23:50:53,608 [nnabla][INFO]: iter=339 {Training error}=0.0421875\n",
            "2023-02-03 23:50:53,635 [nnabla][INFO]: iter=349 {Training loss}=0.16064366698265076\n",
            "2023-02-03 23:50:53,635 [nnabla][INFO]: iter=349 {Training error}=0.0421875\n",
            "2023-02-03 23:50:53,662 [nnabla][INFO]: iter=359 {Training loss}=0.15024647116661072\n",
            "2023-02-03 23:50:53,662 [nnabla][INFO]: iter=359 {Training error}=0.03828125\n",
            "2023-02-03 23:50:53,689 [nnabla][INFO]: iter=369 {Training loss}=0.12782429158687592\n",
            "2023-02-03 23:50:53,689 [nnabla][INFO]: iter=369 {Training error}=0.04296875\n",
            "2023-02-03 23:50:53,716 [nnabla][INFO]: iter=379 {Training loss}=0.11851414293050766\n",
            "2023-02-03 23:50:53,716 [nnabla][INFO]: iter=379 {Training error}=0.0328125\n",
            "2023-02-03 23:50:53,743 [nnabla][INFO]: iter=389 {Training loss}=0.1156163215637207\n",
            "2023-02-03 23:50:53,743 [nnabla][INFO]: iter=389 {Training error}=0.0375\n",
            "2023-02-03 23:50:53,772 [nnabla][INFO]: iter=399 {Training loss}=0.13065798580646515\n",
            "2023-02-03 23:50:53,772 [nnabla][INFO]: iter=399 {Training error}=0.0359375\n",
            "2023-02-03 23:50:53,772 [nnabla][INFO]: iter=399 {Training time}=0.2843184471130371[sec/100iter] 4.1447389125823975[sec]\n",
            "2023-02-03 23:50:53,783 [nnabla][INFO]: iter=400 {Test error}=0.034375\n",
            "2023-02-03 23:50:53,810 [nnabla][INFO]: iter=409 {Training loss}=0.10722924768924713\n",
            "2023-02-03 23:50:53,810 [nnabla][INFO]: iter=409 {Training error}=0.0359375\n",
            "2023-02-03 23:50:53,837 [nnabla][INFO]: iter=419 {Training loss}=0.11478084325790405\n",
            "2023-02-03 23:50:53,837 [nnabla][INFO]: iter=419 {Training error}=0.028125\n",
            "2023-02-03 23:50:53,864 [nnabla][INFO]: iter=429 {Training loss}=0.09809904545545578\n",
            "2023-02-03 23:50:53,865 [nnabla][INFO]: iter=429 {Training error}=0.028125\n",
            "2023-02-03 23:50:53,893 [nnabla][INFO]: iter=439 {Training loss}=0.09603960812091827\n",
            "2023-02-03 23:50:53,893 [nnabla][INFO]: iter=439 {Training error}=0.028125\n",
            "2023-02-03 23:50:53,920 [nnabla][INFO]: iter=449 {Training loss}=0.11423300206661224\n",
            "2023-02-03 23:50:53,920 [nnabla][INFO]: iter=449 {Training error}=0.03515625\n",
            "2023-02-03 23:50:53,946 [nnabla][INFO]: iter=459 {Training loss}=0.13477256894111633\n",
            "2023-02-03 23:50:53,946 [nnabla][INFO]: iter=459 {Training error}=0.0359375\n",
            "2023-02-03 23:50:53,974 [nnabla][INFO]: iter=469 {Training loss}=0.16639982163906097\n",
            "2023-02-03 23:50:53,974 [nnabla][INFO]: iter=469 {Training error}=0.04609375\n",
            "2023-02-03 23:50:54,001 [nnabla][INFO]: iter=479 {Training loss}=0.10053767263889313\n",
            "2023-02-03 23:50:54,001 [nnabla][INFO]: iter=479 {Training error}=0.02890625\n",
            "2023-02-03 23:50:54,027 [nnabla][INFO]: iter=489 {Training loss}=0.08850807696580887\n",
            "2023-02-03 23:50:54,027 [nnabla][INFO]: iter=489 {Training error}=0.0265625\n",
            "2023-02-03 23:50:54,055 [nnabla][INFO]: iter=499 {Training loss}=0.10542807728052139\n",
            "2023-02-03 23:50:54,055 [nnabla][INFO]: iter=499 {Training error}=0.0296875\n",
            "2023-02-03 23:50:54,055 [nnabla][INFO]: iter=499 {Training time}=0.2829439640045166[sec/100iter] 4.427682876586914[sec]\n",
            "2023-02-03 23:50:54,067 [nnabla][INFO]: iter=500 {Test error}=0.0359375\n",
            "2023-02-03 23:50:54,098 [nnabla][INFO]: iter=509 {Training loss}=0.13119405508041382\n",
            "2023-02-03 23:50:54,098 [nnabla][INFO]: iter=509 {Training error}=0.0390625\n",
            "2023-02-03 23:50:54,127 [nnabla][INFO]: iter=519 {Training loss}=0.08369214087724686\n",
            "2023-02-03 23:50:54,127 [nnabla][INFO]: iter=519 {Training error}=0.021875\n",
            "2023-02-03 23:50:54,154 [nnabla][INFO]: iter=529 {Training loss}=0.11373547464609146\n",
            "2023-02-03 23:50:54,154 [nnabla][INFO]: iter=529 {Training error}=0.02890625\n",
            "2023-02-03 23:50:54,181 [nnabla][INFO]: iter=539 {Training loss}=0.1176661029458046\n",
            "2023-02-03 23:50:54,181 [nnabla][INFO]: iter=539 {Training error}=0.0375\n",
            "2023-02-03 23:50:54,209 [nnabla][INFO]: iter=549 {Training loss}=0.10964298248291016\n",
            "2023-02-03 23:50:54,209 [nnabla][INFO]: iter=549 {Training error}=0.03125\n",
            "2023-02-03 23:50:54,235 [nnabla][INFO]: iter=559 {Training loss}=0.08646361529827118\n",
            "2023-02-03 23:50:54,236 [nnabla][INFO]: iter=559 {Training error}=0.02578125\n",
            "2023-02-03 23:50:54,262 [nnabla][INFO]: iter=569 {Training loss}=0.07123099267482758\n",
            "2023-02-03 23:50:54,262 [nnabla][INFO]: iter=569 {Training error}=0.02265625\n",
            "2023-02-03 23:50:54,290 [nnabla][INFO]: iter=579 {Training loss}=0.10085295140743256\n",
            "2023-02-03 23:50:54,290 [nnabla][INFO]: iter=579 {Training error}=0.02890625\n",
            "2023-02-03 23:50:54,318 [nnabla][INFO]: iter=589 {Training loss}=0.11574001610279083\n",
            "2023-02-03 23:50:54,318 [nnabla][INFO]: iter=589 {Training error}=0.034375\n",
            "2023-02-03 23:50:54,345 [nnabla][INFO]: iter=599 {Training loss}=0.08841311186552048\n",
            "2023-02-03 23:50:54,345 [nnabla][INFO]: iter=599 {Training error}=0.0328125\n",
            "2023-02-03 23:50:54,345 [nnabla][INFO]: iter=599 {Training time}=0.29004764556884766[sec/100iter] 4.717730522155762[sec]\n",
            "2023-02-03 23:50:54,356 [nnabla][INFO]: iter=600 {Test error}=0.0234375\n",
            "2023-02-03 23:50:54,383 [nnabla][INFO]: iter=609 {Training loss}=0.11073571443557739\n",
            "2023-02-03 23:50:54,383 [nnabla][INFO]: iter=609 {Training error}=0.0375\n",
            "2023-02-03 23:50:54,412 [nnabla][INFO]: iter=619 {Training loss}=0.10070810467004776\n",
            "2023-02-03 23:50:54,412 [nnabla][INFO]: iter=619 {Training error}=0.0296875\n",
            "2023-02-03 23:50:54,441 [nnabla][INFO]: iter=629 {Training loss}=0.07733546197414398\n",
            "2023-02-03 23:50:54,441 [nnabla][INFO]: iter=629 {Training error}=0.01875\n",
            "2023-02-03 23:50:54,468 [nnabla][INFO]: iter=639 {Training loss}=0.11335086822509766\n",
            "2023-02-03 23:50:54,468 [nnabla][INFO]: iter=639 {Training error}=0.0375\n",
            "2023-02-03 23:50:54,496 [nnabla][INFO]: iter=649 {Training loss}=0.0883718729019165\n",
            "2023-02-03 23:50:54,496 [nnabla][INFO]: iter=649 {Training error}=0.03359375\n",
            "2023-02-03 23:50:54,523 [nnabla][INFO]: iter=659 {Training loss}=0.11523564159870148\n",
            "2023-02-03 23:50:54,523 [nnabla][INFO]: iter=659 {Training error}=0.034375\n",
            "2023-02-03 23:50:54,550 [nnabla][INFO]: iter=669 {Training loss}=0.11351357400417328\n",
            "2023-02-03 23:50:54,550 [nnabla][INFO]: iter=669 {Training error}=0.0265625\n",
            "2023-02-03 23:50:54,577 [nnabla][INFO]: iter=679 {Training loss}=0.08238622546195984\n",
            "2023-02-03 23:50:54,578 [nnabla][INFO]: iter=679 {Training error}=0.025\n",
            "2023-02-03 23:50:54,607 [nnabla][INFO]: iter=689 {Training loss}=0.10777987539768219\n",
            "2023-02-03 23:50:54,607 [nnabla][INFO]: iter=689 {Training error}=0.034375\n",
            "2023-02-03 23:50:54,634 [nnabla][INFO]: iter=699 {Training loss}=0.10097064077854156\n",
            "2023-02-03 23:50:54,634 [nnabla][INFO]: iter=699 {Training error}=0.0296875\n",
            "2023-02-03 23:50:54,634 [nnabla][INFO]: iter=699 {Training time}=0.2889697551727295[sec/100iter] 5.006700277328491[sec]\n",
            "2023-02-03 23:50:54,646 [nnabla][INFO]: iter=700 {Test error}=0.02421875\n",
            "2023-02-03 23:50:54,674 [nnabla][INFO]: iter=709 {Training loss}=0.08255861699581146\n",
            "2023-02-03 23:50:54,674 [nnabla][INFO]: iter=709 {Training error}=0.0328125\n",
            "2023-02-03 23:50:54,701 [nnabla][INFO]: iter=719 {Training loss}=0.10413289070129395\n",
            "2023-02-03 23:50:54,701 [nnabla][INFO]: iter=719 {Training error}=0.03125\n",
            "2023-02-03 23:50:54,728 [nnabla][INFO]: iter=729 {Training loss}=0.09531243145465851\n",
            "2023-02-03 23:50:54,728 [nnabla][INFO]: iter=729 {Training error}=0.02890625\n",
            "2023-02-03 23:50:54,755 [nnabla][INFO]: iter=739 {Training loss}=0.08363323658704758\n",
            "2023-02-03 23:50:54,756 [nnabla][INFO]: iter=739 {Training error}=0.02578125\n",
            "2023-02-03 23:50:54,784 [nnabla][INFO]: iter=749 {Training loss}=0.08112819492816925\n",
            "2023-02-03 23:50:54,785 [nnabla][INFO]: iter=749 {Training error}=0.02890625\n",
            "2023-02-03 23:50:54,812 [nnabla][INFO]: iter=759 {Training loss}=0.07488464564085007\n",
            "2023-02-03 23:50:54,812 [nnabla][INFO]: iter=759 {Training error}=0.01953125\n",
            "2023-02-03 23:50:54,839 [nnabla][INFO]: iter=769 {Training loss}=0.07735829055309296\n",
            "2023-02-03 23:50:54,839 [nnabla][INFO]: iter=769 {Training error}=0.02109375\n",
            "2023-02-03 23:50:54,866 [nnabla][INFO]: iter=779 {Training loss}=0.11026257276535034\n",
            "2023-02-03 23:50:54,866 [nnabla][INFO]: iter=779 {Training error}=0.02734375\n",
            "2023-02-03 23:50:54,893 [nnabla][INFO]: iter=789 {Training loss}=0.08590767532587051\n",
            "2023-02-03 23:50:54,894 [nnabla][INFO]: iter=789 {Training error}=0.03125\n",
            "2023-02-03 23:50:54,920 [nnabla][INFO]: iter=799 {Training loss}=0.07821235805749893\n",
            "2023-02-03 23:50:54,920 [nnabla][INFO]: iter=799 {Training error}=0.02109375\n",
            "2023-02-03 23:50:54,921 [nnabla][INFO]: iter=799 {Training time}=0.2864065170288086[sec/100iter] 5.2931067943573[sec]\n",
            "2023-02-03 23:50:54,931 [nnabla][INFO]: iter=800 {Test error}=0.01953125\n",
            "2023-02-03 23:50:54,958 [nnabla][INFO]: iter=809 {Training loss}=0.09842080622911453\n",
            "2023-02-03 23:50:54,958 [nnabla][INFO]: iter=809 {Training error}=0.028125\n",
            "2023-02-03 23:50:54,985 [nnabla][INFO]: iter=819 {Training loss}=0.0691492110490799\n",
            "2023-02-03 23:50:54,985 [nnabla][INFO]: iter=819 {Training error}=0.0234375\n",
            "2023-02-03 23:50:55,012 [nnabla][INFO]: iter=829 {Training loss}=0.0774318128824234\n",
            "2023-02-03 23:50:55,012 [nnabla][INFO]: iter=829 {Training error}=0.025\n",
            "2023-02-03 23:50:55,039 [nnabla][INFO]: iter=839 {Training loss}=0.09178392589092255\n",
            "2023-02-03 23:50:55,039 [nnabla][INFO]: iter=839 {Training error}=0.028125\n",
            "2023-02-03 23:50:55,066 [nnabla][INFO]: iter=849 {Training loss}=0.09770043939352036\n",
            "2023-02-03 23:50:55,067 [nnabla][INFO]: iter=849 {Training error}=0.03046875\n",
            "2023-02-03 23:50:55,095 [nnabla][INFO]: iter=859 {Training loss}=0.08202812075614929\n",
            "2023-02-03 23:50:55,096 [nnabla][INFO]: iter=859 {Training error}=0.02109375\n",
            "2023-02-03 23:50:55,128 [nnabla][INFO]: iter=869 {Training loss}=0.08047274500131607\n",
            "2023-02-03 23:50:55,128 [nnabla][INFO]: iter=869 {Training error}=0.02578125\n",
            "2023-02-03 23:50:55,155 [nnabla][INFO]: iter=879 {Training loss}=0.06398848444223404\n",
            "2023-02-03 23:50:55,155 [nnabla][INFO]: iter=879 {Training error}=0.01953125\n",
            "2023-02-03 23:50:55,182 [nnabla][INFO]: iter=889 {Training loss}=0.07590704411268234\n",
            "2023-02-03 23:50:55,182 [nnabla][INFO]: iter=889 {Training error}=0.0234375\n",
            "2023-02-03 23:50:55,209 [nnabla][INFO]: iter=899 {Training loss}=0.07949072122573853\n",
            "2023-02-03 23:50:55,209 [nnabla][INFO]: iter=899 {Training error}=0.02421875\n",
            "2023-02-03 23:50:55,209 [nnabla][INFO]: iter=899 {Training time}=0.2887098789215088[sec/100iter] 5.581816673278809[sec]\n",
            "2023-02-03 23:50:55,220 [nnabla][INFO]: iter=900 {Test error}=0.01328125\n",
            "2023-02-03 23:50:55,247 [nnabla][INFO]: iter=909 {Training loss}=0.09301193803548813\n",
            "2023-02-03 23:50:55,247 [nnabla][INFO]: iter=909 {Training error}=0.02109375\n",
            "2023-02-03 23:50:55,275 [nnabla][INFO]: iter=919 {Training loss}=0.08632180839776993\n",
            "2023-02-03 23:50:55,275 [nnabla][INFO]: iter=919 {Training error}=0.0265625\n",
            "2023-02-03 23:50:55,303 [nnabla][INFO]: iter=929 {Training loss}=0.07647792994976044\n",
            "2023-02-03 23:50:55,303 [nnabla][INFO]: iter=929 {Training error}=0.0234375\n",
            "2023-02-03 23:50:55,331 [nnabla][INFO]: iter=939 {Training loss}=0.0864514708518982\n",
            "2023-02-03 23:50:55,331 [nnabla][INFO]: iter=939 {Training error}=0.02734375\n",
            "2023-02-03 23:50:55,358 [nnabla][INFO]: iter=949 {Training loss}=0.06258583813905716\n",
            "2023-02-03 23:50:55,358 [nnabla][INFO]: iter=949 {Training error}=0.0234375\n",
            "2023-02-03 23:50:55,385 [nnabla][INFO]: iter=959 {Training loss}=0.0901465117931366\n",
            "2023-02-03 23:50:55,385 [nnabla][INFO]: iter=959 {Training error}=0.02890625\n",
            "2023-02-03 23:50:55,415 [nnabla][INFO]: iter=969 {Training loss}=0.06037106364965439\n",
            "2023-02-03 23:50:55,416 [nnabla][INFO]: iter=969 {Training error}=0.01796875\n",
            "2023-02-03 23:50:55,443 [nnabla][INFO]: iter=979 {Training loss}=0.060519736260175705\n",
            "2023-02-03 23:50:55,443 [nnabla][INFO]: iter=979 {Training error}=0.01484375\n",
            "2023-02-03 23:50:55,472 [nnabla][INFO]: iter=989 {Training loss}=0.08301088958978653\n",
            "2023-02-03 23:50:55,473 [nnabla][INFO]: iter=989 {Training error}=0.0234375\n",
            "2023-02-03 23:50:55,500 [nnabla][INFO]: iter=999 {Training loss}=0.07528552412986755\n",
            "2023-02-03 23:50:55,500 [nnabla][INFO]: iter=999 {Training error}=0.025\n",
            "2023-02-03 23:50:55,500 [nnabla][INFO]: iter=999 {Training time}=0.2911417484283447[sec/100iter] 5.872958421707153[sec]\n",
            "2023-02-03 23:50:55,511 [nnabla][INFO]: iter=1000 {Test error}=0.0140625\n",
            "2023-02-03 23:50:55,525 [nnabla][INFO]: Solver state save (.h5): output/states_1000.h5\n",
            "2023-02-03 23:50:55,532 [nnabla][INFO]: Parameter save (.h5): output/params_1000.h5\n",
            "2023-02-03 23:50:55,532 [nnabla][INFO]: Checkpoint save (.json): output/checkpoint_1000.json\n",
            "2023-02-03 23:50:55,559 [nnabla][INFO]: iter=1009 {Training loss}=0.061504729092121124\n",
            "2023-02-03 23:50:55,559 [nnabla][INFO]: iter=1009 {Training error}=0.02109375\n",
            "2023-02-03 23:50:55,586 [nnabla][INFO]: iter=1019 {Training loss}=0.06224924325942993\n",
            "2023-02-03 23:50:55,586 [nnabla][INFO]: iter=1019 {Training error}=0.021875\n",
            "2023-02-03 23:50:55,614 [nnabla][INFO]: iter=1029 {Training loss}=0.07465647161006927\n",
            "2023-02-03 23:50:55,614 [nnabla][INFO]: iter=1029 {Training error}=0.0234375\n",
            "2023-02-03 23:50:55,641 [nnabla][INFO]: iter=1039 {Training loss}=0.06482971459627151\n",
            "2023-02-03 23:50:55,641 [nnabla][INFO]: iter=1039 {Training error}=0.0203125\n",
            "2023-02-03 23:50:55,668 [nnabla][INFO]: iter=1049 {Training loss}=0.07365833222866058\n",
            "2023-02-03 23:50:55,668 [nnabla][INFO]: iter=1049 {Training error}=0.0203125\n",
            "2023-02-03 23:50:55,697 [nnabla][INFO]: iter=1059 {Training loss}=0.06288536638021469\n",
            "2023-02-03 23:50:55,697 [nnabla][INFO]: iter=1059 {Training error}=0.01796875\n",
            "2023-02-03 23:50:55,725 [nnabla][INFO]: iter=1069 {Training loss}=0.07018417865037918\n",
            "2023-02-03 23:50:55,725 [nnabla][INFO]: iter=1069 {Training error}=0.021875\n",
            "2023-02-03 23:50:55,752 [nnabla][INFO]: iter=1079 {Training loss}=0.07945646345615387\n",
            "2023-02-03 23:50:55,752 [nnabla][INFO]: iter=1079 {Training error}=0.025\n",
            "2023-02-03 23:50:55,779 [nnabla][INFO]: iter=1089 {Training loss}=0.06493847817182541\n",
            "2023-02-03 23:50:55,779 [nnabla][INFO]: iter=1089 {Training error}=0.01875\n",
            "2023-02-03 23:50:55,809 [nnabla][INFO]: iter=1099 {Training loss}=0.06979065388441086\n",
            "2023-02-03 23:50:55,809 [nnabla][INFO]: iter=1099 {Training error}=0.025\n",
            "2023-02-03 23:50:55,809 [nnabla][INFO]: iter=1099 {Training time}=0.30852746963500977[sec/100iter] 6.181485891342163[sec]\n",
            "2023-02-03 23:50:55,820 [nnabla][INFO]: iter=1100 {Test error}=0.0234375\n",
            "2023-02-03 23:50:55,847 [nnabla][INFO]: iter=1109 {Training loss}=0.06515372544527054\n",
            "2023-02-03 23:50:55,847 [nnabla][INFO]: iter=1109 {Training error}=0.01796875\n",
            "2023-02-03 23:50:55,874 [nnabla][INFO]: iter=1119 {Training loss}=0.06664268672466278\n",
            "2023-02-03 23:50:55,875 [nnabla][INFO]: iter=1119 {Training error}=0.021875\n",
            "2023-02-03 23:50:55,902 [nnabla][INFO]: iter=1129 {Training loss}=0.0694427490234375\n",
            "2023-02-03 23:50:55,902 [nnabla][INFO]: iter=1129 {Training error}=0.02265625\n",
            "2023-02-03 23:50:55,929 [nnabla][INFO]: iter=1139 {Training loss}=0.06814507395029068\n",
            "2023-02-03 23:50:55,930 [nnabla][INFO]: iter=1139 {Training error}=0.021875\n",
            "2023-02-03 23:50:55,956 [nnabla][INFO]: iter=1149 {Training loss}=0.07886190712451935\n",
            "2023-02-03 23:50:55,957 [nnabla][INFO]: iter=1149 {Training error}=0.0265625\n",
            "2023-02-03 23:50:55,983 [nnabla][INFO]: iter=1159 {Training loss}=0.07400768995285034\n",
            "2023-02-03 23:50:55,984 [nnabla][INFO]: iter=1159 {Training error}=0.0234375\n",
            "2023-02-03 23:50:56,012 [nnabla][INFO]: iter=1169 {Training loss}=0.04992310330271721\n",
            "2023-02-03 23:50:56,012 [nnabla][INFO]: iter=1169 {Training error}=0.01484375\n",
            "2023-02-03 23:50:56,039 [nnabla][INFO]: iter=1179 {Training loss}=0.07327192276716232\n",
            "2023-02-03 23:50:56,039 [nnabla][INFO]: iter=1179 {Training error}=0.02421875\n",
            "2023-02-03 23:50:56,066 [nnabla][INFO]: iter=1189 {Training loss}=0.07182561606168747\n",
            "2023-02-03 23:50:56,066 [nnabla][INFO]: iter=1189 {Training error}=0.02265625\n",
            "2023-02-03 23:50:56,094 [nnabla][INFO]: iter=1199 {Training loss}=0.06991978734731674\n",
            "2023-02-03 23:50:56,094 [nnabla][INFO]: iter=1199 {Training error}=0.0203125\n",
            "2023-02-03 23:50:56,095 [nnabla][INFO]: iter=1199 {Training time}=0.28549695014953613[sec/100iter] 6.466982841491699[sec]\n",
            "2023-02-03 23:50:56,105 [nnabla][INFO]: iter=1200 {Test error}=0.01640625\n",
            "2023-02-03 23:50:56,136 [nnabla][INFO]: iter=1209 {Training loss}=0.06463932245969772\n",
            "2023-02-03 23:50:56,136 [nnabla][INFO]: iter=1209 {Training error}=0.01640625\n",
            "2023-02-03 23:50:56,165 [nnabla][INFO]: iter=1219 {Training loss}=0.07229462265968323\n",
            "2023-02-03 23:50:56,165 [nnabla][INFO]: iter=1219 {Training error}=0.02265625\n",
            "2023-02-03 23:50:56,193 [nnabla][INFO]: iter=1229 {Training loss}=0.047360628843307495\n",
            "2023-02-03 23:50:56,194 [nnabla][INFO]: iter=1229 {Training error}=0.015625\n",
            "2023-02-03 23:50:56,221 [nnabla][INFO]: iter=1239 {Training loss}=0.05962469056248665\n",
            "2023-02-03 23:50:56,221 [nnabla][INFO]: iter=1239 {Training error}=0.0171875\n",
            "2023-02-03 23:50:56,248 [nnabla][INFO]: iter=1249 {Training loss}=0.07723528891801834\n",
            "2023-02-03 23:50:56,248 [nnabla][INFO]: iter=1249 {Training error}=0.02578125\n",
            "2023-02-03 23:50:56,276 [nnabla][INFO]: iter=1259 {Training loss}=0.06428500264883041\n",
            "2023-02-03 23:50:56,276 [nnabla][INFO]: iter=1259 {Training error}=0.01953125\n",
            "2023-02-03 23:50:56,305 [nnabla][INFO]: iter=1269 {Training loss}=0.07100038230419159\n",
            "2023-02-03 23:50:56,305 [nnabla][INFO]: iter=1269 {Training error}=0.01875\n",
            "2023-02-03 23:50:56,332 [nnabla][INFO]: iter=1279 {Training loss}=0.08278421312570572\n",
            "2023-02-03 23:50:56,333 [nnabla][INFO]: iter=1279 {Training error}=0.021875\n",
            "2023-02-03 23:50:56,360 [nnabla][INFO]: iter=1289 {Training loss}=0.07410567998886108\n",
            "2023-02-03 23:50:56,360 [nnabla][INFO]: iter=1289 {Training error}=0.01953125\n",
            "2023-02-03 23:50:56,386 [nnabla][INFO]: iter=1299 {Training loss}=0.052757371217012405\n",
            "2023-02-03 23:50:56,387 [nnabla][INFO]: iter=1299 {Training error}=0.015625\n",
            "2023-02-03 23:50:56,387 [nnabla][INFO]: iter=1299 {Training time}=0.2922542095184326[sec/100iter] 6.759237051010132[sec]\n",
            "2023-02-03 23:50:56,398 [nnabla][INFO]: iter=1300 {Test error}=0.021875\n",
            "2023-02-03 23:50:56,426 [nnabla][INFO]: iter=1309 {Training loss}=0.05604211241006851\n",
            "2023-02-03 23:50:56,427 [nnabla][INFO]: iter=1309 {Training error}=0.02109375\n",
            "2023-02-03 23:50:56,454 [nnabla][INFO]: iter=1319 {Training loss}=0.06450419127941132\n",
            "2023-02-03 23:50:56,454 [nnabla][INFO]: iter=1319 {Training error}=0.0234375\n",
            "2023-02-03 23:50:56,481 [nnabla][INFO]: iter=1329 {Training loss}=0.04019010812044144\n",
            "2023-02-03 23:50:56,481 [nnabla][INFO]: iter=1329 {Training error}=0.0109375\n",
            "2023-02-03 23:50:56,508 [nnabla][INFO]: iter=1339 {Training loss}=0.05497417598962784\n",
            "2023-02-03 23:50:56,508 [nnabla][INFO]: iter=1339 {Training error}=0.01796875\n",
            "2023-02-03 23:50:56,537 [nnabla][INFO]: iter=1349 {Training loss}=0.0870247632265091\n",
            "2023-02-03 23:50:56,537 [nnabla][INFO]: iter=1349 {Training error}=0.021875\n",
            "2023-02-03 23:50:56,564 [nnabla][INFO]: iter=1359 {Training loss}=0.04832478612661362\n",
            "2023-02-03 23:50:56,565 [nnabla][INFO]: iter=1359 {Training error}=0.0140625\n",
            "2023-02-03 23:50:56,591 [nnabla][INFO]: iter=1369 {Training loss}=0.07993783801794052\n",
            "2023-02-03 23:50:56,591 [nnabla][INFO]: iter=1369 {Training error}=0.01875\n",
            "2023-02-03 23:50:56,619 [nnabla][INFO]: iter=1379 {Training loss}=0.05308274179697037\n",
            "2023-02-03 23:50:56,619 [nnabla][INFO]: iter=1379 {Training error}=0.01484375\n",
            "2023-02-03 23:50:56,646 [nnabla][INFO]: iter=1389 {Training loss}=0.08081980049610138\n",
            "2023-02-03 23:50:56,646 [nnabla][INFO]: iter=1389 {Training error}=0.0234375\n",
            "2023-02-03 23:50:56,673 [nnabla][INFO]: iter=1399 {Training loss}=0.06184845417737961\n",
            "2023-02-03 23:50:56,674 [nnabla][INFO]: iter=1399 {Training error}=0.01484375\n",
            "2023-02-03 23:50:56,674 [nnabla][INFO]: iter=1399 {Training time}=0.28692054748535156[sec/100iter] 7.046157598495483[sec]\n",
            "2023-02-03 23:50:56,685 [nnabla][INFO]: iter=1400 {Test error}=0.0171875\n",
            "2023-02-03 23:50:56,713 [nnabla][INFO]: iter=1409 {Training loss}=0.06183161586523056\n",
            "2023-02-03 23:50:56,713 [nnabla][INFO]: iter=1409 {Training error}=0.01875\n",
            "2023-02-03 23:50:56,740 [nnabla][INFO]: iter=1419 {Training loss}=0.04814580827951431\n",
            "2023-02-03 23:50:56,741 [nnabla][INFO]: iter=1419 {Training error}=0.01640625\n",
            "2023-02-03 23:50:56,768 [nnabla][INFO]: iter=1429 {Training loss}=0.04713187366724014\n",
            "2023-02-03 23:50:56,768 [nnabla][INFO]: iter=1429 {Training error}=0.0140625\n",
            "2023-02-03 23:50:56,795 [nnabla][INFO]: iter=1439 {Training loss}=0.05644556134939194\n",
            "2023-02-03 23:50:56,795 [nnabla][INFO]: iter=1439 {Training error}=0.01484375\n",
            "2023-02-03 23:50:56,822 [nnabla][INFO]: iter=1449 {Training loss}=0.07201460003852844\n",
            "2023-02-03 23:50:56,822 [nnabla][INFO]: iter=1449 {Training error}=0.01796875\n",
            "2023-02-03 23:50:56,851 [nnabla][INFO]: iter=1459 {Training loss}=0.04557473585009575\n",
            "2023-02-03 23:50:56,851 [nnabla][INFO]: iter=1459 {Training error}=0.01328125\n",
            "2023-02-03 23:50:56,878 [nnabla][INFO]: iter=1469 {Training loss}=0.033734697848558426\n",
            "2023-02-03 23:50:56,878 [nnabla][INFO]: iter=1469 {Training error}=0.0078125\n",
            "2023-02-03 23:50:56,908 [nnabla][INFO]: iter=1479 {Training loss}=0.04740528017282486\n",
            "2023-02-03 23:50:56,909 [nnabla][INFO]: iter=1479 {Training error}=0.01640625\n",
            "2023-02-03 23:50:56,937 [nnabla][INFO]: iter=1489 {Training loss}=0.06209006905555725\n",
            "2023-02-03 23:50:56,937 [nnabla][INFO]: iter=1489 {Training error}=0.02109375\n",
            "2023-02-03 23:50:56,964 [nnabla][INFO]: iter=1499 {Training loss}=0.06716370582580566\n",
            "2023-02-03 23:50:56,964 [nnabla][INFO]: iter=1499 {Training error}=0.0203125\n",
            "2023-02-03 23:50:56,964 [nnabla][INFO]: iter=1499 {Training time}=0.2904632091522217[sec/100iter] 7.336620807647705[sec]\n",
            "2023-02-03 23:50:56,975 [nnabla][INFO]: iter=1500 {Test error}=0.01484375\n",
            "2023-02-03 23:50:57,003 [nnabla][INFO]: iter=1509 {Training loss}=0.05072948336601257\n",
            "2023-02-03 23:50:57,003 [nnabla][INFO]: iter=1509 {Training error}=0.01953125\n",
            "2023-02-03 23:50:57,031 [nnabla][INFO]: iter=1519 {Training loss}=0.05092037841677666\n",
            "2023-02-03 23:50:57,031 [nnabla][INFO]: iter=1519 {Training error}=0.01875\n",
            "2023-02-03 23:50:57,059 [nnabla][INFO]: iter=1529 {Training loss}=0.049648381769657135\n",
            "2023-02-03 23:50:57,059 [nnabla][INFO]: iter=1529 {Training error}=0.01640625\n",
            "2023-02-03 23:50:57,087 [nnabla][INFO]: iter=1539 {Training loss}=0.0468924380838871\n",
            "2023-02-03 23:50:57,087 [nnabla][INFO]: iter=1539 {Training error}=0.0125\n",
            "2023-02-03 23:50:57,114 [nnabla][INFO]: iter=1549 {Training loss}=0.039227135479450226\n",
            "2023-02-03 23:50:57,115 [nnabla][INFO]: iter=1549 {Training error}=0.0140625\n",
            "2023-02-03 23:50:57,144 [nnabla][INFO]: iter=1559 {Training loss}=0.05153108760714531\n",
            "2023-02-03 23:50:57,144 [nnabla][INFO]: iter=1559 {Training error}=0.015625\n",
            "2023-02-03 23:50:57,172 [nnabla][INFO]: iter=1569 {Training loss}=0.051948897540569305\n",
            "2023-02-03 23:50:57,173 [nnabla][INFO]: iter=1569 {Training error}=0.01640625\n",
            "2023-02-03 23:50:57,200 [nnabla][INFO]: iter=1579 {Training loss}=0.055752694606781006\n",
            "2023-02-03 23:50:57,200 [nnabla][INFO]: iter=1579 {Training error}=0.0109375\n",
            "2023-02-03 23:50:57,227 [nnabla][INFO]: iter=1589 {Training loss}=0.044039539992809296\n",
            "2023-02-03 23:50:57,227 [nnabla][INFO]: iter=1589 {Training error}=0.01015625\n",
            "2023-02-03 23:50:57,254 [nnabla][INFO]: iter=1599 {Training loss}=0.0383889265358448\n",
            "2023-02-03 23:50:57,254 [nnabla][INFO]: iter=1599 {Training error}=0.01171875\n",
            "2023-02-03 23:50:57,254 [nnabla][INFO]: iter=1599 {Training time}=0.2899894714355469[sec/100iter] 7.626610279083252[sec]\n",
            "2023-02-03 23:50:57,267 [nnabla][INFO]: iter=1600 {Test error}=0.0125\n",
            "2023-02-03 23:50:57,294 [nnabla][INFO]: iter=1609 {Training loss}=0.04611437767744064\n",
            "2023-02-03 23:50:57,294 [nnabla][INFO]: iter=1609 {Training error}=0.0125\n",
            "2023-02-03 23:50:57,322 [nnabla][INFO]: iter=1619 {Training loss}=0.05044402927160263\n",
            "2023-02-03 23:50:57,323 [nnabla][INFO]: iter=1619 {Training error}=0.015625\n",
            "2023-02-03 23:50:57,350 [nnabla][INFO]: iter=1629 {Training loss}=0.07347222417593002\n",
            "2023-02-03 23:50:57,350 [nnabla][INFO]: iter=1629 {Training error}=0.01875\n",
            "2023-02-03 23:50:57,377 [nnabla][INFO]: iter=1639 {Training loss}=0.04493755102157593\n",
            "2023-02-03 23:50:57,377 [nnabla][INFO]: iter=1639 {Training error}=0.01328125\n",
            "2023-02-03 23:50:57,404 [nnabla][INFO]: iter=1649 {Training loss}=0.06856825202703476\n",
            "2023-02-03 23:50:57,404 [nnabla][INFO]: iter=1649 {Training error}=0.01953125\n",
            "2023-02-03 23:50:57,434 [nnabla][INFO]: iter=1659 {Training loss}=0.06560304015874863\n",
            "2023-02-03 23:50:57,434 [nnabla][INFO]: iter=1659 {Training error}=0.0234375\n",
            "2023-02-03 23:50:57,461 [nnabla][INFO]: iter=1669 {Training loss}=0.06490667164325714\n",
            "2023-02-03 23:50:57,461 [nnabla][INFO]: iter=1669 {Training error}=0.01875\n",
            "2023-02-03 23:50:57,488 [nnabla][INFO]: iter=1679 {Training loss}=0.04538317397236824\n",
            "2023-02-03 23:50:57,488 [nnabla][INFO]: iter=1679 {Training error}=0.01171875\n",
            "2023-02-03 23:50:57,516 [nnabla][INFO]: iter=1689 {Training loss}=0.0635487288236618\n",
            "2023-02-03 23:50:57,517 [nnabla][INFO]: iter=1689 {Training error}=0.015625\n",
            "2023-02-03 23:50:57,543 [nnabla][INFO]: iter=1699 {Training loss}=0.04014797881245613\n",
            "2023-02-03 23:50:57,544 [nnabla][INFO]: iter=1699 {Training error}=0.01171875\n",
            "2023-02-03 23:50:57,544 [nnabla][INFO]: iter=1699 {Training time}=0.28959035873413086[sec/100iter] 7.916200637817383[sec]\n",
            "2023-02-03 23:50:57,554 [nnabla][INFO]: iter=1700 {Test error}=0.01484375\n",
            "2023-02-03 23:50:57,581 [nnabla][INFO]: iter=1709 {Training loss}=0.05677099898457527\n",
            "2023-02-03 23:50:57,581 [nnabla][INFO]: iter=1709 {Training error}=0.0125\n",
            "2023-02-03 23:50:57,608 [nnabla][INFO]: iter=1719 {Training loss}=0.05564602464437485\n",
            "2023-02-03 23:50:57,608 [nnabla][INFO]: iter=1719 {Training error}=0.01796875\n",
            "2023-02-03 23:50:57,635 [nnabla][INFO]: iter=1729 {Training loss}=0.047992970794439316\n",
            "2023-02-03 23:50:57,636 [nnabla][INFO]: iter=1729 {Training error}=0.015625\n",
            "2023-02-03 23:50:57,662 [nnabla][INFO]: iter=1739 {Training loss}=0.05182638019323349\n",
            "2023-02-03 23:50:57,662 [nnabla][INFO]: iter=1739 {Training error}=0.0140625\n",
            "2023-02-03 23:50:57,689 [nnabla][INFO]: iter=1749 {Training loss}=0.03991686552762985\n",
            "2023-02-03 23:50:57,689 [nnabla][INFO]: iter=1749 {Training error}=0.01640625\n",
            "2023-02-03 23:50:57,717 [nnabla][INFO]: iter=1759 {Training loss}=0.051230113953351974\n",
            "2023-02-03 23:50:57,717 [nnabla][INFO]: iter=1759 {Training error}=0.01328125\n",
            "2023-02-03 23:50:57,744 [nnabla][INFO]: iter=1769 {Training loss}=0.06583067029714584\n",
            "2023-02-03 23:50:57,744 [nnabla][INFO]: iter=1769 {Training error}=0.02109375\n",
            "2023-02-03 23:50:57,771 [nnabla][INFO]: iter=1779 {Training loss}=0.05834316462278366\n",
            "2023-02-03 23:50:57,772 [nnabla][INFO]: iter=1779 {Training error}=0.01640625\n",
            "2023-02-03 23:50:57,798 [nnabla][INFO]: iter=1789 {Training loss}=0.048343926668167114\n",
            "2023-02-03 23:50:57,799 [nnabla][INFO]: iter=1789 {Training error}=0.0171875\n",
            "2023-02-03 23:50:57,826 [nnabla][INFO]: iter=1799 {Training loss}=0.041669994592666626\n",
            "2023-02-03 23:50:57,827 [nnabla][INFO]: iter=1799 {Training error}=0.01015625\n",
            "2023-02-03 23:50:57,827 [nnabla][INFO]: iter=1799 {Training time}=0.28301525115966797[sec/100iter] 8.19921588897705[sec]\n",
            "2023-02-03 23:50:57,837 [nnabla][INFO]: iter=1800 {Test error}=0.015625\n",
            "2023-02-03 23:50:57,864 [nnabla][INFO]: iter=1809 {Training loss}=0.04927108809351921\n",
            "2023-02-03 23:50:57,864 [nnabla][INFO]: iter=1809 {Training error}=0.0171875\n",
            "2023-02-03 23:50:57,891 [nnabla][INFO]: iter=1819 {Training loss}=0.06532930582761765\n",
            "2023-02-03 23:50:57,891 [nnabla][INFO]: iter=1819 {Training error}=0.021875\n",
            "2023-02-03 23:50:57,918 [nnabla][INFO]: iter=1829 {Training loss}=0.04328668490052223\n",
            "2023-02-03 23:50:57,918 [nnabla][INFO]: iter=1829 {Training error}=0.01796875\n",
            "2023-02-03 23:50:57,945 [nnabla][INFO]: iter=1839 {Training loss}=0.05519887059926987\n",
            "2023-02-03 23:50:57,945 [nnabla][INFO]: iter=1839 {Training error}=0.01953125\n",
            "2023-02-03 23:50:57,971 [nnabla][INFO]: iter=1849 {Training loss}=0.048033472150564194\n",
            "2023-02-03 23:50:57,972 [nnabla][INFO]: iter=1849 {Training error}=0.0140625\n",
            "2023-02-03 23:50:58,000 [nnabla][INFO]: iter=1859 {Training loss}=0.05240146070718765\n",
            "2023-02-03 23:50:58,000 [nnabla][INFO]: iter=1859 {Training error}=0.01953125\n",
            "2023-02-03 23:50:58,027 [nnabla][INFO]: iter=1869 {Training loss}=0.06400321424007416\n",
            "2023-02-03 23:50:58,027 [nnabla][INFO]: iter=1869 {Training error}=0.02421875\n",
            "2023-02-03 23:50:58,054 [nnabla][INFO]: iter=1879 {Training loss}=0.03449695557355881\n",
            "2023-02-03 23:50:58,055 [nnabla][INFO]: iter=1879 {Training error}=0.009375\n",
            "2023-02-03 23:50:58,081 [nnabla][INFO]: iter=1889 {Training loss}=0.043954379856586456\n",
            "2023-02-03 23:50:58,082 [nnabla][INFO]: iter=1889 {Training error}=0.0140625\n",
            "2023-02-03 23:50:58,111 [nnabla][INFO]: iter=1899 {Training loss}=0.030208265408873558\n",
            "2023-02-03 23:50:58,112 [nnabla][INFO]: iter=1899 {Training error}=0.00859375\n",
            "2023-02-03 23:50:58,112 [nnabla][INFO]: iter=1899 {Training time}=0.28509068489074707[sec/100iter] 8.484306573867798[sec]\n",
            "2023-02-03 23:50:58,124 [nnabla][INFO]: iter=1900 {Test error}=0.0140625\n",
            "2023-02-03 23:50:58,152 [nnabla][INFO]: iter=1909 {Training loss}=0.053644418716430664\n",
            "2023-02-03 23:50:58,152 [nnabla][INFO]: iter=1909 {Training error}=0.01796875\n",
            "2023-02-03 23:50:58,186 [nnabla][INFO]: iter=1919 {Training loss}=0.04819158837199211\n",
            "2023-02-03 23:50:58,187 [nnabla][INFO]: iter=1919 {Training error}=0.0171875\n",
            "2023-02-03 23:50:58,224 [nnabla][INFO]: iter=1929 {Training loss}=0.05186425894498825\n",
            "2023-02-03 23:50:58,225 [nnabla][INFO]: iter=1929 {Training error}=0.01328125\n",
            "2023-02-03 23:50:58,263 [nnabla][INFO]: iter=1939 {Training loss}=0.052218832075595856\n",
            "2023-02-03 23:50:58,263 [nnabla][INFO]: iter=1939 {Training error}=0.01484375\n",
            "2023-02-03 23:50:58,300 [nnabla][INFO]: iter=1949 {Training loss}=0.060140013694763184\n",
            "2023-02-03 23:50:58,300 [nnabla][INFO]: iter=1949 {Training error}=0.0203125\n",
            "2023-02-03 23:50:58,334 [nnabla][INFO]: iter=1959 {Training loss}=0.03873106837272644\n",
            "2023-02-03 23:50:58,334 [nnabla][INFO]: iter=1959 {Training error}=0.01171875\n",
            "2023-02-03 23:50:58,367 [nnabla][INFO]: iter=1969 {Training loss}=0.03869577497243881\n",
            "2023-02-03 23:50:58,367 [nnabla][INFO]: iter=1969 {Training error}=0.009375\n",
            "2023-02-03 23:50:58,401 [nnabla][INFO]: iter=1979 {Training loss}=0.03546110540628433\n",
            "2023-02-03 23:50:58,401 [nnabla][INFO]: iter=1979 {Training error}=0.0125\n",
            "2023-02-03 23:50:58,436 [nnabla][INFO]: iter=1989 {Training loss}=0.04737681895494461\n",
            "2023-02-03 23:50:58,437 [nnabla][INFO]: iter=1989 {Training error}=0.015625\n",
            "2023-02-03 23:50:58,470 [nnabla][INFO]: iter=1999 {Training loss}=0.04198241978883743\n",
            "2023-02-03 23:50:58,470 [nnabla][INFO]: iter=1999 {Training error}=0.0125\n",
            "2023-02-03 23:50:58,470 [nnabla][INFO]: iter=1999 {Training time}=0.3585813045501709[sec/100iter] 8.842887878417969[sec]\n",
            "2023-02-03 23:50:58,488 [nnabla][INFO]: iter=2000 {Test error}=0.015625\n",
            "2023-02-03 23:50:58,507 [nnabla][INFO]: Solver state save (.h5): output/states_2000.h5\n",
            "2023-02-03 23:50:58,516 [nnabla][INFO]: Parameter save (.h5): output/params_2000.h5\n",
            "2023-02-03 23:50:58,516 [nnabla][INFO]: Checkpoint save (.json): output/checkpoint_2000.json\n",
            "2023-02-03 23:50:58,551 [nnabla][INFO]: iter=2009 {Training loss}=0.05493149161338806\n",
            "2023-02-03 23:50:58,551 [nnabla][INFO]: iter=2009 {Training error}=0.01875\n",
            "2023-02-03 23:50:58,584 [nnabla][INFO]: iter=2019 {Training loss}=0.03802264481782913\n",
            "2023-02-03 23:50:58,584 [nnabla][INFO]: iter=2019 {Training error}=0.009375\n",
            "2023-02-03 23:50:58,617 [nnabla][INFO]: iter=2029 {Training loss}=0.0510188452899456\n",
            "2023-02-03 23:50:58,617 [nnabla][INFO]: iter=2029 {Training error}=0.015625\n",
            "2023-02-03 23:50:58,650 [nnabla][INFO]: iter=2039 {Training loss}=0.053475521504879\n",
            "2023-02-03 23:50:58,650 [nnabla][INFO]: iter=2039 {Training error}=0.01875\n",
            "2023-02-03 23:50:58,683 [nnabla][INFO]: iter=2049 {Training loss}=0.04482857882976532\n",
            "2023-02-03 23:50:58,683 [nnabla][INFO]: iter=2049 {Training error}=0.01484375\n",
            "2023-02-03 23:50:58,716 [nnabla][INFO]: iter=2059 {Training loss}=0.02599911391735077\n",
            "2023-02-03 23:50:58,716 [nnabla][INFO]: iter=2059 {Training error}=0.0078125\n",
            "2023-02-03 23:50:58,749 [nnabla][INFO]: iter=2069 {Training loss}=0.04790106415748596\n",
            "2023-02-03 23:50:58,749 [nnabla][INFO]: iter=2069 {Training error}=0.01640625\n",
            "2023-02-03 23:50:58,783 [nnabla][INFO]: iter=2079 {Training loss}=0.028131727129220963\n",
            "2023-02-03 23:50:58,783 [nnabla][INFO]: iter=2079 {Training error}=0.009375\n",
            "2023-02-03 23:50:58,816 [nnabla][INFO]: iter=2089 {Training loss}=0.0474320612847805\n",
            "2023-02-03 23:50:58,816 [nnabla][INFO]: iter=2089 {Training error}=0.0125\n",
            "2023-02-03 23:50:58,849 [nnabla][INFO]: iter=2099 {Training loss}=0.03644722327589989\n",
            "2023-02-03 23:50:58,849 [nnabla][INFO]: iter=2099 {Training error}=0.01171875\n",
            "2023-02-03 23:50:58,849 [nnabla][INFO]: iter=2099 {Training time}=0.3788430690765381[sec/100iter] 9.221730947494507[sec]\n",
            "2023-02-03 23:50:58,866 [nnabla][INFO]: iter=2100 {Test error}=0.01015625\n",
            "2023-02-03 23:50:58,899 [nnabla][INFO]: iter=2109 {Training loss}=0.034792788326740265\n",
            "2023-02-03 23:50:58,899 [nnabla][INFO]: iter=2109 {Training error}=0.01328125\n",
            "2023-02-03 23:50:58,933 [nnabla][INFO]: iter=2119 {Training loss}=0.043464045971632004\n",
            "2023-02-03 23:50:58,933 [nnabla][INFO]: iter=2119 {Training error}=0.01328125\n",
            "2023-02-03 23:50:58,968 [nnabla][INFO]: iter=2129 {Training loss}=0.054343510419130325\n",
            "2023-02-03 23:50:58,968 [nnabla][INFO]: iter=2129 {Training error}=0.01484375\n",
            "2023-02-03 23:50:59,002 [nnabla][INFO]: iter=2139 {Training loss}=0.05081361532211304\n",
            "2023-02-03 23:50:59,002 [nnabla][INFO]: iter=2139 {Training error}=0.01875\n",
            "2023-02-03 23:50:59,035 [nnabla][INFO]: iter=2149 {Training loss}=0.04991353303194046\n",
            "2023-02-03 23:50:59,035 [nnabla][INFO]: iter=2149 {Training error}=0.0125\n",
            "2023-02-03 23:50:59,069 [nnabla][INFO]: iter=2159 {Training loss}=0.03554283455014229\n",
            "2023-02-03 23:50:59,069 [nnabla][INFO]: iter=2159 {Training error}=0.0125\n",
            "2023-02-03 23:50:59,103 [nnabla][INFO]: iter=2169 {Training loss}=0.07596390694379807\n",
            "2023-02-03 23:50:59,103 [nnabla][INFO]: iter=2169 {Training error}=0.01953125\n",
            "2023-02-03 23:50:59,137 [nnabla][INFO]: iter=2179 {Training loss}=0.037108082324266434\n",
            "2023-02-03 23:50:59,137 [nnabla][INFO]: iter=2179 {Training error}=0.01484375\n",
            "2023-02-03 23:50:59,170 [nnabla][INFO]: iter=2189 {Training loss}=0.06630631536245346\n",
            "2023-02-03 23:50:59,170 [nnabla][INFO]: iter=2189 {Training error}=0.0203125\n",
            "2023-02-03 23:50:59,205 [nnabla][INFO]: iter=2199 {Training loss}=0.0414562001824379\n",
            "2023-02-03 23:50:59,205 [nnabla][INFO]: iter=2199 {Training error}=0.01171875\n",
            "2023-02-03 23:50:59,205 [nnabla][INFO]: iter=2199 {Training time}=0.35589051246643066[sec/100iter] 9.577621459960938[sec]\n",
            "2023-02-03 23:50:59,222 [nnabla][INFO]: iter=2200 {Test error}=0.01171875\n",
            "2023-02-03 23:50:59,256 [nnabla][INFO]: iter=2209 {Training loss}=0.04172078147530556\n",
            "2023-02-03 23:50:59,256 [nnabla][INFO]: iter=2209 {Training error}=0.0140625\n",
            "2023-02-03 23:50:59,290 [nnabla][INFO]: iter=2219 {Training loss}=0.03896024823188782\n",
            "2023-02-03 23:50:59,291 [nnabla][INFO]: iter=2219 {Training error}=0.0140625\n",
            "2023-02-03 23:50:59,324 [nnabla][INFO]: iter=2229 {Training loss}=0.035011254251003265\n",
            "2023-02-03 23:50:59,324 [nnabla][INFO]: iter=2229 {Training error}=0.01484375\n",
            "2023-02-03 23:50:59,358 [nnabla][INFO]: iter=2239 {Training loss}=0.037361521273851395\n",
            "2023-02-03 23:50:59,358 [nnabla][INFO]: iter=2239 {Training error}=0.00859375\n",
            "2023-02-03 23:50:59,392 [nnabla][INFO]: iter=2249 {Training loss}=0.04009842872619629\n",
            "2023-02-03 23:50:59,392 [nnabla][INFO]: iter=2249 {Training error}=0.0109375\n",
            "2023-02-03 23:50:59,426 [nnabla][INFO]: iter=2259 {Training loss}=0.027714883908629417\n",
            "2023-02-03 23:50:59,427 [nnabla][INFO]: iter=2259 {Training error}=0.009375\n",
            "2023-02-03 23:50:59,462 [nnabla][INFO]: iter=2269 {Training loss}=0.05372185260057449\n",
            "2023-02-03 23:50:59,462 [nnabla][INFO]: iter=2269 {Training error}=0.01796875\n",
            "2023-02-03 23:50:59,498 [nnabla][INFO]: iter=2279 {Training loss}=0.04215822368860245\n",
            "2023-02-03 23:50:59,498 [nnabla][INFO]: iter=2279 {Training error}=0.01171875\n",
            "2023-02-03 23:50:59,535 [nnabla][INFO]: iter=2289 {Training loss}=0.050399262458086014\n",
            "2023-02-03 23:50:59,535 [nnabla][INFO]: iter=2289 {Training error}=0.0171875\n",
            "2023-02-03 23:50:59,569 [nnabla][INFO]: iter=2299 {Training loss}=0.022523973137140274\n",
            "2023-02-03 23:50:59,570 [nnabla][INFO]: iter=2299 {Training error}=0.00859375\n",
            "2023-02-03 23:50:59,570 [nnabla][INFO]: iter=2299 {Training time}=0.3647308349609375[sec/100iter] 9.942352294921875[sec]\n",
            "2023-02-03 23:50:59,587 [nnabla][INFO]: iter=2300 {Test error}=0.01875\n",
            "2023-02-03 23:50:59,622 [nnabla][INFO]: iter=2309 {Training loss}=0.049961213022470474\n",
            "2023-02-03 23:50:59,622 [nnabla][INFO]: iter=2309 {Training error}=0.01484375\n",
            "2023-02-03 23:50:59,657 [nnabla][INFO]: iter=2319 {Training loss}=0.03756467252969742\n",
            "2023-02-03 23:50:59,658 [nnabla][INFO]: iter=2319 {Training error}=0.01015625\n",
            "2023-02-03 23:50:59,695 [nnabla][INFO]: iter=2329 {Training loss}=0.05371930077672005\n",
            "2023-02-03 23:50:59,695 [nnabla][INFO]: iter=2329 {Training error}=0.01875\n",
            "2023-02-03 23:50:59,732 [nnabla][INFO]: iter=2339 {Training loss}=0.04345469921827316\n",
            "2023-02-03 23:50:59,733 [nnabla][INFO]: iter=2339 {Training error}=0.0140625\n",
            "2023-02-03 23:50:59,769 [nnabla][INFO]: iter=2349 {Training loss}=0.036969516426324844\n",
            "2023-02-03 23:50:59,769 [nnabla][INFO]: iter=2349 {Training error}=0.01484375\n",
            "2023-02-03 23:50:59,804 [nnabla][INFO]: iter=2359 {Training loss}=0.03817986696958542\n",
            "2023-02-03 23:50:59,804 [nnabla][INFO]: iter=2359 {Training error}=0.0109375\n",
            "2023-02-03 23:50:59,839 [nnabla][INFO]: iter=2369 {Training loss}=0.037101954221725464\n",
            "2023-02-03 23:50:59,839 [nnabla][INFO]: iter=2369 {Training error}=0.00859375\n",
            "2023-02-03 23:50:59,874 [nnabla][INFO]: iter=2379 {Training loss}=0.03523910045623779\n",
            "2023-02-03 23:50:59,874 [nnabla][INFO]: iter=2379 {Training error}=0.009375\n",
            "2023-02-03 23:50:59,911 [nnabla][INFO]: iter=2389 {Training loss}=0.028821568936109543\n",
            "2023-02-03 23:50:59,911 [nnabla][INFO]: iter=2389 {Training error}=0.0109375\n",
            "2023-02-03 23:50:59,947 [nnabla][INFO]: iter=2399 {Training loss}=0.045019712299108505\n",
            "2023-02-03 23:50:59,948 [nnabla][INFO]: iter=2399 {Training error}=0.01171875\n",
            "2023-02-03 23:50:59,948 [nnabla][INFO]: iter=2399 {Training time}=0.3778798580169678[sec/100iter] 10.320232152938843[sec]\n",
            "2023-02-03 23:50:59,965 [nnabla][INFO]: iter=2400 {Test error}=0.00859375\n",
            "2023-02-03 23:51:00,007 [nnabla][INFO]: iter=2409 {Training loss}=0.04047699272632599\n",
            "2023-02-03 23:51:00,008 [nnabla][INFO]: iter=2409 {Training error}=0.01484375\n",
            "2023-02-03 23:51:00,043 [nnabla][INFO]: iter=2419 {Training loss}=0.04837331920862198\n",
            "2023-02-03 23:51:00,043 [nnabla][INFO]: iter=2419 {Training error}=0.01015625\n",
            "2023-02-03 23:51:00,078 [nnabla][INFO]: iter=2429 {Training loss}=0.04708145186305046\n",
            "2023-02-03 23:51:00,079 [nnabla][INFO]: iter=2429 {Training error}=0.01171875\n",
            "2023-02-03 23:51:00,116 [nnabla][INFO]: iter=2439 {Training loss}=0.06048223376274109\n",
            "2023-02-03 23:51:00,116 [nnabla][INFO]: iter=2439 {Training error}=0.01484375\n",
            "2023-02-03 23:51:00,150 [nnabla][INFO]: iter=2449 {Training loss}=0.03519395366311073\n",
            "2023-02-03 23:51:00,150 [nnabla][INFO]: iter=2449 {Training error}=0.01171875\n",
            "2023-02-03 23:51:00,184 [nnabla][INFO]: iter=2459 {Training loss}=0.02947598323225975\n",
            "2023-02-03 23:51:00,184 [nnabla][INFO]: iter=2459 {Training error}=0.0109375\n",
            "2023-02-03 23:51:00,218 [nnabla][INFO]: iter=2469 {Training loss}=0.043241556733846664\n",
            "2023-02-03 23:51:00,219 [nnabla][INFO]: iter=2469 {Training error}=0.0171875\n",
            "2023-02-03 23:51:00,259 [nnabla][INFO]: iter=2479 {Training loss}=0.017900507897138596\n",
            "2023-02-03 23:51:00,259 [nnabla][INFO]: iter=2479 {Training error}=0.00546875\n",
            "2023-02-03 23:51:00,294 [nnabla][INFO]: iter=2489 {Training loss}=0.033497367054224014\n",
            "2023-02-03 23:51:00,295 [nnabla][INFO]: iter=2489 {Training error}=0.00859375\n",
            "2023-02-03 23:51:00,329 [nnabla][INFO]: iter=2499 {Training loss}=0.03109678626060486\n",
            "2023-02-03 23:51:00,330 [nnabla][INFO]: iter=2499 {Training error}=0.0125\n",
            "2023-02-03 23:51:00,330 [nnabla][INFO]: iter=2499 {Training time}=0.38199281692504883[sec/100iter] 10.702224969863892[sec]\n",
            "2023-02-03 23:51:00,346 [nnabla][INFO]: iter=2500 {Test error}=0.01328125\n",
            "2023-02-03 23:51:00,381 [nnabla][INFO]: iter=2509 {Training loss}=0.029792645946145058\n",
            "2023-02-03 23:51:00,381 [nnabla][INFO]: iter=2509 {Training error}=0.01015625\n",
            "2023-02-03 23:51:00,416 [nnabla][INFO]: iter=2519 {Training loss}=0.04795924946665764\n",
            "2023-02-03 23:51:00,416 [nnabla][INFO]: iter=2519 {Training error}=0.01171875\n",
            "2023-02-03 23:51:00,450 [nnabla][INFO]: iter=2529 {Training loss}=0.038911715149879456\n",
            "2023-02-03 23:51:00,450 [nnabla][INFO]: iter=2529 {Training error}=0.0140625\n",
            "2023-02-03 23:51:00,487 [nnabla][INFO]: iter=2539 {Training loss}=0.03525501489639282\n",
            "2023-02-03 23:51:00,487 [nnabla][INFO]: iter=2539 {Training error}=0.0140625\n",
            "2023-02-03 23:51:00,522 [nnabla][INFO]: iter=2549 {Training loss}=0.06728599965572357\n",
            "2023-02-03 23:51:00,522 [nnabla][INFO]: iter=2549 {Training error}=0.02109375\n",
            "2023-02-03 23:51:00,556 [nnabla][INFO]: iter=2559 {Training loss}=0.03866247832775116\n",
            "2023-02-03 23:51:00,556 [nnabla][INFO]: iter=2559 {Training error}=0.01640625\n",
            "2023-02-03 23:51:00,592 [nnabla][INFO]: iter=2569 {Training loss}=0.03378668054938316\n",
            "2023-02-03 23:51:00,593 [nnabla][INFO]: iter=2569 {Training error}=0.009375\n",
            "2023-02-03 23:51:00,628 [nnabla][INFO]: iter=2579 {Training loss}=0.03937215358018875\n",
            "2023-02-03 23:51:00,629 [nnabla][INFO]: iter=2579 {Training error}=0.01328125\n",
            "2023-02-03 23:51:00,664 [nnabla][INFO]: iter=2589 {Training loss}=0.035116046667099\n",
            "2023-02-03 23:51:00,664 [nnabla][INFO]: iter=2589 {Training error}=0.01015625\n",
            "2023-02-03 23:51:00,698 [nnabla][INFO]: iter=2599 {Training loss}=0.03077540174126625\n",
            "2023-02-03 23:51:00,698 [nnabla][INFO]: iter=2599 {Training error}=0.00859375\n",
            "2023-02-03 23:51:00,699 [nnabla][INFO]: iter=2599 {Training time}=0.3688955307006836[sec/100iter] 11.071120500564575[sec]\n",
            "2023-02-03 23:51:00,716 [nnabla][INFO]: iter=2600 {Test error}=0.01328125\n",
            "2023-02-03 23:51:00,750 [nnabla][INFO]: iter=2609 {Training loss}=0.03952203318476677\n",
            "2023-02-03 23:51:00,750 [nnabla][INFO]: iter=2609 {Training error}=0.0125\n",
            "2023-02-03 23:51:00,784 [nnabla][INFO]: iter=2619 {Training loss}=0.040209077298641205\n",
            "2023-02-03 23:51:00,784 [nnabla][INFO]: iter=2619 {Training error}=0.0140625\n",
            "2023-02-03 23:51:00,818 [nnabla][INFO]: iter=2629 {Training loss}=0.04112957790493965\n",
            "2023-02-03 23:51:00,818 [nnabla][INFO]: iter=2629 {Training error}=0.01015625\n",
            "2023-02-03 23:51:00,852 [nnabla][INFO]: iter=2639 {Training loss}=0.03246011212468147\n",
            "2023-02-03 23:51:00,852 [nnabla][INFO]: iter=2639 {Training error}=0.009375\n",
            "2023-02-03 23:51:00,885 [nnabla][INFO]: iter=2649 {Training loss}=0.03816259652376175\n",
            "2023-02-03 23:51:00,885 [nnabla][INFO]: iter=2649 {Training error}=0.0171875\n",
            "2023-02-03 23:51:00,919 [nnabla][INFO]: iter=2659 {Training loss}=0.03947953134775162\n",
            "2023-02-03 23:51:00,919 [nnabla][INFO]: iter=2659 {Training error}=0.01171875\n",
            "2023-02-03 23:51:00,952 [nnabla][INFO]: iter=2669 {Training loss}=0.04822837561368942\n",
            "2023-02-03 23:51:00,953 [nnabla][INFO]: iter=2669 {Training error}=0.01484375\n",
            "2023-02-03 23:51:00,986 [nnabla][INFO]: iter=2679 {Training loss}=0.03589073568582535\n",
            "2023-02-03 23:51:00,986 [nnabla][INFO]: iter=2679 {Training error}=0.01015625\n",
            "2023-02-03 23:51:01,019 [nnabla][INFO]: iter=2689 {Training loss}=0.03220365568995476\n",
            "2023-02-03 23:51:01,019 [nnabla][INFO]: iter=2689 {Training error}=0.009375\n",
            "2023-02-03 23:51:01,053 [nnabla][INFO]: iter=2699 {Training loss}=0.04946219548583031\n",
            "2023-02-03 23:51:01,053 [nnabla][INFO]: iter=2699 {Training error}=0.015625\n",
            "2023-02-03 23:51:01,054 [nnabla][INFO]: iter=2699 {Training time}=0.35496950149536133[sec/100iter] 11.426090002059937[sec]\n",
            "2023-02-03 23:51:01,070 [nnabla][INFO]: iter=2700 {Test error}=0.0109375\n",
            "2023-02-03 23:51:01,104 [nnabla][INFO]: iter=2709 {Training loss}=0.04556873068213463\n",
            "2023-02-03 23:51:01,104 [nnabla][INFO]: iter=2709 {Training error}=0.0125\n",
            "2023-02-03 23:51:01,138 [nnabla][INFO]: iter=2719 {Training loss}=0.039641547948122025\n",
            "2023-02-03 23:51:01,138 [nnabla][INFO]: iter=2719 {Training error}=0.0140625\n",
            "2023-02-03 23:51:01,171 [nnabla][INFO]: iter=2729 {Training loss}=0.04088116064667702\n",
            "2023-02-03 23:51:01,172 [nnabla][INFO]: iter=2729 {Training error}=0.0140625\n",
            "2023-02-03 23:51:01,205 [nnabla][INFO]: iter=2739 {Training loss}=0.0334118977189064\n",
            "2023-02-03 23:51:01,205 [nnabla][INFO]: iter=2739 {Training error}=0.0125\n",
            "2023-02-03 23:51:01,239 [nnabla][INFO]: iter=2749 {Training loss}=0.024890238419175148\n",
            "2023-02-03 23:51:01,239 [nnabla][INFO]: iter=2749 {Training error}=0.00859375\n",
            "2023-02-03 23:51:01,275 [nnabla][INFO]: iter=2759 {Training loss}=0.03402874618768692\n",
            "2023-02-03 23:51:01,275 [nnabla][INFO]: iter=2759 {Training error}=0.0125\n",
            "2023-02-03 23:51:01,311 [nnabla][INFO]: iter=2769 {Training loss}=0.03666234388947487\n",
            "2023-02-03 23:51:01,312 [nnabla][INFO]: iter=2769 {Training error}=0.01171875\n",
            "2023-02-03 23:51:01,346 [nnabla][INFO]: iter=2779 {Training loss}=0.03253629058599472\n",
            "2023-02-03 23:51:01,346 [nnabla][INFO]: iter=2779 {Training error}=0.00859375\n",
            "2023-02-03 23:51:01,384 [nnabla][INFO]: iter=2789 {Training loss}=0.037343092262744904\n",
            "2023-02-03 23:51:01,384 [nnabla][INFO]: iter=2789 {Training error}=0.01171875\n",
            "2023-02-03 23:51:01,421 [nnabla][INFO]: iter=2799 {Training loss}=0.032905615866184235\n",
            "2023-02-03 23:51:01,422 [nnabla][INFO]: iter=2799 {Training error}=0.01015625\n",
            "2023-02-03 23:51:01,422 [nnabla][INFO]: iter=2799 {Training time}=0.3681657314300537[sec/100iter] 11.79425573348999[sec]\n",
            "2023-02-03 23:51:01,442 [nnabla][INFO]: iter=2800 {Test error}=0.0140625\n",
            "2023-02-03 23:51:01,476 [nnabla][INFO]: iter=2809 {Training loss}=0.03162766993045807\n",
            "2023-02-03 23:51:01,476 [nnabla][INFO]: iter=2809 {Training error}=0.01171875\n",
            "2023-02-03 23:51:01,517 [nnabla][INFO]: iter=2819 {Training loss}=0.02542572282254696\n",
            "2023-02-03 23:51:01,517 [nnabla][INFO]: iter=2819 {Training error}=0.00703125\n",
            "2023-02-03 23:51:01,551 [nnabla][INFO]: iter=2829 {Training loss}=0.030763590708374977\n",
            "2023-02-03 23:51:01,551 [nnabla][INFO]: iter=2829 {Training error}=0.00859375\n",
            "2023-02-03 23:51:01,586 [nnabla][INFO]: iter=2839 {Training loss}=0.03635231405496597\n",
            "2023-02-03 23:51:01,586 [nnabla][INFO]: iter=2839 {Training error}=0.015625\n",
            "2023-02-03 23:51:01,620 [nnabla][INFO]: iter=2849 {Training loss}=0.03442249819636345\n",
            "2023-02-03 23:51:01,620 [nnabla][INFO]: iter=2849 {Training error}=0.009375\n",
            "2023-02-03 23:51:01,654 [nnabla][INFO]: iter=2859 {Training loss}=0.040970511734485626\n",
            "2023-02-03 23:51:01,654 [nnabla][INFO]: iter=2859 {Training error}=0.01015625\n",
            "2023-02-03 23:51:01,688 [nnabla][INFO]: iter=2869 {Training loss}=0.04235587269067764\n",
            "2023-02-03 23:51:01,688 [nnabla][INFO]: iter=2869 {Training error}=0.01953125\n",
            "2023-02-03 23:51:01,724 [nnabla][INFO]: iter=2879 {Training loss}=0.039602894335985184\n",
            "2023-02-03 23:51:01,725 [nnabla][INFO]: iter=2879 {Training error}=0.01328125\n",
            "2023-02-03 23:51:01,759 [nnabla][INFO]: iter=2889 {Training loss}=0.040856920182704926\n",
            "2023-02-03 23:51:01,759 [nnabla][INFO]: iter=2889 {Training error}=0.0125\n",
            "2023-02-03 23:51:01,793 [nnabla][INFO]: iter=2899 {Training loss}=0.03166128322482109\n",
            "2023-02-03 23:51:01,793 [nnabla][INFO]: iter=2899 {Training error}=0.009375\n",
            "2023-02-03 23:51:01,793 [nnabla][INFO]: iter=2899 {Training time}=0.3713839054107666[sec/100iter] 12.165639638900757[sec]\n",
            "2023-02-03 23:51:01,811 [nnabla][INFO]: iter=2900 {Test error}=0.0125\n",
            "2023-02-03 23:51:01,846 [nnabla][INFO]: iter=2909 {Training loss}=0.043077271431684494\n",
            "2023-02-03 23:51:01,847 [nnabla][INFO]: iter=2909 {Training error}=0.0125\n",
            "2023-02-03 23:51:01,883 [nnabla][INFO]: iter=2919 {Training loss}=0.03056635521352291\n",
            "2023-02-03 23:51:01,883 [nnabla][INFO]: iter=2919 {Training error}=0.01015625\n",
            "2023-02-03 23:51:01,920 [nnabla][INFO]: iter=2929 {Training loss}=0.03256712853908539\n",
            "2023-02-03 23:51:01,920 [nnabla][INFO]: iter=2929 {Training error}=0.0078125\n",
            "2023-02-03 23:51:01,955 [nnabla][INFO]: iter=2939 {Training loss}=0.01781727559864521\n",
            "2023-02-03 23:51:01,955 [nnabla][INFO]: iter=2939 {Training error}=0.00390625\n",
            "2023-02-03 23:51:01,990 [nnabla][INFO]: iter=2949 {Training loss}=0.05946866795420647\n",
            "2023-02-03 23:51:01,990 [nnabla][INFO]: iter=2949 {Training error}=0.0109375\n",
            "2023-02-03 23:51:02,027 [nnabla][INFO]: iter=2959 {Training loss}=0.031416572630405426\n",
            "2023-02-03 23:51:02,028 [nnabla][INFO]: iter=2959 {Training error}=0.00703125\n",
            "2023-02-03 23:51:02,062 [nnabla][INFO]: iter=2969 {Training loss}=0.039371050894260406\n",
            "2023-02-03 23:51:02,062 [nnabla][INFO]: iter=2969 {Training error}=0.01015625\n",
            "2023-02-03 23:51:02,099 [nnabla][INFO]: iter=2979 {Training loss}=0.022492514923214912\n",
            "2023-02-03 23:51:02,100 [nnabla][INFO]: iter=2979 {Training error}=0.00703125\n",
            "2023-02-03 23:51:02,135 [nnabla][INFO]: iter=2989 {Training loss}=0.035304807126522064\n",
            "2023-02-03 23:51:02,135 [nnabla][INFO]: iter=2989 {Training error}=0.015625\n",
            "2023-02-03 23:51:02,169 [nnabla][INFO]: iter=2999 {Training loss}=0.03230426460504532\n",
            "2023-02-03 23:51:02,169 [nnabla][INFO]: iter=2999 {Training error}=0.0078125\n",
            "2023-02-03 23:51:02,169 [nnabla][INFO]: iter=2999 {Training time}=0.3757896423339844[sec/100iter] 12.541429281234741[sec]\n",
            "2023-02-03 23:51:02,187 [nnabla][INFO]: iter=3000 {Test error}=0.00703125\n",
            "2023-02-03 23:51:02,207 [nnabla][INFO]: Solver state save (.h5): output/states_3000.h5\n",
            "2023-02-03 23:51:02,218 [nnabla][INFO]: Parameter save (.h5): output/params_3000.h5\n",
            "2023-02-03 23:51:02,218 [nnabla][INFO]: Checkpoint save (.json): output/checkpoint_3000.json\n",
            "2023-02-03 23:51:02,253 [nnabla][INFO]: iter=3009 {Training loss}=0.031794626265764236\n",
            "2023-02-03 23:51:02,253 [nnabla][INFO]: iter=3009 {Training error}=0.0109375\n",
            "2023-02-03 23:51:02,288 [nnabla][INFO]: iter=3019 {Training loss}=0.03194747865200043\n",
            "2023-02-03 23:51:02,288 [nnabla][INFO]: iter=3019 {Training error}=0.00859375\n",
            "2023-02-03 23:51:02,332 [nnabla][INFO]: iter=3029 {Training loss}=0.037529073655605316\n",
            "2023-02-03 23:51:02,332 [nnabla][INFO]: iter=3029 {Training error}=0.01015625\n",
            "2023-02-03 23:51:02,373 [nnabla][INFO]: iter=3039 {Training loss}=0.02671642042696476\n",
            "2023-02-03 23:51:02,373 [nnabla][INFO]: iter=3039 {Training error}=0.009375\n",
            "2023-02-03 23:51:02,409 [nnabla][INFO]: iter=3049 {Training loss}=0.040683869272470474\n",
            "2023-02-03 23:51:02,409 [nnabla][INFO]: iter=3049 {Training error}=0.01484375\n",
            "2023-02-03 23:51:02,442 [nnabla][INFO]: iter=3059 {Training loss}=0.0355096310377121\n",
            "2023-02-03 23:51:02,442 [nnabla][INFO]: iter=3059 {Training error}=0.01015625\n",
            "2023-02-03 23:51:02,476 [nnabla][INFO]: iter=3069 {Training loss}=0.03065810166299343\n",
            "2023-02-03 23:51:02,476 [nnabla][INFO]: iter=3069 {Training error}=0.00859375\n",
            "2023-02-03 23:51:02,512 [nnabla][INFO]: iter=3079 {Training loss}=0.03544741868972778\n",
            "2023-02-03 23:51:02,513 [nnabla][INFO]: iter=3079 {Training error}=0.0109375\n",
            "2023-02-03 23:51:02,547 [nnabla][INFO]: iter=3089 {Training loss}=0.03263115510344505\n",
            "2023-02-03 23:51:02,548 [nnabla][INFO]: iter=3089 {Training error}=0.0078125\n",
            "2023-02-03 23:51:02,581 [nnabla][INFO]: iter=3099 {Training loss}=0.036526452749967575\n",
            "2023-02-03 23:51:02,581 [nnabla][INFO]: iter=3099 {Training error}=0.0109375\n",
            "2023-02-03 23:51:02,582 [nnabla][INFO]: iter=3099 {Training time}=0.4125790596008301[sec/100iter] 12.954008340835571[sec]\n",
            "2023-02-03 23:51:02,599 [nnabla][INFO]: iter=3100 {Test error}=0.01171875\n",
            "2023-02-03 23:51:02,637 [nnabla][INFO]: iter=3109 {Training loss}=0.026576334610581398\n",
            "2023-02-03 23:51:02,637 [nnabla][INFO]: iter=3109 {Training error}=0.00703125\n",
            "2023-02-03 23:51:02,672 [nnabla][INFO]: iter=3119 {Training loss}=0.029279137030243874\n",
            "2023-02-03 23:51:02,672 [nnabla][INFO]: iter=3119 {Training error}=0.009375\n",
            "2023-02-03 23:51:02,707 [nnabla][INFO]: iter=3129 {Training loss}=0.030940264463424683\n",
            "2023-02-03 23:51:02,707 [nnabla][INFO]: iter=3129 {Training error}=0.009375\n",
            "2023-02-03 23:51:02,743 [nnabla][INFO]: iter=3139 {Training loss}=0.03573335334658623\n",
            "2023-02-03 23:51:02,743 [nnabla][INFO]: iter=3139 {Training error}=0.01484375\n",
            "2023-02-03 23:51:02,777 [nnabla][INFO]: iter=3149 {Training loss}=0.01567690819501877\n",
            "2023-02-03 23:51:02,777 [nnabla][INFO]: iter=3149 {Training error}=0.00390625\n",
            "2023-02-03 23:51:02,813 [nnabla][INFO]: iter=3159 {Training loss}=0.03282700479030609\n",
            "2023-02-03 23:51:02,813 [nnabla][INFO]: iter=3159 {Training error}=0.01015625\n",
            "2023-02-03 23:51:02,848 [nnabla][INFO]: iter=3169 {Training loss}=0.031928930431604385\n",
            "2023-02-03 23:51:02,848 [nnabla][INFO]: iter=3169 {Training error}=0.01015625\n",
            "2023-02-03 23:51:02,884 [nnabla][INFO]: iter=3179 {Training loss}=0.030699724331498146\n",
            "2023-02-03 23:51:02,884 [nnabla][INFO]: iter=3179 {Training error}=0.01171875\n",
            "2023-02-03 23:51:02,919 [nnabla][INFO]: iter=3189 {Training loss}=0.046805717051029205\n",
            "2023-02-03 23:51:02,919 [nnabla][INFO]: iter=3189 {Training error}=0.015625\n",
            "2023-02-03 23:51:02,963 [nnabla][INFO]: iter=3199 {Training loss}=0.03361944109201431\n",
            "2023-02-03 23:51:02,963 [nnabla][INFO]: iter=3199 {Training error}=0.0109375\n",
            "2023-02-03 23:51:02,963 [nnabla][INFO]: iter=3199 {Training time}=0.38158297538757324[sec/100iter] 13.335591316223145[sec]\n",
            "2023-02-03 23:51:02,980 [nnabla][INFO]: iter=3200 {Test error}=0.01171875\n",
            "2023-02-03 23:51:03,014 [nnabla][INFO]: iter=3209 {Training loss}=0.04359887167811394\n",
            "2023-02-03 23:51:03,015 [nnabla][INFO]: iter=3209 {Training error}=0.015625\n",
            "2023-02-03 23:51:03,049 [nnabla][INFO]: iter=3219 {Training loss}=0.03444372117519379\n",
            "2023-02-03 23:51:03,049 [nnabla][INFO]: iter=3219 {Training error}=0.01015625\n",
            "2023-02-03 23:51:03,083 [nnabla][INFO]: iter=3229 {Training loss}=0.04208546131849289\n",
            "2023-02-03 23:51:03,083 [nnabla][INFO]: iter=3229 {Training error}=0.0125\n",
            "2023-02-03 23:51:03,112 [nnabla][INFO]: iter=3239 {Training loss}=0.04286490008234978\n",
            "2023-02-03 23:51:03,112 [nnabla][INFO]: iter=3239 {Training error}=0.01328125\n",
            "2023-02-03 23:51:03,141 [nnabla][INFO]: iter=3249 {Training loss}=0.027671078220009804\n",
            "2023-02-03 23:51:03,141 [nnabla][INFO]: iter=3249 {Training error}=0.01015625\n",
            "2023-02-03 23:51:03,168 [nnabla][INFO]: iter=3259 {Training loss}=0.027375608682632446\n",
            "2023-02-03 23:51:03,168 [nnabla][INFO]: iter=3259 {Training error}=0.0078125\n",
            "2023-02-03 23:51:03,195 [nnabla][INFO]: iter=3269 {Training loss}=0.022309880703687668\n",
            "2023-02-03 23:51:03,195 [nnabla][INFO]: iter=3269 {Training error}=0.00625\n",
            "2023-02-03 23:51:03,221 [nnabla][INFO]: iter=3279 {Training loss}=0.03825255110859871\n",
            "2023-02-03 23:51:03,222 [nnabla][INFO]: iter=3279 {Training error}=0.01171875\n",
            "2023-02-03 23:51:03,251 [nnabla][INFO]: iter=3289 {Training loss}=0.03342181816697121\n",
            "2023-02-03 23:51:03,251 [nnabla][INFO]: iter=3289 {Training error}=0.009375\n",
            "2023-02-03 23:51:03,278 [nnabla][INFO]: iter=3299 {Training loss}=0.027282346040010452\n",
            "2023-02-03 23:51:03,278 [nnabla][INFO]: iter=3299 {Training error}=0.0109375\n",
            "2023-02-03 23:51:03,278 [nnabla][INFO]: iter=3299 {Training time}=0.3149564266204834[sec/100iter] 13.650547742843628[sec]\n",
            "2023-02-03 23:51:03,289 [nnabla][INFO]: iter=3300 {Test error}=0.009375\n",
            "2023-02-03 23:51:03,316 [nnabla][INFO]: iter=3309 {Training loss}=0.024067118763923645\n",
            "2023-02-03 23:51:03,316 [nnabla][INFO]: iter=3309 {Training error}=0.00625\n",
            "2023-02-03 23:51:03,343 [nnabla][INFO]: iter=3319 {Training loss}=0.01753869839012623\n",
            "2023-02-03 23:51:03,344 [nnabla][INFO]: iter=3319 {Training error}=0.00703125\n",
            "2023-02-03 23:51:03,370 [nnabla][INFO]: iter=3329 {Training loss}=0.05003747344017029\n",
            "2023-02-03 23:51:03,370 [nnabla][INFO]: iter=3329 {Training error}=0.01171875\n",
            "2023-02-03 23:51:03,401 [nnabla][INFO]: iter=3339 {Training loss}=0.02027386799454689\n",
            "2023-02-03 23:51:03,401 [nnabla][INFO]: iter=3339 {Training error}=0.00546875\n",
            "2023-02-03 23:51:03,428 [nnabla][INFO]: iter=3349 {Training loss}=0.020645666867494583\n",
            "2023-02-03 23:51:03,429 [nnabla][INFO]: iter=3349 {Training error}=0.00625\n",
            "2023-02-03 23:51:03,456 [nnabla][INFO]: iter=3359 {Training loss}=0.028057539835572243\n",
            "2023-02-03 23:51:03,456 [nnabla][INFO]: iter=3359 {Training error}=0.009375\n",
            "2023-02-03 23:51:03,483 [nnabla][INFO]: iter=3369 {Training loss}=0.03869184851646423\n",
            "2023-02-03 23:51:03,483 [nnabla][INFO]: iter=3369 {Training error}=0.0125\n",
            "2023-02-03 23:51:03,510 [nnabla][INFO]: iter=3379 {Training loss}=0.01831548660993576\n",
            "2023-02-03 23:51:03,510 [nnabla][INFO]: iter=3379 {Training error}=0.00703125\n",
            "2023-02-03 23:51:03,543 [nnabla][INFO]: iter=3389 {Training loss}=0.02742069400846958\n",
            "2023-02-03 23:51:03,543 [nnabla][INFO]: iter=3389 {Training error}=0.00546875\n",
            "2023-02-03 23:51:03,571 [nnabla][INFO]: iter=3399 {Training loss}=0.021281057968735695\n",
            "2023-02-03 23:51:03,571 [nnabla][INFO]: iter=3399 {Training error}=0.009375\n",
            "2023-02-03 23:51:03,571 [nnabla][INFO]: iter=3399 {Training time}=0.2929255962371826[sec/100iter] 13.94347333908081[sec]\n",
            "2023-02-03 23:51:03,582 [nnabla][INFO]: iter=3400 {Test error}=0.01171875\n",
            "2023-02-03 23:51:03,608 [nnabla][INFO]: iter=3409 {Training loss}=0.036624133586883545\n",
            "2023-02-03 23:51:03,608 [nnabla][INFO]: iter=3409 {Training error}=0.01015625\n",
            "2023-02-03 23:51:03,635 [nnabla][INFO]: iter=3419 {Training loss}=0.03343293070793152\n",
            "2023-02-03 23:51:03,636 [nnabla][INFO]: iter=3419 {Training error}=0.01328125\n",
            "2023-02-03 23:51:03,662 [nnabla][INFO]: iter=3429 {Training loss}=0.037005599588155746\n",
            "2023-02-03 23:51:03,663 [nnabla][INFO]: iter=3429 {Training error}=0.0125\n",
            "2023-02-03 23:51:03,691 [nnabla][INFO]: iter=3439 {Training loss}=0.026576409116387367\n",
            "2023-02-03 23:51:03,691 [nnabla][INFO]: iter=3439 {Training error}=0.0109375\n",
            "2023-02-03 23:51:03,718 [nnabla][INFO]: iter=3449 {Training loss}=0.02428225241601467\n",
            "2023-02-03 23:51:03,718 [nnabla][INFO]: iter=3449 {Training error}=0.0046875\n",
            "2023-02-03 23:51:03,746 [nnabla][INFO]: iter=3459 {Training loss}=0.03878609463572502\n",
            "2023-02-03 23:51:03,746 [nnabla][INFO]: iter=3459 {Training error}=0.009375\n",
            "2023-02-03 23:51:03,773 [nnabla][INFO]: iter=3469 {Training loss}=0.043587569147348404\n",
            "2023-02-03 23:51:03,773 [nnabla][INFO]: iter=3469 {Training error}=0.01328125\n",
            "2023-02-03 23:51:03,800 [nnabla][INFO]: iter=3479 {Training loss}=0.026817282661795616\n",
            "2023-02-03 23:51:03,800 [nnabla][INFO]: iter=3479 {Training error}=0.0078125\n",
            "2023-02-03 23:51:03,829 [nnabla][INFO]: iter=3489 {Training loss}=0.025845732539892197\n",
            "2023-02-03 23:51:03,829 [nnabla][INFO]: iter=3489 {Training error}=0.0078125\n",
            "2023-02-03 23:51:03,856 [nnabla][INFO]: iter=3499 {Training loss}=0.02586938999593258\n",
            "2023-02-03 23:51:03,856 [nnabla][INFO]: iter=3499 {Training error}=0.00703125\n",
            "2023-02-03 23:51:03,857 [nnabla][INFO]: iter=3499 {Training time}=0.28565001487731934[sec/100iter] 14.22912335395813[sec]\n",
            "2023-02-03 23:51:03,867 [nnabla][INFO]: iter=3500 {Test error}=0.01875\n",
            "2023-02-03 23:51:03,894 [nnabla][INFO]: iter=3509 {Training loss}=0.039663638919591904\n",
            "2023-02-03 23:51:03,894 [nnabla][INFO]: iter=3509 {Training error}=0.01171875\n",
            "2023-02-03 23:51:03,921 [nnabla][INFO]: iter=3519 {Training loss}=0.033913224935531616\n",
            "2023-02-03 23:51:03,922 [nnabla][INFO]: iter=3519 {Training error}=0.0125\n",
            "2023-02-03 23:51:03,948 [nnabla][INFO]: iter=3529 {Training loss}=0.051153600215911865\n",
            "2023-02-03 23:51:03,949 [nnabla][INFO]: iter=3529 {Training error}=0.015625\n",
            "2023-02-03 23:51:03,976 [nnabla][INFO]: iter=3539 {Training loss}=0.02324637770652771\n",
            "2023-02-03 23:51:03,976 [nnabla][INFO]: iter=3539 {Training error}=0.00859375\n",
            "2023-02-03 23:51:04,003 [nnabla][INFO]: iter=3549 {Training loss}=0.019848214462399483\n",
            "2023-02-03 23:51:04,003 [nnabla][INFO]: iter=3549 {Training error}=0.00546875\n",
            "2023-02-03 23:51:04,031 [nnabla][INFO]: iter=3559 {Training loss}=0.02450469136238098\n",
            "2023-02-03 23:51:04,031 [nnabla][INFO]: iter=3559 {Training error}=0.00703125\n",
            "2023-02-03 23:51:04,061 [nnabla][INFO]: iter=3569 {Training loss}=0.03118819370865822\n",
            "2023-02-03 23:51:04,061 [nnabla][INFO]: iter=3569 {Training error}=0.009375\n",
            "2023-02-03 23:51:04,090 [nnabla][INFO]: iter=3579 {Training loss}=0.04041442275047302\n",
            "2023-02-03 23:51:04,090 [nnabla][INFO]: iter=3579 {Training error}=0.015625\n",
            "2023-02-03 23:51:04,116 [nnabla][INFO]: iter=3589 {Training loss}=0.03185165673494339\n",
            "2023-02-03 23:51:04,117 [nnabla][INFO]: iter=3589 {Training error}=0.01015625\n",
            "2023-02-03 23:51:04,143 [nnabla][INFO]: iter=3599 {Training loss}=0.026180187240242958\n",
            "2023-02-03 23:51:04,143 [nnabla][INFO]: iter=3599 {Training error}=0.009375\n",
            "2023-02-03 23:51:04,143 [nnabla][INFO]: iter=3599 {Training time}=0.28672194480895996[sec/100iter] 14.51584529876709[sec]\n",
            "2023-02-03 23:51:04,154 [nnabla][INFO]: iter=3600 {Test error}=0.015625\n",
            "2023-02-03 23:51:04,180 [nnabla][INFO]: iter=3609 {Training loss}=0.03594310209155083\n",
            "2023-02-03 23:51:04,180 [nnabla][INFO]: iter=3609 {Training error}=0.01328125\n",
            "2023-02-03 23:51:04,207 [nnabla][INFO]: iter=3619 {Training loss}=0.054004788398742676\n",
            "2023-02-03 23:51:04,207 [nnabla][INFO]: iter=3619 {Training error}=0.01953125\n",
            "2023-02-03 23:51:04,233 [nnabla][INFO]: iter=3629 {Training loss}=0.03945709019899368\n",
            "2023-02-03 23:51:04,233 [nnabla][INFO]: iter=3629 {Training error}=0.01484375\n",
            "2023-02-03 23:51:04,260 [nnabla][INFO]: iter=3639 {Training loss}=0.023731958121061325\n",
            "2023-02-03 23:51:04,260 [nnabla][INFO]: iter=3639 {Training error}=0.0078125\n",
            "2023-02-03 23:51:04,287 [nnabla][INFO]: iter=3649 {Training loss}=0.017620492726564407\n",
            "2023-02-03 23:51:04,287 [nnabla][INFO]: iter=3649 {Training error}=0.0078125\n",
            "2023-02-03 23:51:04,314 [nnabla][INFO]: iter=3659 {Training loss}=0.018743671476840973\n",
            "2023-02-03 23:51:04,314 [nnabla][INFO]: iter=3659 {Training error}=0.00625\n",
            "2023-02-03 23:51:04,343 [nnabla][INFO]: iter=3669 {Training loss}=0.02427026256918907\n",
            "2023-02-03 23:51:04,343 [nnabla][INFO]: iter=3669 {Training error}=0.009375\n",
            "2023-02-03 23:51:04,370 [nnabla][INFO]: iter=3679 {Training loss}=0.045486535876989365\n",
            "2023-02-03 23:51:04,370 [nnabla][INFO]: iter=3679 {Training error}=0.01015625\n",
            "2023-02-03 23:51:04,397 [nnabla][INFO]: iter=3689 {Training loss}=0.017679834738373756\n",
            "2023-02-03 23:51:04,397 [nnabla][INFO]: iter=3689 {Training error}=0.00546875\n",
            "2023-02-03 23:51:04,427 [nnabla][INFO]: iter=3699 {Training loss}=0.028700539842247963\n",
            "2023-02-03 23:51:04,427 [nnabla][INFO]: iter=3699 {Training error}=0.00859375\n",
            "2023-02-03 23:51:04,427 [nnabla][INFO]: iter=3699 {Training time}=0.2841000556945801[sec/100iter] 14.79994535446167[sec]\n",
            "2023-02-03 23:51:04,438 [nnabla][INFO]: iter=3700 {Test error}=0.009375\n",
            "2023-02-03 23:51:04,465 [nnabla][INFO]: iter=3709 {Training loss}=0.025523746386170387\n",
            "2023-02-03 23:51:04,465 [nnabla][INFO]: iter=3709 {Training error}=0.009375\n",
            "2023-02-03 23:51:04,492 [nnabla][INFO]: iter=3719 {Training loss}=0.025112595409154892\n",
            "2023-02-03 23:51:04,492 [nnabla][INFO]: iter=3719 {Training error}=0.009375\n",
            "2023-02-03 23:51:04,519 [nnabla][INFO]: iter=3729 {Training loss}=0.021259276196360588\n",
            "2023-02-03 23:51:04,519 [nnabla][INFO]: iter=3729 {Training error}=0.00703125\n",
            "2023-02-03 23:51:04,547 [nnabla][INFO]: iter=3739 {Training loss}=0.021823551505804062\n",
            "2023-02-03 23:51:04,547 [nnabla][INFO]: iter=3739 {Training error}=0.00625\n",
            "2023-02-03 23:51:04,577 [nnabla][INFO]: iter=3749 {Training loss}=0.03376654535531998\n",
            "2023-02-03 23:51:04,577 [nnabla][INFO]: iter=3749 {Training error}=0.01015625\n",
            "2023-02-03 23:51:04,603 [nnabla][INFO]: iter=3759 {Training loss}=0.023809371516108513\n",
            "2023-02-03 23:51:04,603 [nnabla][INFO]: iter=3759 {Training error}=0.0046875\n",
            "2023-02-03 23:51:04,630 [nnabla][INFO]: iter=3769 {Training loss}=0.025909867137670517\n",
            "2023-02-03 23:51:04,630 [nnabla][INFO]: iter=3769 {Training error}=0.0078125\n",
            "2023-02-03 23:51:04,658 [nnabla][INFO]: iter=3779 {Training loss}=0.023167196661233902\n",
            "2023-02-03 23:51:04,658 [nnabla][INFO]: iter=3779 {Training error}=0.0078125\n",
            "2023-02-03 23:51:04,685 [nnabla][INFO]: iter=3789 {Training loss}=0.030541205778717995\n",
            "2023-02-03 23:51:04,685 [nnabla][INFO]: iter=3789 {Training error}=0.01015625\n",
            "2023-02-03 23:51:04,712 [nnabla][INFO]: iter=3799 {Training loss}=0.017985688522458076\n",
            "2023-02-03 23:51:04,712 [nnabla][INFO]: iter=3799 {Training error}=0.00625\n",
            "2023-02-03 23:51:04,712 [nnabla][INFO]: iter=3799 {Training time}=0.28484058380126953[sec/100iter] 15.08478593826294[sec]\n",
            "2023-02-03 23:51:04,723 [nnabla][INFO]: iter=3800 {Test error}=0.0125\n",
            "2023-02-03 23:51:04,750 [nnabla][INFO]: iter=3809 {Training loss}=0.019200531765818596\n",
            "2023-02-03 23:51:04,751 [nnabla][INFO]: iter=3809 {Training error}=0.00546875\n",
            "2023-02-03 23:51:04,777 [nnabla][INFO]: iter=3819 {Training loss}=0.020996470004320145\n",
            "2023-02-03 23:51:04,777 [nnabla][INFO]: iter=3819 {Training error}=0.00703125\n",
            "2023-02-03 23:51:04,805 [nnabla][INFO]: iter=3829 {Training loss}=0.028303563594818115\n",
            "2023-02-03 23:51:04,805 [nnabla][INFO]: iter=3829 {Training error}=0.009375\n",
            "2023-02-03 23:51:04,833 [nnabla][INFO]: iter=3839 {Training loss}=0.03574124723672867\n",
            "2023-02-03 23:51:04,833 [nnabla][INFO]: iter=3839 {Training error}=0.01328125\n",
            "2023-02-03 23:51:04,863 [nnabla][INFO]: iter=3849 {Training loss}=0.023693885654211044\n",
            "2023-02-03 23:51:04,863 [nnabla][INFO]: iter=3849 {Training error}=0.00703125\n",
            "2023-02-03 23:51:04,895 [nnabla][INFO]: iter=3859 {Training loss}=0.018139734864234924\n",
            "2023-02-03 23:51:04,895 [nnabla][INFO]: iter=3859 {Training error}=0.00703125\n",
            "2023-02-03 23:51:04,924 [nnabla][INFO]: iter=3869 {Training loss}=0.042556483298540115\n",
            "2023-02-03 23:51:04,924 [nnabla][INFO]: iter=3869 {Training error}=0.01328125\n",
            "2023-02-03 23:51:04,953 [nnabla][INFO]: iter=3879 {Training loss}=0.04305608570575714\n",
            "2023-02-03 23:51:04,954 [nnabla][INFO]: iter=3879 {Training error}=0.0125\n",
            "2023-02-03 23:51:04,982 [nnabla][INFO]: iter=3889 {Training loss}=0.04844580590724945\n",
            "2023-02-03 23:51:04,982 [nnabla][INFO]: iter=3889 {Training error}=0.00859375\n",
            "2023-02-03 23:51:05,011 [nnabla][INFO]: iter=3899 {Training loss}=0.035747677087783813\n",
            "2023-02-03 23:51:05,011 [nnabla][INFO]: iter=3899 {Training error}=0.0109375\n",
            "2023-02-03 23:51:05,011 [nnabla][INFO]: iter=3899 {Training time}=0.2986025810241699[sec/100iter] 15.38338851928711[sec]\n",
            "2023-02-03 23:51:05,023 [nnabla][INFO]: iter=3900 {Test error}=0.01328125\n",
            "2023-02-03 23:51:05,051 [nnabla][INFO]: iter=3909 {Training loss}=0.018498336896300316\n",
            "2023-02-03 23:51:05,051 [nnabla][INFO]: iter=3909 {Training error}=0.0046875\n",
            "2023-02-03 23:51:05,078 [nnabla][INFO]: iter=3919 {Training loss}=0.026203662157058716\n",
            "2023-02-03 23:51:05,078 [nnabla][INFO]: iter=3919 {Training error}=0.00859375\n",
            "2023-02-03 23:51:05,106 [nnabla][INFO]: iter=3929 {Training loss}=0.026853585615754128\n",
            "2023-02-03 23:51:05,106 [nnabla][INFO]: iter=3929 {Training error}=0.00703125\n",
            "2023-02-03 23:51:05,133 [nnabla][INFO]: iter=3939 {Training loss}=0.032848261296749115\n",
            "2023-02-03 23:51:05,133 [nnabla][INFO]: iter=3939 {Training error}=0.0109375\n",
            "2023-02-03 23:51:05,161 [nnabla][INFO]: iter=3949 {Training loss}=0.025676075369119644\n",
            "2023-02-03 23:51:05,161 [nnabla][INFO]: iter=3949 {Training error}=0.0078125\n",
            "2023-02-03 23:51:05,187 [nnabla][INFO]: iter=3959 {Training loss}=0.02250947430729866\n",
            "2023-02-03 23:51:05,188 [nnabla][INFO]: iter=3959 {Training error}=0.00703125\n",
            "2023-02-03 23:51:05,214 [nnabla][INFO]: iter=3969 {Training loss}=0.015652697533369064\n",
            "2023-02-03 23:51:05,215 [nnabla][INFO]: iter=3969 {Training error}=0.00546875\n",
            "2023-02-03 23:51:05,241 [nnabla][INFO]: iter=3979 {Training loss}=0.012894481420516968\n",
            "2023-02-03 23:51:05,241 [nnabla][INFO]: iter=3979 {Training error}=0.003125\n",
            "2023-02-03 23:51:05,268 [nnabla][INFO]: iter=3989 {Training loss}=0.026700541377067566\n",
            "2023-02-03 23:51:05,268 [nnabla][INFO]: iter=3989 {Training error}=0.00859375\n",
            "2023-02-03 23:51:05,295 [nnabla][INFO]: iter=3999 {Training loss}=0.025909248739480972\n",
            "2023-02-03 23:51:05,295 [nnabla][INFO]: iter=3999 {Training error}=0.00859375\n",
            "2023-02-03 23:51:05,295 [nnabla][INFO]: iter=3999 {Training time}=0.28418731689453125[sec/100iter] 15.66757583618164[sec]\n",
            "2023-02-03 23:51:05,306 [nnabla][INFO]: iter=4000 {Test error}=0.00703125\n",
            "2023-02-03 23:51:05,319 [nnabla][INFO]: Solver state save (.h5): output/states_4000.h5\n",
            "2023-02-03 23:51:05,327 [nnabla][INFO]: Parameter save (.h5): output/params_4000.h5\n",
            "2023-02-03 23:51:05,327 [nnabla][INFO]: Checkpoint save (.json): output/checkpoint_4000.json\n",
            "2023-02-03 23:51:05,354 [nnabla][INFO]: iter=4009 {Training loss}=0.02992822229862213\n",
            "2023-02-03 23:51:05,354 [nnabla][INFO]: iter=4009 {Training error}=0.00859375\n",
            "2023-02-03 23:51:05,380 [nnabla][INFO]: iter=4019 {Training loss}=0.020201940089464188\n",
            "2023-02-03 23:51:05,381 [nnabla][INFO]: iter=4019 {Training error}=0.00390625\n",
            "2023-02-03 23:51:05,409 [nnabla][INFO]: iter=4029 {Training loss}=0.03107033297419548\n",
            "2023-02-03 23:51:05,409 [nnabla][INFO]: iter=4029 {Training error}=0.009375\n",
            "2023-02-03 23:51:05,440 [nnabla][INFO]: iter=4039 {Training loss}=0.024962367489933968\n",
            "2023-02-03 23:51:05,441 [nnabla][INFO]: iter=4039 {Training error}=0.00859375\n",
            "2023-02-03 23:51:05,468 [nnabla][INFO]: iter=4049 {Training loss}=0.022626105695962906\n",
            "2023-02-03 23:51:05,468 [nnabla][INFO]: iter=4049 {Training error}=0.009375\n",
            "2023-02-03 23:51:05,495 [nnabla][INFO]: iter=4059 {Training loss}=0.01891849935054779\n",
            "2023-02-03 23:51:05,495 [nnabla][INFO]: iter=4059 {Training error}=0.00625\n",
            "2023-02-03 23:51:05,522 [nnabla][INFO]: iter=4069 {Training loss}=0.023339631035923958\n",
            "2023-02-03 23:51:05,522 [nnabla][INFO]: iter=4069 {Training error}=0.00859375\n",
            "2023-02-03 23:51:05,549 [nnabla][INFO]: iter=4079 {Training loss}=0.028612777590751648\n",
            "2023-02-03 23:51:05,549 [nnabla][INFO]: iter=4079 {Training error}=0.00859375\n",
            "2023-02-03 23:51:05,577 [nnabla][INFO]: iter=4089 {Training loss}=0.030299818143248558\n",
            "2023-02-03 23:51:05,578 [nnabla][INFO]: iter=4089 {Training error}=0.00859375\n",
            "2023-02-03 23:51:05,604 [nnabla][INFO]: iter=4099 {Training loss}=0.02570134960114956\n",
            "2023-02-03 23:51:05,604 [nnabla][INFO]: iter=4099 {Training error}=0.00703125\n",
            "2023-02-03 23:51:05,604 [nnabla][INFO]: iter=4099 {Training time}=0.30933523178100586[sec/100iter] 15.976911067962646[sec]\n",
            "2023-02-03 23:51:05,615 [nnabla][INFO]: iter=4100 {Test error}=0.0109375\n",
            "2023-02-03 23:51:05,642 [nnabla][INFO]: iter=4109 {Training loss}=0.03139674663543701\n",
            "2023-02-03 23:51:05,642 [nnabla][INFO]: iter=4109 {Training error}=0.01015625\n",
            "2023-02-03 23:51:05,668 [nnabla][INFO]: iter=4119 {Training loss}=0.02138346992433071\n",
            "2023-02-03 23:51:05,669 [nnabla][INFO]: iter=4119 {Training error}=0.00546875\n",
            "2023-02-03 23:51:05,695 [nnabla][INFO]: iter=4129 {Training loss}=0.02736864611506462\n",
            "2023-02-03 23:51:05,696 [nnabla][INFO]: iter=4129 {Training error}=0.01015625\n",
            "2023-02-03 23:51:05,722 [nnabla][INFO]: iter=4139 {Training loss}=0.041453562676906586\n",
            "2023-02-03 23:51:05,723 [nnabla][INFO]: iter=4139 {Training error}=0.01328125\n",
            "2023-02-03 23:51:05,751 [nnabla][INFO]: iter=4149 {Training loss}=0.03429693356156349\n",
            "2023-02-03 23:51:05,751 [nnabla][INFO]: iter=4149 {Training error}=0.0125\n",
            "2023-02-03 23:51:05,780 [nnabla][INFO]: iter=4159 {Training loss}=0.02831518091261387\n",
            "2023-02-03 23:51:05,780 [nnabla][INFO]: iter=4159 {Training error}=0.009375\n",
            "2023-02-03 23:51:05,807 [nnabla][INFO]: iter=4169 {Training loss}=0.03382245451211929\n",
            "2023-02-03 23:51:05,807 [nnabla][INFO]: iter=4169 {Training error}=0.00703125\n",
            "2023-02-03 23:51:05,834 [nnabla][INFO]: iter=4179 {Training loss}=0.03128361701965332\n",
            "2023-02-03 23:51:05,834 [nnabla][INFO]: iter=4179 {Training error}=0.009375\n",
            "2023-02-03 23:51:05,861 [nnabla][INFO]: iter=4189 {Training loss}=0.023224670439958572\n",
            "2023-02-03 23:51:05,861 [nnabla][INFO]: iter=4189 {Training error}=0.00859375\n",
            "2023-02-03 23:51:05,888 [nnabla][INFO]: iter=4199 {Training loss}=0.025012442842125893\n",
            "2023-02-03 23:51:05,888 [nnabla][INFO]: iter=4199 {Training error}=0.0078125\n",
            "2023-02-03 23:51:05,888 [nnabla][INFO]: iter=4199 {Training time}=0.28356504440307617[sec/100iter] 16.260476112365723[sec]\n",
            "2023-02-03 23:51:05,899 [nnabla][INFO]: iter=4200 {Test error}=0.01171875\n",
            "2023-02-03 23:51:05,926 [nnabla][INFO]: iter=4209 {Training loss}=0.020521238446235657\n",
            "2023-02-03 23:51:05,926 [nnabla][INFO]: iter=4209 {Training error}=0.0046875\n",
            "2023-02-03 23:51:05,954 [nnabla][INFO]: iter=4219 {Training loss}=0.024731164798140526\n",
            "2023-02-03 23:51:05,954 [nnabla][INFO]: iter=4219 {Training error}=0.00859375\n",
            "2023-02-03 23:51:05,980 [nnabla][INFO]: iter=4229 {Training loss}=0.024674778804183006\n",
            "2023-02-03 23:51:05,980 [nnabla][INFO]: iter=4229 {Training error}=0.00703125\n",
            "2023-02-03 23:51:06,007 [nnabla][INFO]: iter=4239 {Training loss}=0.025116756558418274\n",
            "2023-02-03 23:51:06,007 [nnabla][INFO]: iter=4239 {Training error}=0.00859375\n",
            "2023-02-03 23:51:06,034 [nnabla][INFO]: iter=4249 {Training loss}=0.01549266092479229\n",
            "2023-02-03 23:51:06,034 [nnabla][INFO]: iter=4249 {Training error}=0.00546875\n",
            "2023-02-03 23:51:06,063 [nnabla][INFO]: iter=4259 {Training loss}=0.025007927790284157\n",
            "2023-02-03 23:51:06,063 [nnabla][INFO]: iter=4259 {Training error}=0.00859375\n",
            "2023-02-03 23:51:06,090 [nnabla][INFO]: iter=4269 {Training loss}=0.03138069063425064\n",
            "2023-02-03 23:51:06,090 [nnabla][INFO]: iter=4269 {Training error}=0.00546875\n",
            "2023-02-03 23:51:06,117 [nnabla][INFO]: iter=4279 {Training loss}=0.022197755053639412\n",
            "2023-02-03 23:51:06,117 [nnabla][INFO]: iter=4279 {Training error}=0.0078125\n",
            "2023-02-03 23:51:06,145 [nnabla][INFO]: iter=4289 {Training loss}=0.018687855452299118\n",
            "2023-02-03 23:51:06,146 [nnabla][INFO]: iter=4289 {Training error}=0.00703125\n",
            "2023-02-03 23:51:06,173 [nnabla][INFO]: iter=4299 {Training loss}=0.0262804813683033\n",
            "2023-02-03 23:51:06,173 [nnabla][INFO]: iter=4299 {Training error}=0.0109375\n",
            "2023-02-03 23:51:06,173 [nnabla][INFO]: iter=4299 {Training time}=0.2852592468261719[sec/100iter] 16.545735359191895[sec]\n",
            "2023-02-03 23:51:06,184 [nnabla][INFO]: iter=4300 {Test error}=0.00859375\n",
            "2023-02-03 23:51:06,211 [nnabla][INFO]: iter=4309 {Training loss}=0.038598429411649704\n",
            "2023-02-03 23:51:06,211 [nnabla][INFO]: iter=4309 {Training error}=0.0140625\n",
            "2023-02-03 23:51:06,238 [nnabla][INFO]: iter=4319 {Training loss}=0.01733238250017166\n",
            "2023-02-03 23:51:06,238 [nnabla][INFO]: iter=4319 {Training error}=0.00859375\n",
            "2023-02-03 23:51:06,265 [nnabla][INFO]: iter=4329 {Training loss}=0.019540827721357346\n",
            "2023-02-03 23:51:06,265 [nnabla][INFO]: iter=4329 {Training error}=0.0078125\n",
            "2023-02-03 23:51:06,292 [nnabla][INFO]: iter=4339 {Training loss}=0.018528902903199196\n",
            "2023-02-03 23:51:06,292 [nnabla][INFO]: iter=4339 {Training error}=0.00703125\n",
            "2023-02-03 23:51:06,319 [nnabla][INFO]: iter=4349 {Training loss}=0.024226048961281776\n",
            "2023-02-03 23:51:06,319 [nnabla][INFO]: iter=4349 {Training error}=0.00703125\n",
            "2023-02-03 23:51:06,348 [nnabla][INFO]: iter=4359 {Training loss}=0.01901867985725403\n",
            "2023-02-03 23:51:06,348 [nnabla][INFO]: iter=4359 {Training error}=0.00546875\n",
            "2023-02-03 23:51:06,375 [nnabla][INFO]: iter=4369 {Training loss}=0.019444327801465988\n",
            "2023-02-03 23:51:06,375 [nnabla][INFO]: iter=4369 {Training error}=0.00703125\n",
            "2023-02-03 23:51:06,402 [nnabla][INFO]: iter=4379 {Training loss}=0.013527980074286461\n",
            "2023-02-03 23:51:06,402 [nnabla][INFO]: iter=4379 {Training error}=0.00390625\n",
            "2023-02-03 23:51:06,429 [nnabla][INFO]: iter=4389 {Training loss}=0.019777417182922363\n",
            "2023-02-03 23:51:06,429 [nnabla][INFO]: iter=4389 {Training error}=0.00703125\n",
            "2023-02-03 23:51:06,459 [nnabla][INFO]: iter=4399 {Training loss}=0.020487941801548004\n",
            "2023-02-03 23:51:06,460 [nnabla][INFO]: iter=4399 {Training error}=0.0046875\n",
            "2023-02-03 23:51:06,460 [nnabla][INFO]: iter=4399 {Training time}=0.28638458251953125[sec/100iter] 16.832119941711426[sec]\n",
            "2023-02-03 23:51:06,471 [nnabla][INFO]: iter=4400 {Test error}=0.01171875\n",
            "2023-02-03 23:51:06,497 [nnabla][INFO]: iter=4409 {Training loss}=0.019649092108011246\n",
            "2023-02-03 23:51:06,498 [nnabla][INFO]: iter=4409 {Training error}=0.0046875\n",
            "2023-02-03 23:51:06,527 [nnabla][INFO]: iter=4419 {Training loss}=0.01863747462630272\n",
            "2023-02-03 23:51:06,527 [nnabla][INFO]: iter=4419 {Training error}=0.00546875\n",
            "2023-02-03 23:51:06,554 [nnabla][INFO]: iter=4429 {Training loss}=0.016070958226919174\n",
            "2023-02-03 23:51:06,554 [nnabla][INFO]: iter=4429 {Training error}=0.00625\n",
            "2023-02-03 23:51:06,582 [nnabla][INFO]: iter=4439 {Training loss}=0.013457225635647774\n",
            "2023-02-03 23:51:06,583 [nnabla][INFO]: iter=4439 {Training error}=0.003125\n",
            "2023-02-03 23:51:06,609 [nnabla][INFO]: iter=4449 {Training loss}=0.0365251824259758\n",
            "2023-02-03 23:51:06,609 [nnabla][INFO]: iter=4449 {Training error}=0.009375\n",
            "2023-02-03 23:51:06,636 [nnabla][INFO]: iter=4459 {Training loss}=0.03346969932317734\n",
            "2023-02-03 23:51:06,636 [nnabla][INFO]: iter=4459 {Training error}=0.009375\n",
            "2023-02-03 23:51:06,664 [nnabla][INFO]: iter=4469 {Training loss}=0.030346736311912537\n",
            "2023-02-03 23:51:06,665 [nnabla][INFO]: iter=4469 {Training error}=0.0078125\n",
            "2023-02-03 23:51:06,691 [nnabla][INFO]: iter=4479 {Training loss}=0.031176388263702393\n",
            "2023-02-03 23:51:06,692 [nnabla][INFO]: iter=4479 {Training error}=0.00859375\n",
            "2023-02-03 23:51:06,719 [nnabla][INFO]: iter=4489 {Training loss}=0.029049813747406006\n",
            "2023-02-03 23:51:06,719 [nnabla][INFO]: iter=4489 {Training error}=0.00859375\n",
            "2023-02-03 23:51:06,747 [nnabla][INFO]: iter=4499 {Training loss}=0.02717476710677147\n",
            "2023-02-03 23:51:06,747 [nnabla][INFO]: iter=4499 {Training error}=0.00546875\n",
            "2023-02-03 23:51:06,747 [nnabla][INFO]: iter=4499 {Training time}=0.2874259948730469[sec/100iter] 17.119545936584473[sec]\n",
            "2023-02-03 23:51:06,758 [nnabla][INFO]: iter=4500 {Test error}=0.01171875\n",
            "2023-02-03 23:51:06,784 [nnabla][INFO]: iter=4509 {Training loss}=0.030305420979857445\n",
            "2023-02-03 23:51:06,784 [nnabla][INFO]: iter=4509 {Training error}=0.0125\n",
            "2023-02-03 23:51:06,811 [nnabla][INFO]: iter=4519 {Training loss}=0.028995823115110397\n",
            "2023-02-03 23:51:06,811 [nnabla][INFO]: iter=4519 {Training error}=0.00703125\n",
            "2023-02-03 23:51:06,840 [nnabla][INFO]: iter=4529 {Training loss}=0.015306249260902405\n",
            "2023-02-03 23:51:06,840 [nnabla][INFO]: iter=4529 {Training error}=0.00546875\n",
            "2023-02-03 23:51:06,867 [nnabla][INFO]: iter=4539 {Training loss}=0.031156575307250023\n",
            "2023-02-03 23:51:06,867 [nnabla][INFO]: iter=4539 {Training error}=0.00859375\n",
            "2023-02-03 23:51:06,894 [nnabla][INFO]: iter=4549 {Training loss}=0.04134072735905647\n",
            "2023-02-03 23:51:06,894 [nnabla][INFO]: iter=4549 {Training error}=0.0078125\n",
            "2023-02-03 23:51:06,921 [nnabla][INFO]: iter=4559 {Training loss}=0.01841668039560318\n",
            "2023-02-03 23:51:06,921 [nnabla][INFO]: iter=4559 {Training error}=0.00546875\n",
            "2023-02-03 23:51:06,950 [nnabla][INFO]: iter=4569 {Training loss}=0.028631772845983505\n",
            "2023-02-03 23:51:06,950 [nnabla][INFO]: iter=4569 {Training error}=0.0078125\n",
            "2023-02-03 23:51:06,977 [nnabla][INFO]: iter=4579 {Training loss}=0.02651754952967167\n",
            "2023-02-03 23:51:06,978 [nnabla][INFO]: iter=4579 {Training error}=0.00859375\n",
            "2023-02-03 23:51:07,004 [nnabla][INFO]: iter=4589 {Training loss}=0.03451569378376007\n",
            "2023-02-03 23:51:07,004 [nnabla][INFO]: iter=4589 {Training error}=0.009375\n",
            "2023-02-03 23:51:07,031 [nnabla][INFO]: iter=4599 {Training loss}=0.02720627747476101\n",
            "2023-02-03 23:51:07,031 [nnabla][INFO]: iter=4599 {Training error}=0.00703125\n",
            "2023-02-03 23:51:07,031 [nnabla][INFO]: iter=4599 {Training time}=0.2841298580169678[sec/100iter] 17.40367579460144[sec]\n",
            "2023-02-03 23:51:07,042 [nnabla][INFO]: iter=4600 {Test error}=0.0125\n",
            "2023-02-03 23:51:07,069 [nnabla][INFO]: iter=4609 {Training loss}=0.024348245933651924\n",
            "2023-02-03 23:51:07,070 [nnabla][INFO]: iter=4609 {Training error}=0.009375\n",
            "2023-02-03 23:51:07,097 [nnabla][INFO]: iter=4619 {Training loss}=0.02227148786187172\n",
            "2023-02-03 23:51:07,097 [nnabla][INFO]: iter=4619 {Training error}=0.00625\n",
            "2023-02-03 23:51:07,124 [nnabla][INFO]: iter=4629 {Training loss}=0.021919380873441696\n",
            "2023-02-03 23:51:07,124 [nnabla][INFO]: iter=4629 {Training error}=0.0046875\n",
            "2023-02-03 23:51:07,153 [nnabla][INFO]: iter=4639 {Training loss}=0.020303502678871155\n",
            "2023-02-03 23:51:07,154 [nnabla][INFO]: iter=4639 {Training error}=0.00546875\n",
            "2023-02-03 23:51:07,182 [nnabla][INFO]: iter=4649 {Training loss}=0.03171340376138687\n",
            "2023-02-03 23:51:07,183 [nnabla][INFO]: iter=4649 {Training error}=0.00859375\n",
            "2023-02-03 23:51:07,210 [nnabla][INFO]: iter=4659 {Training loss}=0.019385142251849174\n",
            "2023-02-03 23:51:07,210 [nnabla][INFO]: iter=4659 {Training error}=0.003125\n",
            "2023-02-03 23:51:07,237 [nnabla][INFO]: iter=4669 {Training loss}=0.020466970279812813\n",
            "2023-02-03 23:51:07,237 [nnabla][INFO]: iter=4669 {Training error}=0.00625\n",
            "2023-02-03 23:51:07,264 [nnabla][INFO]: iter=4679 {Training loss}=0.019087448716163635\n",
            "2023-02-03 23:51:07,264 [nnabla][INFO]: iter=4679 {Training error}=0.00546875\n",
            "2023-02-03 23:51:07,292 [nnabla][INFO]: iter=4689 {Training loss}=0.013845318928360939\n",
            "2023-02-03 23:51:07,292 [nnabla][INFO]: iter=4689 {Training error}=0.00546875\n",
            "2023-02-03 23:51:07,319 [nnabla][INFO]: iter=4699 {Training loss}=0.016276005655527115\n",
            "2023-02-03 23:51:07,319 [nnabla][INFO]: iter=4699 {Training error}=0.00625\n",
            "2023-02-03 23:51:07,319 [nnabla][INFO]: iter=4699 {Training time}=0.2878584861755371[sec/100iter] 17.691534280776978[sec]\n",
            "2023-02-03 23:51:07,331 [nnabla][INFO]: iter=4700 {Test error}=0.0109375\n",
            "2023-02-03 23:51:07,357 [nnabla][INFO]: iter=4709 {Training loss}=0.028973769396543503\n",
            "2023-02-03 23:51:07,357 [nnabla][INFO]: iter=4709 {Training error}=0.00859375\n",
            "2023-02-03 23:51:07,384 [nnabla][INFO]: iter=4719 {Training loss}=0.017709216102957726\n",
            "2023-02-03 23:51:07,384 [nnabla][INFO]: iter=4719 {Training error}=0.00703125\n",
            "2023-02-03 23:51:07,411 [nnabla][INFO]: iter=4729 {Training loss}=0.019261986017227173\n",
            "2023-02-03 23:51:07,411 [nnabla][INFO]: iter=4729 {Training error}=0.00703125\n",
            "2023-02-03 23:51:07,438 [nnabla][INFO]: iter=4739 {Training loss}=0.012854985892772675\n",
            "2023-02-03 23:51:07,438 [nnabla][INFO]: iter=4739 {Training error}=0.00390625\n",
            "2023-02-03 23:51:07,469 [nnabla][INFO]: iter=4749 {Training loss}=0.016607321798801422\n",
            "2023-02-03 23:51:07,469 [nnabla][INFO]: iter=4749 {Training error}=0.0046875\n",
            "2023-02-03 23:51:07,497 [nnabla][INFO]: iter=4759 {Training loss}=0.017423268407583237\n",
            "2023-02-03 23:51:07,497 [nnabla][INFO]: iter=4759 {Training error}=0.0046875\n",
            "2023-02-03 23:51:07,524 [nnabla][INFO]: iter=4769 {Training loss}=0.018706202507019043\n",
            "2023-02-03 23:51:07,524 [nnabla][INFO]: iter=4769 {Training error}=0.00703125\n",
            "2023-02-03 23:51:07,553 [nnabla][INFO]: iter=4779 {Training loss}=0.020069098100066185\n",
            "2023-02-03 23:51:07,553 [nnabla][INFO]: iter=4779 {Training error}=0.00625\n",
            "2023-02-03 23:51:07,580 [nnabla][INFO]: iter=4789 {Training loss}=0.020869914442300797\n",
            "2023-02-03 23:51:07,580 [nnabla][INFO]: iter=4789 {Training error}=0.00859375\n",
            "2023-02-03 23:51:07,609 [nnabla][INFO]: iter=4799 {Training loss}=0.030827248468995094\n",
            "2023-02-03 23:51:07,610 [nnabla][INFO]: iter=4799 {Training error}=0.00859375\n",
            "2023-02-03 23:51:07,610 [nnabla][INFO]: iter=4799 {Training time}=0.29063987731933594[sec/100iter] 17.982174158096313[sec]\n",
            "2023-02-03 23:51:07,620 [nnabla][INFO]: iter=4800 {Test error}=0.01328125\n",
            "2023-02-03 23:51:07,648 [nnabla][INFO]: iter=4809 {Training loss}=0.02102476917207241\n",
            "2023-02-03 23:51:07,648 [nnabla][INFO]: iter=4809 {Training error}=0.00625\n",
            "2023-02-03 23:51:07,675 [nnabla][INFO]: iter=4819 {Training loss}=0.029489576816558838\n",
            "2023-02-03 23:51:07,675 [nnabla][INFO]: iter=4819 {Training error}=0.0125\n",
            "2023-02-03 23:51:07,702 [nnabla][INFO]: iter=4829 {Training loss}=0.024198606610298157\n",
            "2023-02-03 23:51:07,702 [nnabla][INFO]: iter=4829 {Training error}=0.0078125\n",
            "2023-02-03 23:51:07,729 [nnabla][INFO]: iter=4839 {Training loss}=0.015477369539439678\n",
            "2023-02-03 23:51:07,729 [nnabla][INFO]: iter=4839 {Training error}=0.00546875\n",
            "2023-02-03 23:51:07,758 [nnabla][INFO]: iter=4849 {Training loss}=0.024622805416584015\n",
            "2023-02-03 23:51:07,758 [nnabla][INFO]: iter=4849 {Training error}=0.009375\n",
            "2023-02-03 23:51:07,784 [nnabla][INFO]: iter=4859 {Training loss}=0.014627104625105858\n",
            "2023-02-03 23:51:07,784 [nnabla][INFO]: iter=4859 {Training error}=0.00390625\n",
            "2023-02-03 23:51:07,811 [nnabla][INFO]: iter=4869 {Training loss}=0.01638765260577202\n",
            "2023-02-03 23:51:07,812 [nnabla][INFO]: iter=4869 {Training error}=0.00625\n",
            "2023-02-03 23:51:07,839 [nnabla][INFO]: iter=4879 {Training loss}=0.014497252181172371\n",
            "2023-02-03 23:51:07,839 [nnabla][INFO]: iter=4879 {Training error}=0.0046875\n",
            "2023-02-03 23:51:07,867 [nnabla][INFO]: iter=4889 {Training loss}=0.017953071743249893\n",
            "2023-02-03 23:51:07,867 [nnabla][INFO]: iter=4889 {Training error}=0.0046875\n",
            "2023-02-03 23:51:07,895 [nnabla][INFO]: iter=4899 {Training loss}=0.013785216026008129\n",
            "2023-02-03 23:51:07,895 [nnabla][INFO]: iter=4899 {Training error}=0.00234375\n",
            "2023-02-03 23:51:07,895 [nnabla][INFO]: iter=4899 {Training time}=0.2854630947113037[sec/100iter] 18.267637252807617[sec]\n",
            "2023-02-03 23:51:07,909 [nnabla][INFO]: iter=4900 {Test error}=0.01484375\n",
            "2023-02-03 23:51:07,937 [nnabla][INFO]: iter=4909 {Training loss}=0.03000962734222412\n",
            "2023-02-03 23:51:07,937 [nnabla][INFO]: iter=4909 {Training error}=0.0078125\n",
            "2023-02-03 23:51:07,964 [nnabla][INFO]: iter=4919 {Training loss}=0.021494287997484207\n",
            "2023-02-03 23:51:07,964 [nnabla][INFO]: iter=4919 {Training error}=0.00703125\n",
            "2023-02-03 23:51:07,990 [nnabla][INFO]: iter=4929 {Training loss}=0.020515451207756996\n",
            "2023-02-03 23:51:07,991 [nnabla][INFO]: iter=4929 {Training error}=0.00703125\n",
            "2023-02-03 23:51:08,017 [nnabla][INFO]: iter=4939 {Training loss}=0.017966492101550102\n",
            "2023-02-03 23:51:08,017 [nnabla][INFO]: iter=4939 {Training error}=0.00625\n",
            "2023-02-03 23:51:08,044 [nnabla][INFO]: iter=4949 {Training loss}=0.020700979977846146\n",
            "2023-02-03 23:51:08,044 [nnabla][INFO]: iter=4949 {Training error}=0.00703125\n",
            "2023-02-03 23:51:08,073 [nnabla][INFO]: iter=4959 {Training loss}=0.025633269920945168\n",
            "2023-02-03 23:51:08,073 [nnabla][INFO]: iter=4959 {Training error}=0.00859375\n",
            "2023-02-03 23:51:08,100 [nnabla][INFO]: iter=4969 {Training loss}=0.017334843054413795\n",
            "2023-02-03 23:51:08,101 [nnabla][INFO]: iter=4969 {Training error}=0.00546875\n",
            "2023-02-03 23:51:08,127 [nnabla][INFO]: iter=4979 {Training loss}=0.01226065494120121\n",
            "2023-02-03 23:51:08,127 [nnabla][INFO]: iter=4979 {Training error}=0.0046875\n",
            "2023-02-03 23:51:08,154 [nnabla][INFO]: iter=4989 {Training loss}=0.017536669969558716\n",
            "2023-02-03 23:51:08,154 [nnabla][INFO]: iter=4989 {Training error}=0.00546875\n",
            "2023-02-03 23:51:08,181 [nnabla][INFO]: iter=4999 {Training loss}=0.027454176917672157\n",
            "2023-02-03 23:51:08,181 [nnabla][INFO]: iter=4999 {Training error}=0.009375\n",
            "2023-02-03 23:51:08,181 [nnabla][INFO]: iter=4999 {Training time}=0.2862684726715088[sec/100iter] 18.553905725479126[sec]\n",
            "2023-02-03 23:51:08,192 [nnabla][INFO]: iter=5000 {Test error}=0.009375\n",
            "2023-02-03 23:51:08,206 [nnabla][INFO]: Solver state save (.h5): output/states_5000.h5\n",
            "2023-02-03 23:51:08,212 [nnabla][INFO]: Parameter save (.h5): output/params_5000.h5\n",
            "2023-02-03 23:51:08,213 [nnabla][INFO]: Checkpoint save (.json): output/checkpoint_5000.json\n",
            "2023-02-03 23:51:08,242 [nnabla][INFO]: iter=5009 {Training loss}=0.02646087110042572\n",
            "2023-02-03 23:51:08,242 [nnabla][INFO]: iter=5009 {Training error}=0.0078125\n",
            "2023-02-03 23:51:08,270 [nnabla][INFO]: iter=5019 {Training loss}=0.01944415643811226\n",
            "2023-02-03 23:51:08,270 [nnabla][INFO]: iter=5019 {Training error}=0.00703125\n",
            "2023-02-03 23:51:08,297 [nnabla][INFO]: iter=5029 {Training loss}=0.039542607963085175\n",
            "2023-02-03 23:51:08,297 [nnabla][INFO]: iter=5029 {Training error}=0.00859375\n",
            "2023-02-03 23:51:08,323 [nnabla][INFO]: iter=5039 {Training loss}=0.019640443846583366\n",
            "2023-02-03 23:51:08,323 [nnabla][INFO]: iter=5039 {Training error}=0.00546875\n",
            "2023-02-03 23:51:08,351 [nnabla][INFO]: iter=5049 {Training loss}=0.012342339381575584\n",
            "2023-02-03 23:51:08,351 [nnabla][INFO]: iter=5049 {Training error}=0.00390625\n",
            "2023-02-03 23:51:08,378 [nnabla][INFO]: iter=5059 {Training loss}=0.02206728234887123\n",
            "2023-02-03 23:51:08,379 [nnabla][INFO]: iter=5059 {Training error}=0.00625\n",
            "2023-02-03 23:51:08,405 [nnabla][INFO]: iter=5069 {Training loss}=0.016382981091737747\n",
            "2023-02-03 23:51:08,406 [nnabla][INFO]: iter=5069 {Training error}=0.00625\n",
            "2023-02-03 23:51:08,432 [nnabla][INFO]: iter=5079 {Training loss}=0.01725674234330654\n",
            "2023-02-03 23:51:08,432 [nnabla][INFO]: iter=5079 {Training error}=0.00390625\n",
            "2023-02-03 23:51:08,459 [nnabla][INFO]: iter=5089 {Training loss}=0.02526848576962948\n",
            "2023-02-03 23:51:08,459 [nnabla][INFO]: iter=5089 {Training error}=0.0078125\n",
            "2023-02-03 23:51:08,489 [nnabla][INFO]: iter=5099 {Training loss}=0.01876470446586609\n",
            "2023-02-03 23:51:08,489 [nnabla][INFO]: iter=5099 {Training error}=0.00390625\n",
            "2023-02-03 23:51:08,489 [nnabla][INFO]: iter=5099 {Training time}=0.30802297592163086[sec/100iter] 18.861928701400757[sec]\n",
            "2023-02-03 23:51:08,500 [nnabla][INFO]: iter=5100 {Test error}=0.0125\n",
            "2023-02-03 23:51:08,527 [nnabla][INFO]: iter=5109 {Training loss}=0.02395808696746826\n",
            "2023-02-03 23:51:08,527 [nnabla][INFO]: iter=5109 {Training error}=0.009375\n",
            "2023-02-03 23:51:08,553 [nnabla][INFO]: iter=5119 {Training loss}=0.019412150606513023\n",
            "2023-02-03 23:51:08,554 [nnabla][INFO]: iter=5119 {Training error}=0.00390625\n",
            "2023-02-03 23:51:08,580 [nnabla][INFO]: iter=5129 {Training loss}=0.01668555662035942\n",
            "2023-02-03 23:51:08,580 [nnabla][INFO]: iter=5129 {Training error}=0.00625\n",
            "2023-02-03 23:51:08,611 [nnabla][INFO]: iter=5139 {Training loss}=0.02136511728167534\n",
            "2023-02-03 23:51:08,611 [nnabla][INFO]: iter=5139 {Training error}=0.0078125\n",
            "2023-02-03 23:51:08,638 [nnabla][INFO]: iter=5149 {Training loss}=0.028438827022910118\n",
            "2023-02-03 23:51:08,638 [nnabla][INFO]: iter=5149 {Training error}=0.0109375\n",
            "2023-02-03 23:51:08,667 [nnabla][INFO]: iter=5159 {Training loss}=0.03173793852329254\n",
            "2023-02-03 23:51:08,667 [nnabla][INFO]: iter=5159 {Training error}=0.01171875\n",
            "2023-02-03 23:51:08,694 [nnabla][INFO]: iter=5169 {Training loss}=0.008447771891951561\n",
            "2023-02-03 23:51:08,694 [nnabla][INFO]: iter=5169 {Training error}=0.003125\n",
            "2023-02-03 23:51:08,721 [nnabla][INFO]: iter=5179 {Training loss}=0.01832365244626999\n",
            "2023-02-03 23:51:08,721 [nnabla][INFO]: iter=5179 {Training error}=0.00625\n",
            "2023-02-03 23:51:08,749 [nnabla][INFO]: iter=5189 {Training loss}=0.015547546558082104\n",
            "2023-02-03 23:51:08,749 [nnabla][INFO]: iter=5189 {Training error}=0.00390625\n",
            "2023-02-03 23:51:08,776 [nnabla][INFO]: iter=5199 {Training loss}=0.03151562064886093\n",
            "2023-02-03 23:51:08,776 [nnabla][INFO]: iter=5199 {Training error}=0.01015625\n",
            "2023-02-03 23:51:08,776 [nnabla][INFO]: iter=5199 {Training time}=0.28664493560791016[sec/100iter] 19.148573637008667[sec]\n",
            "2023-02-03 23:51:08,787 [nnabla][INFO]: iter=5200 {Test error}=0.01015625\n",
            "2023-02-03 23:51:08,814 [nnabla][INFO]: iter=5209 {Training loss}=0.016033444553613663\n",
            "2023-02-03 23:51:08,814 [nnabla][INFO]: iter=5209 {Training error}=0.0046875\n",
            "2023-02-03 23:51:08,841 [nnabla][INFO]: iter=5219 {Training loss}=0.014668417163193226\n",
            "2023-02-03 23:51:08,841 [nnabla][INFO]: iter=5219 {Training error}=0.00390625\n",
            "2023-02-03 23:51:08,868 [nnabla][INFO]: iter=5229 {Training loss}=0.014434168115258217\n",
            "2023-02-03 23:51:08,868 [nnabla][INFO]: iter=5229 {Training error}=0.00859375\n",
            "2023-02-03 23:51:08,895 [nnabla][INFO]: iter=5239 {Training loss}=0.012859498150646687\n",
            "2023-02-03 23:51:08,896 [nnabla][INFO]: iter=5239 {Training error}=0.0046875\n",
            "2023-02-03 23:51:08,923 [nnabla][INFO]: iter=5249 {Training loss}=0.01777789369225502\n",
            "2023-02-03 23:51:08,924 [nnabla][INFO]: iter=5249 {Training error}=0.00703125\n",
            "2023-02-03 23:51:08,951 [nnabla][INFO]: iter=5259 {Training loss}=0.01976529322564602\n",
            "2023-02-03 23:51:08,951 [nnabla][INFO]: iter=5259 {Training error}=0.00625\n",
            "2023-02-03 23:51:08,979 [nnabla][INFO]: iter=5269 {Training loss}=0.027139976620674133\n",
            "2023-02-03 23:51:08,979 [nnabla][INFO]: iter=5269 {Training error}=0.00703125\n",
            "2023-02-03 23:51:09,006 [nnabla][INFO]: iter=5279 {Training loss}=0.014573216438293457\n",
            "2023-02-03 23:51:09,006 [nnabla][INFO]: iter=5279 {Training error}=0.00625\n",
            "2023-02-03 23:51:09,035 [nnabla][INFO]: iter=5289 {Training loss}=0.028273751959204674\n",
            "2023-02-03 23:51:09,036 [nnabla][INFO]: iter=5289 {Training error}=0.00703125\n",
            "2023-02-03 23:51:09,065 [nnabla][INFO]: iter=5299 {Training loss}=0.015045632608234882\n",
            "2023-02-03 23:51:09,066 [nnabla][INFO]: iter=5299 {Training error}=0.00546875\n",
            "2023-02-03 23:51:09,066 [nnabla][INFO]: iter=5299 {Training time}=0.2895946502685547[sec/100iter] 19.43816828727722[sec]\n",
            "2023-02-03 23:51:09,076 [nnabla][INFO]: iter=5300 {Test error}=0.00703125\n",
            "2023-02-03 23:51:09,104 [nnabla][INFO]: iter=5309 {Training loss}=0.02333221212029457\n",
            "2023-02-03 23:51:09,104 [nnabla][INFO]: iter=5309 {Training error}=0.00859375\n",
            "2023-02-03 23:51:09,131 [nnabla][INFO]: iter=5319 {Training loss}=0.02135181985795498\n",
            "2023-02-03 23:51:09,131 [nnabla][INFO]: iter=5319 {Training error}=0.00546875\n",
            "2023-02-03 23:51:09,158 [nnabla][INFO]: iter=5329 {Training loss}=0.01733488216996193\n",
            "2023-02-03 23:51:09,158 [nnabla][INFO]: iter=5329 {Training error}=0.00625\n",
            "2023-02-03 23:51:09,184 [nnabla][INFO]: iter=5339 {Training loss}=0.023682065308094025\n",
            "2023-02-03 23:51:09,184 [nnabla][INFO]: iter=5339 {Training error}=0.0078125\n",
            "2023-02-03 23:51:09,211 [nnabla][INFO]: iter=5349 {Training loss}=0.018372246995568275\n",
            "2023-02-03 23:51:09,212 [nnabla][INFO]: iter=5349 {Training error}=0.0078125\n",
            "2023-02-03 23:51:09,238 [nnabla][INFO]: iter=5359 {Training loss}=0.013192916288971901\n",
            "2023-02-03 23:51:09,238 [nnabla][INFO]: iter=5359 {Training error}=0.00546875\n",
            "2023-02-03 23:51:09,267 [nnabla][INFO]: iter=5369 {Training loss}=0.02676336094737053\n",
            "2023-02-03 23:51:09,267 [nnabla][INFO]: iter=5369 {Training error}=0.0078125\n",
            "2023-02-03 23:51:09,296 [nnabla][INFO]: iter=5379 {Training loss}=0.021139109507203102\n",
            "2023-02-03 23:51:09,296 [nnabla][INFO]: iter=5379 {Training error}=0.0078125\n",
            "2023-02-03 23:51:09,323 [nnabla][INFO]: iter=5389 {Training loss}=0.017159100621938705\n",
            "2023-02-03 23:51:09,323 [nnabla][INFO]: iter=5389 {Training error}=0.0046875\n",
            "2023-02-03 23:51:09,350 [nnabla][INFO]: iter=5399 {Training loss}=0.015487737953662872\n",
            "2023-02-03 23:51:09,350 [nnabla][INFO]: iter=5399 {Training error}=0.00703125\n",
            "2023-02-03 23:51:09,350 [nnabla][INFO]: iter=5399 {Training time}=0.2845485210418701[sec/100iter] 19.722716808319092[sec]\n",
            "2023-02-03 23:51:09,362 [nnabla][INFO]: iter=5400 {Test error}=0.009375\n",
            "2023-02-03 23:51:09,389 [nnabla][INFO]: iter=5409 {Training loss}=0.014355769380927086\n",
            "2023-02-03 23:51:09,389 [nnabla][INFO]: iter=5409 {Training error}=0.00390625\n",
            "2023-02-03 23:51:09,416 [nnabla][INFO]: iter=5419 {Training loss}=0.010865236632525921\n",
            "2023-02-03 23:51:09,416 [nnabla][INFO]: iter=5419 {Training error}=0.00390625\n",
            "2023-02-03 23:51:09,443 [nnabla][INFO]: iter=5429 {Training loss}=0.01104535348713398\n",
            "2023-02-03 23:51:09,443 [nnabla][INFO]: iter=5429 {Training error}=0.0046875\n",
            "2023-02-03 23:51:09,470 [nnabla][INFO]: iter=5439 {Training loss}=0.015259087085723877\n",
            "2023-02-03 23:51:09,471 [nnabla][INFO]: iter=5439 {Training error}=0.003125\n",
            "2023-02-03 23:51:09,501 [nnabla][INFO]: iter=5449 {Training loss}=0.018374543637037277\n",
            "2023-02-03 23:51:09,501 [nnabla][INFO]: iter=5449 {Training error}=0.00546875\n",
            "2023-02-03 23:51:09,528 [nnabla][INFO]: iter=5459 {Training loss}=0.01870827004313469\n",
            "2023-02-03 23:51:09,528 [nnabla][INFO]: iter=5459 {Training error}=0.00625\n",
            "2023-02-03 23:51:09,555 [nnabla][INFO]: iter=5469 {Training loss}=0.02872401475906372\n",
            "2023-02-03 23:51:09,556 [nnabla][INFO]: iter=5469 {Training error}=0.00703125\n",
            "2023-02-03 23:51:09,584 [nnabla][INFO]: iter=5479 {Training loss}=0.04835982248187065\n",
            "2023-02-03 23:51:09,584 [nnabla][INFO]: iter=5479 {Training error}=0.01484375\n",
            "2023-02-03 23:51:09,612 [nnabla][INFO]: iter=5489 {Training loss}=0.023976843804121017\n",
            "2023-02-03 23:51:09,613 [nnabla][INFO]: iter=5489 {Training error}=0.01015625\n",
            "2023-02-03 23:51:09,642 [nnabla][INFO]: iter=5499 {Training loss}=0.010341191664338112\n",
            "2023-02-03 23:51:09,642 [nnabla][INFO]: iter=5499 {Training error}=0.00234375\n",
            "2023-02-03 23:51:09,642 [nnabla][INFO]: iter=5499 {Training time}=0.2918837070465088[sec/100iter] 20.0146005153656[sec]\n",
            "2023-02-03 23:51:09,653 [nnabla][INFO]: iter=5500 {Test error}=0.01015625\n",
            "2023-02-03 23:51:09,680 [nnabla][INFO]: iter=5509 {Training loss}=0.034020788967609406\n",
            "2023-02-03 23:51:09,680 [nnabla][INFO]: iter=5509 {Training error}=0.009375\n",
            "2023-02-03 23:51:09,707 [nnabla][INFO]: iter=5519 {Training loss}=0.023272966966032982\n",
            "2023-02-03 23:51:09,707 [nnabla][INFO]: iter=5519 {Training error}=0.009375\n",
            "2023-02-03 23:51:09,734 [nnabla][INFO]: iter=5529 {Training loss}=0.014009577222168446\n",
            "2023-02-03 23:51:09,735 [nnabla][INFO]: iter=5529 {Training error}=0.00546875\n",
            "2023-02-03 23:51:09,762 [nnabla][INFO]: iter=5539 {Training loss}=0.02692491188645363\n",
            "2023-02-03 23:51:09,762 [nnabla][INFO]: iter=5539 {Training error}=0.009375\n",
            "2023-02-03 23:51:09,789 [nnabla][INFO]: iter=5549 {Training loss}=0.03142915666103363\n",
            "2023-02-03 23:51:09,790 [nnabla][INFO]: iter=5549 {Training error}=0.0078125\n",
            "2023-02-03 23:51:09,816 [nnabla][INFO]: iter=5559 {Training loss}=0.0306406170129776\n",
            "2023-02-03 23:51:09,816 [nnabla][INFO]: iter=5559 {Training error}=0.0078125\n",
            "2023-02-03 23:51:09,843 [nnabla][INFO]: iter=5569 {Training loss}=0.03215435519814491\n",
            "2023-02-03 23:51:09,843 [nnabla][INFO]: iter=5569 {Training error}=0.0109375\n",
            "2023-02-03 23:51:09,871 [nnabla][INFO]: iter=5579 {Training loss}=0.017013993114233017\n",
            "2023-02-03 23:51:09,872 [nnabla][INFO]: iter=5579 {Training error}=0.0078125\n",
            "2023-02-03 23:51:09,898 [nnabla][INFO]: iter=5589 {Training loss}=0.017521269619464874\n",
            "2023-02-03 23:51:09,898 [nnabla][INFO]: iter=5589 {Training error}=0.00625\n",
            "2023-02-03 23:51:09,925 [nnabla][INFO]: iter=5599 {Training loss}=0.018237734213471413\n",
            "2023-02-03 23:51:09,925 [nnabla][INFO]: iter=5599 {Training error}=0.00859375\n",
            "2023-02-03 23:51:09,925 [nnabla][INFO]: iter=5599 {Training time}=0.2832775115966797[sec/100iter] 20.29787802696228[sec]\n",
            "2023-02-03 23:51:09,936 [nnabla][INFO]: iter=5600 {Test error}=0.0078125\n",
            "2023-02-03 23:51:09,963 [nnabla][INFO]: iter=5609 {Training loss}=0.01997239515185356\n",
            "2023-02-03 23:51:09,963 [nnabla][INFO]: iter=5609 {Training error}=0.00625\n",
            "2023-02-03 23:51:09,992 [nnabla][INFO]: iter=5619 {Training loss}=0.020188670605421066\n",
            "2023-02-03 23:51:09,992 [nnabla][INFO]: iter=5619 {Training error}=0.00625\n",
            "2023-02-03 23:51:10,019 [nnabla][INFO]: iter=5629 {Training loss}=0.030119111761450768\n",
            "2023-02-03 23:51:10,020 [nnabla][INFO]: iter=5629 {Training error}=0.0109375\n",
            "2023-02-03 23:51:10,046 [nnabla][INFO]: iter=5639 {Training loss}=0.012934662401676178\n",
            "2023-02-03 23:51:10,046 [nnabla][INFO]: iter=5639 {Training error}=0.00390625\n",
            "2023-02-03 23:51:10,074 [nnabla][INFO]: iter=5649 {Training loss}=0.012604041025042534\n",
            "2023-02-03 23:51:10,074 [nnabla][INFO]: iter=5649 {Training error}=0.003125\n",
            "2023-02-03 23:51:10,101 [nnabla][INFO]: iter=5659 {Training loss}=0.009011979214847088\n",
            "2023-02-03 23:51:10,101 [nnabla][INFO]: iter=5659 {Training error}=0.00234375\n",
            "2023-02-03 23:51:10,128 [nnabla][INFO]: iter=5669 {Training loss}=0.017695028334856033\n",
            "2023-02-03 23:51:10,128 [nnabla][INFO]: iter=5669 {Training error}=0.00625\n",
            "2023-02-03 23:51:10,155 [nnabla][INFO]: iter=5679 {Training loss}=0.016265643760561943\n",
            "2023-02-03 23:51:10,155 [nnabla][INFO]: iter=5679 {Training error}=0.00234375\n",
            "2023-02-03 23:51:10,184 [nnabla][INFO]: iter=5689 {Training loss}=0.017160601913928986\n",
            "2023-02-03 23:51:10,184 [nnabla][INFO]: iter=5689 {Training error}=0.00625\n",
            "2023-02-03 23:51:10,211 [nnabla][INFO]: iter=5699 {Training loss}=0.010976024903357029\n",
            "2023-02-03 23:51:10,211 [nnabla][INFO]: iter=5699 {Training error}=0.003125\n",
            "2023-02-03 23:51:10,211 [nnabla][INFO]: iter=5699 {Training time}=0.2858922481536865[sec/100iter] 20.583770275115967[sec]\n",
            "2023-02-03 23:51:10,222 [nnabla][INFO]: iter=5700 {Test error}=0.0046875\n",
            "2023-02-03 23:51:10,249 [nnabla][INFO]: iter=5709 {Training loss}=0.006426072213798761\n",
            "2023-02-03 23:51:10,249 [nnabla][INFO]: iter=5709 {Training error}=0.0015625\n",
            "2023-02-03 23:51:10,276 [nnabla][INFO]: iter=5719 {Training loss}=0.00862750131636858\n",
            "2023-02-03 23:51:10,276 [nnabla][INFO]: iter=5719 {Training error}=0.00234375\n",
            "2023-02-03 23:51:10,303 [nnabla][INFO]: iter=5729 {Training loss}=0.011058342643082142\n",
            "2023-02-03 23:51:10,303 [nnabla][INFO]: iter=5729 {Training error}=0.00234375\n",
            "2023-02-03 23:51:10,330 [nnabla][INFO]: iter=5739 {Training loss}=0.013497086241841316\n",
            "2023-02-03 23:51:10,330 [nnabla][INFO]: iter=5739 {Training error}=0.0046875\n",
            "2023-02-03 23:51:10,359 [nnabla][INFO]: iter=5749 {Training loss}=0.013111576437950134\n",
            "2023-02-03 23:51:10,359 [nnabla][INFO]: iter=5749 {Training error}=0.00390625\n",
            "2023-02-03 23:51:10,386 [nnabla][INFO]: iter=5759 {Training loss}=0.014842336066067219\n",
            "2023-02-03 23:51:10,386 [nnabla][INFO]: iter=5759 {Training error}=0.0046875\n",
            "2023-02-03 23:51:10,413 [nnabla][INFO]: iter=5769 {Training loss}=0.022195983678102493\n",
            "2023-02-03 23:51:10,413 [nnabla][INFO]: iter=5769 {Training error}=0.00859375\n",
            "2023-02-03 23:51:10,439 [nnabla][INFO]: iter=5779 {Training loss}=0.01922481507062912\n",
            "2023-02-03 23:51:10,439 [nnabla][INFO]: iter=5779 {Training error}=0.00703125\n",
            "2023-02-03 23:51:10,468 [nnabla][INFO]: iter=5789 {Training loss}=0.022196609526872635\n",
            "2023-02-03 23:51:10,468 [nnabla][INFO]: iter=5789 {Training error}=0.00546875\n",
            "2023-02-03 23:51:10,496 [nnabla][INFO]: iter=5799 {Training loss}=0.01709660515189171\n",
            "2023-02-03 23:51:10,496 [nnabla][INFO]: iter=5799 {Training error}=0.00859375\n",
            "2023-02-03 23:51:10,496 [nnabla][INFO]: iter=5799 {Training time}=0.2846801280975342[sec/100iter] 20.8684504032135[sec]\n",
            "2023-02-03 23:51:10,509 [nnabla][INFO]: iter=5800 {Test error}=0.00546875\n",
            "2023-02-03 23:51:10,538 [nnabla][INFO]: iter=5809 {Training loss}=0.02238333784043789\n",
            "2023-02-03 23:51:10,538 [nnabla][INFO]: iter=5809 {Training error}=0.00703125\n",
            "2023-02-03 23:51:10,565 [nnabla][INFO]: iter=5819 {Training loss}=0.008061805739998817\n",
            "2023-02-03 23:51:10,565 [nnabla][INFO]: iter=5819 {Training error}=0.00078125\n",
            "2023-02-03 23:51:10,591 [nnabla][INFO]: iter=5829 {Training loss}=0.009705998934805393\n",
            "2023-02-03 23:51:10,592 [nnabla][INFO]: iter=5829 {Training error}=0.003125\n",
            "2023-02-03 23:51:10,619 [nnabla][INFO]: iter=5839 {Training loss}=0.02494213916361332\n",
            "2023-02-03 23:51:10,619 [nnabla][INFO]: iter=5839 {Training error}=0.0078125\n",
            "2023-02-03 23:51:10,648 [nnabla][INFO]: iter=5849 {Training loss}=0.012078015133738518\n",
            "2023-02-03 23:51:10,648 [nnabla][INFO]: iter=5849 {Training error}=0.00234375\n",
            "2023-02-03 23:51:10,675 [nnabla][INFO]: iter=5859 {Training loss}=0.022380009293556213\n",
            "2023-02-03 23:51:10,675 [nnabla][INFO]: iter=5859 {Training error}=0.0078125\n",
            "2023-02-03 23:51:10,703 [nnabla][INFO]: iter=5869 {Training loss}=0.01484752632677555\n",
            "2023-02-03 23:51:10,703 [nnabla][INFO]: iter=5869 {Training error}=0.0046875\n",
            "2023-02-03 23:51:10,732 [nnabla][INFO]: iter=5879 {Training loss}=0.010114109143614769\n",
            "2023-02-03 23:51:10,732 [nnabla][INFO]: iter=5879 {Training error}=0.003125\n",
            "2023-02-03 23:51:10,759 [nnabla][INFO]: iter=5889 {Training loss}=0.016248131170868874\n",
            "2023-02-03 23:51:10,759 [nnabla][INFO]: iter=5889 {Training error}=0.00546875\n",
            "2023-02-03 23:51:10,787 [nnabla][INFO]: iter=5899 {Training loss}=0.014888880774378777\n",
            "2023-02-03 23:51:10,788 [nnabla][INFO]: iter=5899 {Training error}=0.003125\n",
            "2023-02-03 23:51:10,788 [nnabla][INFO]: iter=5899 {Training time}=0.29166674613952637[sec/100iter] 21.160117149353027[sec]\n",
            "2023-02-03 23:51:10,798 [nnabla][INFO]: iter=5900 {Test error}=0.0125\n",
            "2023-02-03 23:51:10,825 [nnabla][INFO]: iter=5909 {Training loss}=0.009013418108224869\n",
            "2023-02-03 23:51:10,825 [nnabla][INFO]: iter=5909 {Training error}=0.003125\n",
            "2023-02-03 23:51:10,852 [nnabla][INFO]: iter=5919 {Training loss}=0.017193863168358803\n",
            "2023-02-03 23:51:10,852 [nnabla][INFO]: iter=5919 {Training error}=0.00859375\n",
            "2023-02-03 23:51:10,879 [nnabla][INFO]: iter=5929 {Training loss}=0.0178906861692667\n",
            "2023-02-03 23:51:10,879 [nnabla][INFO]: iter=5929 {Training error}=0.0078125\n",
            "2023-02-03 23:51:10,906 [nnabla][INFO]: iter=5939 {Training loss}=0.03297638148069382\n",
            "2023-02-03 23:51:10,906 [nnabla][INFO]: iter=5939 {Training error}=0.01171875\n",
            "2023-02-03 23:51:10,933 [nnabla][INFO]: iter=5949 {Training loss}=0.03723466396331787\n",
            "2023-02-03 23:51:10,933 [nnabla][INFO]: iter=5949 {Training error}=0.01015625\n",
            "2023-02-03 23:51:10,959 [nnabla][INFO]: iter=5959 {Training loss}=0.01704627275466919\n",
            "2023-02-03 23:51:10,960 [nnabla][INFO]: iter=5959 {Training error}=0.00390625\n",
            "2023-02-03 23:51:10,986 [nnabla][INFO]: iter=5969 {Training loss}=0.022800667211413383\n",
            "2023-02-03 23:51:10,986 [nnabla][INFO]: iter=5969 {Training error}=0.00703125\n",
            "2023-02-03 23:51:11,013 [nnabla][INFO]: iter=5979 {Training loss}=0.01863088272511959\n",
            "2023-02-03 23:51:11,013 [nnabla][INFO]: iter=5979 {Training error}=0.00703125\n",
            "2023-02-03 23:51:11,040 [nnabla][INFO]: iter=5989 {Training loss}=0.011106330901384354\n",
            "2023-02-03 23:51:11,040 [nnabla][INFO]: iter=5989 {Training error}=0.003125\n",
            "2023-02-03 23:51:11,070 [nnabla][INFO]: iter=5999 {Training loss}=0.017585892230272293\n",
            "2023-02-03 23:51:11,070 [nnabla][INFO]: iter=5999 {Training error}=0.0046875\n",
            "2023-02-03 23:51:11,070 [nnabla][INFO]: iter=5999 {Training time}=0.28267884254455566[sec/100iter] 21.442795991897583[sec]\n",
            "2023-02-03 23:51:11,081 [nnabla][INFO]: iter=6000 {Test error}=0.00859375\n",
            "2023-02-03 23:51:11,095 [nnabla][INFO]: Solver state save (.h5): output/states_6000.h5\n",
            "2023-02-03 23:51:11,102 [nnabla][INFO]: Parameter save (.h5): output/params_6000.h5\n",
            "2023-02-03 23:51:11,102 [nnabla][INFO]: Checkpoint save (.json): output/checkpoint_6000.json\n",
            "2023-02-03 23:51:11,129 [nnabla][INFO]: iter=6009 {Training loss}=0.010482224635779858\n",
            "2023-02-03 23:51:11,129 [nnabla][INFO]: iter=6009 {Training error}=0.003125\n",
            "2023-02-03 23:51:11,156 [nnabla][INFO]: iter=6019 {Training loss}=0.014585038647055626\n",
            "2023-02-03 23:51:11,156 [nnabla][INFO]: iter=6019 {Training error}=0.00546875\n",
            "2023-02-03 23:51:11,183 [nnabla][INFO]: iter=6029 {Training loss}=0.02504158578813076\n",
            "2023-02-03 23:51:11,183 [nnabla][INFO]: iter=6029 {Training error}=0.00546875\n",
            "2023-02-03 23:51:11,210 [nnabla][INFO]: iter=6039 {Training loss}=0.02180718258023262\n",
            "2023-02-03 23:51:11,210 [nnabla][INFO]: iter=6039 {Training error}=0.00703125\n",
            "2023-02-03 23:51:11,236 [nnabla][INFO]: iter=6049 {Training loss}=0.007038028445094824\n",
            "2023-02-03 23:51:11,236 [nnabla][INFO]: iter=6049 {Training error}=0.00234375\n",
            "2023-02-03 23:51:11,263 [nnabla][INFO]: iter=6059 {Training loss}=0.021005257964134216\n",
            "2023-02-03 23:51:11,263 [nnabla][INFO]: iter=6059 {Training error}=0.00625\n",
            "2023-02-03 23:51:11,290 [nnabla][INFO]: iter=6069 {Training loss}=0.012568923644721508\n",
            "2023-02-03 23:51:11,290 [nnabla][INFO]: iter=6069 {Training error}=0.00390625\n",
            "2023-02-03 23:51:11,317 [nnabla][INFO]: iter=6079 {Training loss}=0.012259915471076965\n",
            "2023-02-03 23:51:11,317 [nnabla][INFO]: iter=6079 {Training error}=0.00390625\n",
            "2023-02-03 23:51:11,344 [nnabla][INFO]: iter=6089 {Training loss}=0.016380608081817627\n",
            "2023-02-03 23:51:11,344 [nnabla][INFO]: iter=6089 {Training error}=0.00390625\n",
            "2023-02-03 23:51:11,373 [nnabla][INFO]: iter=6099 {Training loss}=0.022834543138742447\n",
            "2023-02-03 23:51:11,373 [nnabla][INFO]: iter=6099 {Training error}=0.00625\n",
            "2023-02-03 23:51:11,373 [nnabla][INFO]: iter=6099 {Training time}=0.30315613746643066[sec/100iter] 21.745952129364014[sec]\n",
            "2023-02-03 23:51:11,384 [nnabla][INFO]: iter=6100 {Test error}=0.01640625\n",
            "2023-02-03 23:51:11,411 [nnabla][INFO]: iter=6109 {Training loss}=0.019879143685102463\n",
            "2023-02-03 23:51:11,411 [nnabla][INFO]: iter=6109 {Training error}=0.00703125\n",
            "2023-02-03 23:51:11,438 [nnabla][INFO]: iter=6119 {Training loss}=0.013730064034461975\n",
            "2023-02-03 23:51:11,438 [nnabla][INFO]: iter=6119 {Training error}=0.00390625\n",
            "2023-02-03 23:51:11,465 [nnabla][INFO]: iter=6129 {Training loss}=0.016711153090000153\n",
            "2023-02-03 23:51:11,465 [nnabla][INFO]: iter=6129 {Training error}=0.00703125\n",
            "2023-02-03 23:51:11,491 [nnabla][INFO]: iter=6139 {Training loss}=0.016263674944639206\n",
            "2023-02-03 23:51:11,492 [nnabla][INFO]: iter=6139 {Training error}=0.00546875\n",
            "2023-02-03 23:51:11,518 [nnabla][INFO]: iter=6149 {Training loss}=0.01610887423157692\n",
            "2023-02-03 23:51:11,518 [nnabla][INFO]: iter=6149 {Training error}=0.00625\n",
            "2023-02-03 23:51:11,548 [nnabla][INFO]: iter=6159 {Training loss}=0.015656528994441032\n",
            "2023-02-03 23:51:11,549 [nnabla][INFO]: iter=6159 {Training error}=0.0046875\n",
            "2023-02-03 23:51:11,577 [nnabla][INFO]: iter=6169 {Training loss}=0.02180943451821804\n",
            "2023-02-03 23:51:11,577 [nnabla][INFO]: iter=6169 {Training error}=0.01015625\n",
            "2023-02-03 23:51:11,604 [nnabla][INFO]: iter=6179 {Training loss}=0.02271648310124874\n",
            "2023-02-03 23:51:11,604 [nnabla][INFO]: iter=6179 {Training error}=0.00625\n",
            "2023-02-03 23:51:11,632 [nnabla][INFO]: iter=6189 {Training loss}=0.02526174485683441\n",
            "2023-02-03 23:51:11,632 [nnabla][INFO]: iter=6189 {Training error}=0.00703125\n",
            "2023-02-03 23:51:11,659 [nnabla][INFO]: iter=6199 {Training loss}=0.01610049232840538\n",
            "2023-02-03 23:51:11,660 [nnabla][INFO]: iter=6199 {Training error}=0.0046875\n",
            "2023-02-03 23:51:11,660 [nnabla][INFO]: iter=6199 {Training time}=0.2862551212310791[sec/100iter] 22.032207250595093[sec]\n",
            "2023-02-03 23:51:11,674 [nnabla][INFO]: iter=6200 {Test error}=0.00703125\n",
            "2023-02-03 23:51:11,705 [nnabla][INFO]: iter=6209 {Training loss}=0.018003908917307854\n",
            "2023-02-03 23:51:11,705 [nnabla][INFO]: iter=6209 {Training error}=0.0078125\n",
            "2023-02-03 23:51:11,732 [nnabla][INFO]: iter=6219 {Training loss}=0.017727073282003403\n",
            "2023-02-03 23:51:11,732 [nnabla][INFO]: iter=6219 {Training error}=0.00625\n",
            "2023-02-03 23:51:11,759 [nnabla][INFO]: iter=6229 {Training loss}=0.02371571958065033\n",
            "2023-02-03 23:51:11,759 [nnabla][INFO]: iter=6229 {Training error}=0.00546875\n",
            "2023-02-03 23:51:11,786 [nnabla][INFO]: iter=6239 {Training loss}=0.02195073664188385\n",
            "2023-02-03 23:51:11,787 [nnabla][INFO]: iter=6239 {Training error}=0.00703125\n",
            "2023-02-03 23:51:11,814 [nnabla][INFO]: iter=6249 {Training loss}=0.011951610445976257\n",
            "2023-02-03 23:51:11,814 [nnabla][INFO]: iter=6249 {Training error}=0.003125\n",
            "2023-02-03 23:51:11,841 [nnabla][INFO]: iter=6259 {Training loss}=0.013268252834677696\n",
            "2023-02-03 23:51:11,841 [nnabla][INFO]: iter=6259 {Training error}=0.00546875\n",
            "2023-02-03 23:51:11,868 [nnabla][INFO]: iter=6269 {Training loss}=0.011582110077142715\n",
            "2023-02-03 23:51:11,868 [nnabla][INFO]: iter=6269 {Training error}=0.00390625\n",
            "2023-02-03 23:51:11,895 [nnabla][INFO]: iter=6279 {Training loss}=0.012447448447346687\n",
            "2023-02-03 23:51:11,896 [nnabla][INFO]: iter=6279 {Training error}=0.00390625\n",
            "2023-02-03 23:51:11,928 [nnabla][INFO]: iter=6289 {Training loss}=0.021009139716625214\n",
            "2023-02-03 23:51:11,929 [nnabla][INFO]: iter=6289 {Training error}=0.00625\n",
            "2023-02-03 23:51:11,957 [nnabla][INFO]: iter=6299 {Training loss}=0.010408553294837475\n",
            "2023-02-03 23:51:11,957 [nnabla][INFO]: iter=6299 {Training error}=0.00234375\n",
            "2023-02-03 23:51:11,957 [nnabla][INFO]: iter=6299 {Training time}=0.2971503734588623[sec/100iter] 22.329357624053955[sec]\n",
            "2023-02-03 23:51:11,970 [nnabla][INFO]: iter=6300 {Test error}=0.00859375\n",
            "2023-02-03 23:51:11,997 [nnabla][INFO]: iter=6309 {Training loss}=0.009660748764872551\n",
            "2023-02-03 23:51:11,997 [nnabla][INFO]: iter=6309 {Training error}=0.003125\n",
            "2023-02-03 23:51:12,024 [nnabla][INFO]: iter=6319 {Training loss}=0.024774307385087013\n",
            "2023-02-03 23:51:12,024 [nnabla][INFO]: iter=6319 {Training error}=0.00859375\n",
            "2023-02-03 23:51:12,051 [nnabla][INFO]: iter=6329 {Training loss}=0.014735694043338299\n",
            "2023-02-03 23:51:12,051 [nnabla][INFO]: iter=6329 {Training error}=0.0078125\n",
            "2023-02-03 23:51:12,078 [nnabla][INFO]: iter=6339 {Training loss}=0.005897284019738436\n",
            "2023-02-03 23:51:12,079 [nnabla][INFO]: iter=6339 {Training error}=0.0015625\n",
            "2023-02-03 23:51:12,106 [nnabla][INFO]: iter=6349 {Training loss}=0.035149652510881424\n",
            "2023-02-03 23:51:12,106 [nnabla][INFO]: iter=6349 {Training error}=0.0046875\n",
            "2023-02-03 23:51:12,133 [nnabla][INFO]: iter=6359 {Training loss}=0.012978054583072662\n",
            "2023-02-03 23:51:12,133 [nnabla][INFO]: iter=6359 {Training error}=0.0046875\n",
            "2023-02-03 23:51:12,159 [nnabla][INFO]: iter=6369 {Training loss}=0.017798420041799545\n",
            "2023-02-03 23:51:12,159 [nnabla][INFO]: iter=6369 {Training error}=0.0078125\n",
            "2023-02-03 23:51:12,186 [nnabla][INFO]: iter=6379 {Training loss}=0.02411046251654625\n",
            "2023-02-03 23:51:12,187 [nnabla][INFO]: iter=6379 {Training error}=0.00625\n",
            "2023-02-03 23:51:12,216 [nnabla][INFO]: iter=6389 {Training loss}=0.02123575285077095\n",
            "2023-02-03 23:51:12,216 [nnabla][INFO]: iter=6389 {Training error}=0.00703125\n",
            "2023-02-03 23:51:12,243 [nnabla][INFO]: iter=6399 {Training loss}=0.01786794327199459\n",
            "2023-02-03 23:51:12,243 [nnabla][INFO]: iter=6399 {Training error}=0.00703125\n",
            "2023-02-03 23:51:12,243 [nnabla][INFO]: iter=6399 {Training time}=0.2861335277557373[sec/100iter] 22.615491151809692[sec]\n",
            "2023-02-03 23:51:12,254 [nnabla][INFO]: iter=6400 {Test error}=0.0109375\n",
            "2023-02-03 23:51:12,282 [nnabla][INFO]: iter=6409 {Training loss}=0.019120600074529648\n",
            "2023-02-03 23:51:12,282 [nnabla][INFO]: iter=6409 {Training error}=0.00703125\n",
            "2023-02-03 23:51:12,309 [nnabla][INFO]: iter=6419 {Training loss}=0.0188203863799572\n",
            "2023-02-03 23:51:12,309 [nnabla][INFO]: iter=6419 {Training error}=0.0078125\n",
            "2023-02-03 23:51:12,336 [nnabla][INFO]: iter=6429 {Training loss}=0.010672149248421192\n",
            "2023-02-03 23:51:12,337 [nnabla][INFO]: iter=6429 {Training error}=0.00234375\n",
            "2023-02-03 23:51:12,363 [nnabla][INFO]: iter=6439 {Training loss}=0.022241368889808655\n",
            "2023-02-03 23:51:12,364 [nnabla][INFO]: iter=6439 {Training error}=0.0046875\n",
            "2023-02-03 23:51:12,391 [nnabla][INFO]: iter=6449 {Training loss}=0.012187749147415161\n",
            "2023-02-03 23:51:12,391 [nnabla][INFO]: iter=6449 {Training error}=0.00234375\n",
            "2023-02-03 23:51:12,417 [nnabla][INFO]: iter=6459 {Training loss}=0.011144569143652916\n",
            "2023-02-03 23:51:12,418 [nnabla][INFO]: iter=6459 {Training error}=0.003125\n",
            "2023-02-03 23:51:12,444 [nnabla][INFO]: iter=6469 {Training loss}=0.021878331899642944\n",
            "2023-02-03 23:51:12,444 [nnabla][INFO]: iter=6469 {Training error}=0.0078125\n",
            "2023-02-03 23:51:12,471 [nnabla][INFO]: iter=6479 {Training loss}=0.012437695637345314\n",
            "2023-02-03 23:51:12,471 [nnabla][INFO]: iter=6479 {Training error}=0.00390625\n",
            "2023-02-03 23:51:12,498 [nnabla][INFO]: iter=6489 {Training loss}=0.014781665988266468\n",
            "2023-02-03 23:51:12,498 [nnabla][INFO]: iter=6489 {Training error}=0.00703125\n",
            "2023-02-03 23:51:12,525 [nnabla][INFO]: iter=6499 {Training loss}=0.013181569054722786\n",
            "2023-02-03 23:51:12,525 [nnabla][INFO]: iter=6499 {Training error}=0.00390625\n",
            "2023-02-03 23:51:12,525 [nnabla][INFO]: iter=6499 {Training time}=0.2819983959197998[sec/100iter] 22.897489547729492[sec]\n",
            "2023-02-03 23:51:12,540 [nnabla][INFO]: iter=6500 {Test error}=0.01171875\n",
            "2023-02-03 23:51:12,571 [nnabla][INFO]: iter=6509 {Training loss}=0.026464765891432762\n",
            "2023-02-03 23:51:12,571 [nnabla][INFO]: iter=6509 {Training error}=0.0046875\n",
            "2023-02-03 23:51:12,598 [nnabla][INFO]: iter=6519 {Training loss}=0.014972549863159657\n",
            "2023-02-03 23:51:12,599 [nnabla][INFO]: iter=6519 {Training error}=0.00546875\n",
            "2023-02-03 23:51:12,625 [nnabla][INFO]: iter=6529 {Training loss}=0.010855909436941147\n",
            "2023-02-03 23:51:12,625 [nnabla][INFO]: iter=6529 {Training error}=0.00234375\n",
            "2023-02-03 23:51:12,654 [nnabla][INFO]: iter=6539 {Training loss}=0.012301899492740631\n",
            "2023-02-03 23:51:12,654 [nnabla][INFO]: iter=6539 {Training error}=0.00390625\n",
            "2023-02-03 23:51:12,681 [nnabla][INFO]: iter=6549 {Training loss}=0.01720663346350193\n",
            "2023-02-03 23:51:12,681 [nnabla][INFO]: iter=6549 {Training error}=0.00625\n",
            "2023-02-03 23:51:12,708 [nnabla][INFO]: iter=6559 {Training loss}=0.012982593849301338\n",
            "2023-02-03 23:51:12,708 [nnabla][INFO]: iter=6559 {Training error}=0.0046875\n",
            "2023-02-03 23:51:12,736 [nnabla][INFO]: iter=6569 {Training loss}=0.010910658165812492\n",
            "2023-02-03 23:51:12,736 [nnabla][INFO]: iter=6569 {Training error}=0.00390625\n",
            "2023-02-03 23:51:12,763 [nnabla][INFO]: iter=6579 {Training loss}=0.00942966528236866\n",
            "2023-02-03 23:51:12,763 [nnabla][INFO]: iter=6579 {Training error}=0.00234375\n",
            "2023-02-03 23:51:12,790 [nnabla][INFO]: iter=6589 {Training loss}=0.012333895079791546\n",
            "2023-02-03 23:51:12,790 [nnabla][INFO]: iter=6589 {Training error}=0.00390625\n",
            "2023-02-03 23:51:12,817 [nnabla][INFO]: iter=6599 {Training loss}=0.008138439618051052\n",
            "2023-02-03 23:51:12,817 [nnabla][INFO]: iter=6599 {Training error}=0.0015625\n",
            "2023-02-03 23:51:12,817 [nnabla][INFO]: iter=6599 {Training time}=0.29189252853393555[sec/100iter] 23.189382076263428[sec]\n",
            "2023-02-03 23:51:12,827 [nnabla][INFO]: iter=6600 {Test error}=0.01171875\n",
            "2023-02-03 23:51:12,854 [nnabla][INFO]: iter=6609 {Training loss}=0.0055204895325005054\n",
            "2023-02-03 23:51:12,854 [nnabla][INFO]: iter=6609 {Training error}=0.0015625\n",
            "2023-02-03 23:51:12,883 [nnabla][INFO]: iter=6619 {Training loss}=0.010922564193606377\n",
            "2023-02-03 23:51:12,883 [nnabla][INFO]: iter=6619 {Training error}=0.003125\n",
            "2023-02-03 23:51:12,912 [nnabla][INFO]: iter=6629 {Training loss}=0.00783739797770977\n",
            "2023-02-03 23:51:12,913 [nnabla][INFO]: iter=6629 {Training error}=0.003125\n",
            "2023-02-03 23:51:12,939 [nnabla][INFO]: iter=6639 {Training loss}=0.005204099230468273\n",
            "2023-02-03 23:51:12,940 [nnabla][INFO]: iter=6639 {Training error}=0.0015625\n",
            "2023-02-03 23:51:12,966 [nnabla][INFO]: iter=6649 {Training loss}=0.014958387240767479\n",
            "2023-02-03 23:51:12,967 [nnabla][INFO]: iter=6649 {Training error}=0.0046875\n",
            "2023-02-03 23:51:12,993 [nnabla][INFO]: iter=6659 {Training loss}=0.007069809827953577\n",
            "2023-02-03 23:51:12,994 [nnabla][INFO]: iter=6659 {Training error}=0.0015625\n",
            "2023-02-03 23:51:13,020 [nnabla][INFO]: iter=6669 {Training loss}=0.00958104245364666\n",
            "2023-02-03 23:51:13,020 [nnabla][INFO]: iter=6669 {Training error}=0.00390625\n",
            "2023-02-03 23:51:13,047 [nnabla][INFO]: iter=6679 {Training loss}=0.010228387080132961\n",
            "2023-02-03 23:51:13,047 [nnabla][INFO]: iter=6679 {Training error}=0.00234375\n",
            "2023-02-03 23:51:13,074 [nnabla][INFO]: iter=6689 {Training loss}=0.006854590028524399\n",
            "2023-02-03 23:51:13,075 [nnabla][INFO]: iter=6689 {Training error}=0.0015625\n",
            "2023-02-03 23:51:13,106 [nnabla][INFO]: iter=6699 {Training loss}=0.013400420546531677\n",
            "2023-02-03 23:51:13,107 [nnabla][INFO]: iter=6699 {Training error}=0.00546875\n",
            "2023-02-03 23:51:13,107 [nnabla][INFO]: iter=6699 {Training time}=0.289870023727417[sec/100iter] 23.479252099990845[sec]\n",
            "2023-02-03 23:51:13,125 [nnabla][INFO]: iter=6700 {Test error}=0.00859375\n",
            "2023-02-03 23:51:13,157 [nnabla][INFO]: iter=6709 {Training loss}=0.008571918122470379\n",
            "2023-02-03 23:51:13,157 [nnabla][INFO]: iter=6709 {Training error}=0.00234375\n",
            "2023-02-03 23:51:13,200 [nnabla][INFO]: iter=6719 {Training loss}=0.008130000904202461\n",
            "2023-02-03 23:51:13,200 [nnabla][INFO]: iter=6719 {Training error}=0.00234375\n",
            "2023-02-03 23:51:13,234 [nnabla][INFO]: iter=6729 {Training loss}=0.03873705491423607\n",
            "2023-02-03 23:51:13,234 [nnabla][INFO]: iter=6729 {Training error}=0.0109375\n",
            "2023-02-03 23:51:13,267 [nnabla][INFO]: iter=6739 {Training loss}=0.02037966251373291\n",
            "2023-02-03 23:51:13,267 [nnabla][INFO]: iter=6739 {Training error}=0.00625\n",
            "2023-02-03 23:51:13,300 [nnabla][INFO]: iter=6749 {Training loss}=0.009073569439351559\n",
            "2023-02-03 23:51:13,300 [nnabla][INFO]: iter=6749 {Training error}=0.00234375\n",
            "2023-02-03 23:51:13,335 [nnabla][INFO]: iter=6759 {Training loss}=0.012926027178764343\n",
            "2023-02-03 23:51:13,335 [nnabla][INFO]: iter=6759 {Training error}=0.00390625\n",
            "2023-02-03 23:51:13,370 [nnabla][INFO]: iter=6769 {Training loss}=0.02330857701599598\n",
            "2023-02-03 23:51:13,370 [nnabla][INFO]: iter=6769 {Training error}=0.0046875\n",
            "2023-02-03 23:51:13,403 [nnabla][INFO]: iter=6779 {Training loss}=0.01683739572763443\n",
            "2023-02-03 23:51:13,403 [nnabla][INFO]: iter=6779 {Training error}=0.003125\n",
            "2023-02-03 23:51:13,437 [nnabla][INFO]: iter=6789 {Training loss}=0.020390627905726433\n",
            "2023-02-03 23:51:13,438 [nnabla][INFO]: iter=6789 {Training error}=0.0078125\n",
            "2023-02-03 23:51:13,471 [nnabla][INFO]: iter=6799 {Training loss}=0.016869833692908287\n",
            "2023-02-03 23:51:13,471 [nnabla][INFO]: iter=6799 {Training error}=0.00390625\n",
            "2023-02-03 23:51:13,471 [nnabla][INFO]: iter=6799 {Training time}=0.36430931091308594[sec/100iter] 23.84356141090393[sec]\n",
            "2023-02-03 23:51:13,488 [nnabla][INFO]: iter=6800 {Test error}=0.00625\n",
            "2023-02-03 23:51:13,521 [nnabla][INFO]: iter=6809 {Training loss}=0.018419573083519936\n",
            "2023-02-03 23:51:13,521 [nnabla][INFO]: iter=6809 {Training error}=0.00390625\n",
            "2023-02-03 23:51:13,554 [nnabla][INFO]: iter=6819 {Training loss}=0.011138493195176125\n",
            "2023-02-03 23:51:13,555 [nnabla][INFO]: iter=6819 {Training error}=0.0046875\n",
            "2023-02-03 23:51:13,589 [nnabla][INFO]: iter=6829 {Training loss}=0.01315355021506548\n",
            "2023-02-03 23:51:13,589 [nnabla][INFO]: iter=6829 {Training error}=0.00390625\n",
            "2023-02-03 23:51:13,633 [nnabla][INFO]: iter=6839 {Training loss}=0.019169706851243973\n",
            "2023-02-03 23:51:13,633 [nnabla][INFO]: iter=6839 {Training error}=0.00625\n",
            "2023-02-03 23:51:13,673 [nnabla][INFO]: iter=6849 {Training loss}=0.014268186874687672\n",
            "2023-02-03 23:51:13,673 [nnabla][INFO]: iter=6849 {Training error}=0.00390625\n",
            "2023-02-03 23:51:13,707 [nnabla][INFO]: iter=6859 {Training loss}=0.01729191280901432\n",
            "2023-02-03 23:51:13,707 [nnabla][INFO]: iter=6859 {Training error}=0.00625\n",
            "2023-02-03 23:51:13,740 [nnabla][INFO]: iter=6869 {Training loss}=0.0077140554785728455\n",
            "2023-02-03 23:51:13,740 [nnabla][INFO]: iter=6869 {Training error}=0.003125\n",
            "2023-02-03 23:51:13,774 [nnabla][INFO]: iter=6879 {Training loss}=0.014994258992373943\n",
            "2023-02-03 23:51:13,774 [nnabla][INFO]: iter=6879 {Training error}=0.00546875\n",
            "2023-02-03 23:51:13,809 [nnabla][INFO]: iter=6889 {Training loss}=0.011013982817530632\n",
            "2023-02-03 23:51:13,809 [nnabla][INFO]: iter=6889 {Training error}=0.00390625\n",
            "2023-02-03 23:51:13,842 [nnabla][INFO]: iter=6899 {Training loss}=0.01670469157397747\n",
            "2023-02-03 23:51:13,842 [nnabla][INFO]: iter=6899 {Training error}=0.0078125\n",
            "2023-02-03 23:51:13,842 [nnabla][INFO]: iter=6899 {Training time}=0.37108802795410156[sec/100iter] 24.214649438858032[sec]\n",
            "2023-02-03 23:51:13,858 [nnabla][INFO]: iter=6900 {Test error}=0.00859375\n",
            "2023-02-03 23:51:13,892 [nnabla][INFO]: iter=6909 {Training loss}=0.011493143625557423\n",
            "2023-02-03 23:51:13,892 [nnabla][INFO]: iter=6909 {Training error}=0.00390625\n",
            "2023-02-03 23:51:13,925 [nnabla][INFO]: iter=6919 {Training loss}=0.007661312818527222\n",
            "2023-02-03 23:51:13,926 [nnabla][INFO]: iter=6919 {Training error}=0.00234375\n",
            "2023-02-03 23:51:13,959 [nnabla][INFO]: iter=6929 {Training loss}=0.019560372456908226\n",
            "2023-02-03 23:51:13,959 [nnabla][INFO]: iter=6929 {Training error}=0.00703125\n",
            "2023-02-03 23:51:13,992 [nnabla][INFO]: iter=6939 {Training loss}=0.014167259447276592\n",
            "2023-02-03 23:51:13,992 [nnabla][INFO]: iter=6939 {Training error}=0.00625\n",
            "2023-02-03 23:51:14,025 [nnabla][INFO]: iter=6949 {Training loss}=0.0048097348771989346\n",
            "2023-02-03 23:51:14,025 [nnabla][INFO]: iter=6949 {Training error}=0.00078125\n",
            "2023-02-03 23:51:14,058 [nnabla][INFO]: iter=6959 {Training loss}=0.014258837327361107\n",
            "2023-02-03 23:51:14,058 [nnabla][INFO]: iter=6959 {Training error}=0.00703125\n",
            "2023-02-03 23:51:14,092 [nnabla][INFO]: iter=6969 {Training loss}=0.01077829860150814\n",
            "2023-02-03 23:51:14,092 [nnabla][INFO]: iter=6969 {Training error}=0.0046875\n",
            "2023-02-03 23:51:14,125 [nnabla][INFO]: iter=6979 {Training loss}=0.013639787212014198\n",
            "2023-02-03 23:51:14,125 [nnabla][INFO]: iter=6979 {Training error}=0.00390625\n",
            "2023-02-03 23:51:14,158 [nnabla][INFO]: iter=6989 {Training loss}=0.019480526447296143\n",
            "2023-02-03 23:51:14,158 [nnabla][INFO]: iter=6989 {Training error}=0.009375\n",
            "2023-02-03 23:51:14,192 [nnabla][INFO]: iter=6999 {Training loss}=0.012953400611877441\n",
            "2023-02-03 23:51:14,193 [nnabla][INFO]: iter=6999 {Training error}=0.0046875\n",
            "2023-02-03 23:51:14,193 [nnabla][INFO]: iter=6999 {Training time}=0.3505420684814453[sec/100iter] 24.565191507339478[sec]\n",
            "2023-02-03 23:51:14,210 [nnabla][INFO]: iter=7000 {Test error}=0.00625\n",
            "2023-02-03 23:51:14,228 [nnabla][INFO]: Solver state save (.h5): output/states_7000.h5\n",
            "2023-02-03 23:51:14,240 [nnabla][INFO]: Parameter save (.h5): output/params_7000.h5\n",
            "2023-02-03 23:51:14,240 [nnabla][INFO]: Checkpoint save (.json): output/checkpoint_7000.json\n",
            "2023-02-03 23:51:14,274 [nnabla][INFO]: iter=7009 {Training loss}=0.014321526512503624\n",
            "2023-02-03 23:51:14,274 [nnabla][INFO]: iter=7009 {Training error}=0.00390625\n",
            "2023-02-03 23:51:14,308 [nnabla][INFO]: iter=7019 {Training loss}=0.009444503113627434\n",
            "2023-02-03 23:51:14,309 [nnabla][INFO]: iter=7019 {Training error}=0.00390625\n",
            "2023-02-03 23:51:14,342 [nnabla][INFO]: iter=7029 {Training loss}=0.0199117548763752\n",
            "2023-02-03 23:51:14,342 [nnabla][INFO]: iter=7029 {Training error}=0.00859375\n",
            "2023-02-03 23:51:14,377 [nnabla][INFO]: iter=7039 {Training loss}=0.01806899532675743\n",
            "2023-02-03 23:51:14,377 [nnabla][INFO]: iter=7039 {Training error}=0.0046875\n",
            "2023-02-03 23:51:14,411 [nnabla][INFO]: iter=7049 {Training loss}=0.011354963295161724\n",
            "2023-02-03 23:51:14,411 [nnabla][INFO]: iter=7049 {Training error}=0.00390625\n",
            "2023-02-03 23:51:14,445 [nnabla][INFO]: iter=7059 {Training loss}=0.004516324959695339\n",
            "2023-02-03 23:51:14,445 [nnabla][INFO]: iter=7059 {Training error}=0.0\n",
            "2023-02-03 23:51:14,478 [nnabla][INFO]: iter=7069 {Training loss}=0.008526064455509186\n",
            "2023-02-03 23:51:14,479 [nnabla][INFO]: iter=7069 {Training error}=0.00234375\n",
            "2023-02-03 23:51:14,512 [nnabla][INFO]: iter=7079 {Training loss}=0.014097144827246666\n",
            "2023-02-03 23:51:14,512 [nnabla][INFO]: iter=7079 {Training error}=0.00546875\n",
            "2023-02-03 23:51:14,546 [nnabla][INFO]: iter=7089 {Training loss}=0.017175735905766487\n",
            "2023-02-03 23:51:14,546 [nnabla][INFO]: iter=7089 {Training error}=0.0078125\n",
            "2023-02-03 23:51:14,580 [nnabla][INFO]: iter=7099 {Training loss}=0.009649905376136303\n",
            "2023-02-03 23:51:14,580 [nnabla][INFO]: iter=7099 {Training error}=0.003125\n",
            "2023-02-03 23:51:14,580 [nnabla][INFO]: iter=7099 {Training time}=0.38738489151000977[sec/100iter] 24.952576398849487[sec]\n",
            "2023-02-03 23:51:14,597 [nnabla][INFO]: iter=7100 {Test error}=0.0109375\n",
            "2023-02-03 23:51:14,633 [nnabla][INFO]: iter=7109 {Training loss}=0.008196970447897911\n",
            "2023-02-03 23:51:14,633 [nnabla][INFO]: iter=7109 {Training error}=0.003125\n",
            "2023-02-03 23:51:14,670 [nnabla][INFO]: iter=7119 {Training loss}=0.009311302565038204\n",
            "2023-02-03 23:51:14,670 [nnabla][INFO]: iter=7119 {Training error}=0.003125\n",
            "2023-02-03 23:51:14,709 [nnabla][INFO]: iter=7129 {Training loss}=0.011505255475640297\n",
            "2023-02-03 23:51:14,709 [nnabla][INFO]: iter=7129 {Training error}=0.00390625\n",
            "2023-02-03 23:51:14,743 [nnabla][INFO]: iter=7139 {Training loss}=0.011119285598397255\n",
            "2023-02-03 23:51:14,743 [nnabla][INFO]: iter=7139 {Training error}=0.00390625\n",
            "2023-02-03 23:51:14,777 [nnabla][INFO]: iter=7149 {Training loss}=0.029776349663734436\n",
            "2023-02-03 23:51:14,778 [nnabla][INFO]: iter=7149 {Training error}=0.00546875\n",
            "2023-02-03 23:51:14,812 [nnabla][INFO]: iter=7159 {Training loss}=0.006154394708573818\n",
            "2023-02-03 23:51:14,813 [nnabla][INFO]: iter=7159 {Training error}=0.0015625\n",
            "2023-02-03 23:51:14,847 [nnabla][INFO]: iter=7169 {Training loss}=0.007438625209033489\n",
            "2023-02-03 23:51:14,848 [nnabla][INFO]: iter=7169 {Training error}=0.00078125\n",
            "2023-02-03 23:51:14,882 [nnabla][INFO]: iter=7179 {Training loss}=0.007549019996076822\n",
            "2023-02-03 23:51:14,882 [nnabla][INFO]: iter=7179 {Training error}=0.00234375\n",
            "2023-02-03 23:51:14,916 [nnabla][INFO]: iter=7189 {Training loss}=0.020533405244350433\n",
            "2023-02-03 23:51:14,916 [nnabla][INFO]: iter=7189 {Training error}=0.00546875\n",
            "2023-02-03 23:51:14,950 [nnabla][INFO]: iter=7199 {Training loss}=0.006303905043751001\n",
            "2023-02-03 23:51:14,950 [nnabla][INFO]: iter=7199 {Training error}=0.0015625\n",
            "2023-02-03 23:51:14,950 [nnabla][INFO]: iter=7199 {Training time}=0.3701317310333252[sec/100iter] 25.322708129882812[sec]\n",
            "2023-02-03 23:51:14,967 [nnabla][INFO]: iter=7200 {Test error}=0.0109375\n",
            "2023-02-03 23:51:15,002 [nnabla][INFO]: iter=7209 {Training loss}=0.011152627877891064\n",
            "2023-02-03 23:51:15,002 [nnabla][INFO]: iter=7209 {Training error}=0.00546875\n",
            "2023-02-03 23:51:15,036 [nnabla][INFO]: iter=7219 {Training loss}=0.009764468297362328\n",
            "2023-02-03 23:51:15,036 [nnabla][INFO]: iter=7219 {Training error}=0.0046875\n",
            "2023-02-03 23:51:15,070 [nnabla][INFO]: iter=7229 {Training loss}=0.008646545000374317\n",
            "2023-02-03 23:51:15,070 [nnabla][INFO]: iter=7229 {Training error}=0.0015625\n",
            "2023-02-03 23:51:15,104 [nnabla][INFO]: iter=7239 {Training loss}=0.008721873164176941\n",
            "2023-02-03 23:51:15,104 [nnabla][INFO]: iter=7239 {Training error}=0.0015625\n",
            "2023-02-03 23:51:15,137 [nnabla][INFO]: iter=7249 {Training loss}=0.02357776090502739\n",
            "2023-02-03 23:51:15,138 [nnabla][INFO]: iter=7249 {Training error}=0.00546875\n",
            "2023-02-03 23:51:15,172 [nnabla][INFO]: iter=7259 {Training loss}=0.02001865580677986\n",
            "2023-02-03 23:51:15,173 [nnabla][INFO]: iter=7259 {Training error}=0.00703125\n",
            "2023-02-03 23:51:15,208 [nnabla][INFO]: iter=7269 {Training loss}=0.006805176846683025\n",
            "2023-02-03 23:51:15,209 [nnabla][INFO]: iter=7269 {Training error}=0.0015625\n",
            "2023-02-03 23:51:15,243 [nnabla][INFO]: iter=7279 {Training loss}=0.008210130035877228\n",
            "2023-02-03 23:51:15,244 [nnabla][INFO]: iter=7279 {Training error}=0.0015625\n",
            "2023-02-03 23:51:15,278 [nnabla][INFO]: iter=7289 {Training loss}=0.012701514177024364\n",
            "2023-02-03 23:51:15,278 [nnabla][INFO]: iter=7289 {Training error}=0.0015625\n",
            "2023-02-03 23:51:15,311 [nnabla][INFO]: iter=7299 {Training loss}=0.011815110221505165\n",
            "2023-02-03 23:51:15,311 [nnabla][INFO]: iter=7299 {Training error}=0.0046875\n",
            "2023-02-03 23:51:15,311 [nnabla][INFO]: iter=7299 {Training time}=0.36121249198913574[sec/100iter] 25.68392062187195[sec]\n",
            "2023-02-03 23:51:15,327 [nnabla][INFO]: iter=7300 {Test error}=0.01171875\n",
            "2023-02-03 23:51:15,361 [nnabla][INFO]: iter=7309 {Training loss}=0.013698458671569824\n",
            "2023-02-03 23:51:15,362 [nnabla][INFO]: iter=7309 {Training error}=0.00546875\n",
            "2023-02-03 23:51:15,394 [nnabla][INFO]: iter=7319 {Training loss}=0.012572547420859337\n",
            "2023-02-03 23:51:15,395 [nnabla][INFO]: iter=7319 {Training error}=0.00234375\n",
            "2023-02-03 23:51:15,427 [nnabla][INFO]: iter=7329 {Training loss}=0.013794094324111938\n",
            "2023-02-03 23:51:15,428 [nnabla][INFO]: iter=7329 {Training error}=0.0046875\n",
            "2023-02-03 23:51:15,462 [nnabla][INFO]: iter=7339 {Training loss}=0.008812100626528263\n",
            "2023-02-03 23:51:15,463 [nnabla][INFO]: iter=7339 {Training error}=0.00078125\n",
            "2023-02-03 23:51:15,497 [nnabla][INFO]: iter=7349 {Training loss}=0.0070010037161409855\n",
            "2023-02-03 23:51:15,497 [nnabla][INFO]: iter=7349 {Training error}=0.00234375\n",
            "2023-02-03 23:51:15,530 [nnabla][INFO]: iter=7359 {Training loss}=0.006302681751549244\n",
            "2023-02-03 23:51:15,531 [nnabla][INFO]: iter=7359 {Training error}=0.00234375\n",
            "2023-02-03 23:51:15,565 [nnabla][INFO]: iter=7369 {Training loss}=0.009824469685554504\n",
            "2023-02-03 23:51:15,565 [nnabla][INFO]: iter=7369 {Training error}=0.003125\n",
            "2023-02-03 23:51:15,599 [nnabla][INFO]: iter=7379 {Training loss}=0.012547604739665985\n",
            "2023-02-03 23:51:15,599 [nnabla][INFO]: iter=7379 {Training error}=0.00390625\n",
            "2023-02-03 23:51:15,633 [nnabla][INFO]: iter=7389 {Training loss}=0.012921104207634926\n",
            "2023-02-03 23:51:15,633 [nnabla][INFO]: iter=7389 {Training error}=0.00234375\n",
            "2023-02-03 23:51:15,669 [nnabla][INFO]: iter=7399 {Training loss}=0.005379654932767153\n",
            "2023-02-03 23:51:15,669 [nnabla][INFO]: iter=7399 {Training error}=0.0015625\n",
            "2023-02-03 23:51:15,669 [nnabla][INFO]: iter=7399 {Training time}=0.35800647735595703[sec/100iter] 26.041927099227905[sec]\n",
            "2023-02-03 23:51:15,687 [nnabla][INFO]: iter=7400 {Test error}=0.01015625\n",
            "2023-02-03 23:51:15,723 [nnabla][INFO]: iter=7409 {Training loss}=0.0076612308621406555\n",
            "2023-02-03 23:51:15,724 [nnabla][INFO]: iter=7409 {Training error}=0.00234375\n",
            "2023-02-03 23:51:15,757 [nnabla][INFO]: iter=7419 {Training loss}=0.015153785236179829\n",
            "2023-02-03 23:51:15,758 [nnabla][INFO]: iter=7419 {Training error}=0.00625\n",
            "2023-02-03 23:51:15,799 [nnabla][INFO]: iter=7429 {Training loss}=0.018262673169374466\n",
            "2023-02-03 23:51:15,800 [nnabla][INFO]: iter=7429 {Training error}=0.00546875\n",
            "2023-02-03 23:51:15,834 [nnabla][INFO]: iter=7439 {Training loss}=0.007597075309604406\n",
            "2023-02-03 23:51:15,834 [nnabla][INFO]: iter=7439 {Training error}=0.003125\n",
            "2023-02-03 23:51:15,869 [nnabla][INFO]: iter=7449 {Training loss}=0.01366948802024126\n",
            "2023-02-03 23:51:15,869 [nnabla][INFO]: iter=7449 {Training error}=0.003125\n",
            "2023-02-03 23:51:15,903 [nnabla][INFO]: iter=7459 {Training loss}=0.024941759184002876\n",
            "2023-02-03 23:51:15,903 [nnabla][INFO]: iter=7459 {Training error}=0.0078125\n",
            "2023-02-03 23:51:15,936 [nnabla][INFO]: iter=7469 {Training loss}=0.02391723543405533\n",
            "2023-02-03 23:51:15,937 [nnabla][INFO]: iter=7469 {Training error}=0.00625\n",
            "2023-02-03 23:51:15,970 [nnabla][INFO]: iter=7479 {Training loss}=0.016848113387823105\n",
            "2023-02-03 23:51:15,970 [nnabla][INFO]: iter=7479 {Training error}=0.00625\n",
            "2023-02-03 23:51:16,004 [nnabla][INFO]: iter=7489 {Training loss}=0.025175493210554123\n",
            "2023-02-03 23:51:16,004 [nnabla][INFO]: iter=7489 {Training error}=0.00703125\n",
            "2023-02-03 23:51:16,040 [nnabla][INFO]: iter=7499 {Training loss}=0.015378966927528381\n",
            "2023-02-03 23:51:16,040 [nnabla][INFO]: iter=7499 {Training error}=0.00703125\n",
            "2023-02-03 23:51:16,040 [nnabla][INFO]: iter=7499 {Training time}=0.3707599639892578[sec/100iter] 26.412687063217163[sec]\n",
            "2023-02-03 23:51:16,058 [nnabla][INFO]: iter=7500 {Test error}=0.0109375\n",
            "2023-02-03 23:51:16,105 [nnabla][INFO]: iter=7509 {Training loss}=0.013093067333102226\n",
            "2023-02-03 23:51:16,106 [nnabla][INFO]: iter=7509 {Training error}=0.0046875\n",
            "2023-02-03 23:51:16,143 [nnabla][INFO]: iter=7519 {Training loss}=0.011702463962137699\n",
            "2023-02-03 23:51:16,143 [nnabla][INFO]: iter=7519 {Training error}=0.003125\n",
            "2023-02-03 23:51:16,180 [nnabla][INFO]: iter=7529 {Training loss}=0.013830041512846947\n",
            "2023-02-03 23:51:16,180 [nnabla][INFO]: iter=7529 {Training error}=0.00625\n",
            "2023-02-03 23:51:16,217 [nnabla][INFO]: iter=7539 {Training loss}=0.010204298421740532\n",
            "2023-02-03 23:51:16,218 [nnabla][INFO]: iter=7539 {Training error}=0.00234375\n",
            "2023-02-03 23:51:16,253 [nnabla][INFO]: iter=7549 {Training loss}=0.007672617677599192\n",
            "2023-02-03 23:51:16,254 [nnabla][INFO]: iter=7549 {Training error}=0.003125\n",
            "2023-02-03 23:51:16,288 [nnabla][INFO]: iter=7559 {Training loss}=0.0076405564323067665\n",
            "2023-02-03 23:51:16,288 [nnabla][INFO]: iter=7559 {Training error}=0.00078125\n",
            "2023-02-03 23:51:16,322 [nnabla][INFO]: iter=7569 {Training loss}=0.007009667344391346\n",
            "2023-02-03 23:51:16,322 [nnabla][INFO]: iter=7569 {Training error}=0.003125\n",
            "2023-02-03 23:51:16,357 [nnabla][INFO]: iter=7579 {Training loss}=0.004313780460506678\n",
            "2023-02-03 23:51:16,357 [nnabla][INFO]: iter=7579 {Training error}=0.00078125\n",
            "2023-02-03 23:51:16,395 [nnabla][INFO]: iter=7589 {Training loss}=0.01397929061204195\n",
            "2023-02-03 23:51:16,396 [nnabla][INFO]: iter=7589 {Training error}=0.00390625\n",
            "2023-02-03 23:51:16,437 [nnabla][INFO]: iter=7599 {Training loss}=0.007937783375382423\n",
            "2023-02-03 23:51:16,437 [nnabla][INFO]: iter=7599 {Training error}=0.003125\n",
            "2023-02-03 23:51:16,437 [nnabla][INFO]: iter=7599 {Training time}=0.3971993923187256[sec/100iter] 26.80988645553589[sec]\n",
            "2023-02-03 23:51:16,455 [nnabla][INFO]: iter=7600 {Test error}=0.0046875\n",
            "2023-02-03 23:51:16,490 [nnabla][INFO]: iter=7609 {Training loss}=0.006127840839326382\n",
            "2023-02-03 23:51:16,490 [nnabla][INFO]: iter=7609 {Training error}=0.00234375\n",
            "2023-02-03 23:51:16,524 [nnabla][INFO]: iter=7619 {Training loss}=0.012217432260513306\n",
            "2023-02-03 23:51:16,524 [nnabla][INFO]: iter=7619 {Training error}=0.003125\n",
            "2023-02-03 23:51:16,558 [nnabla][INFO]: iter=7629 {Training loss}=0.008421748876571655\n",
            "2023-02-03 23:51:16,558 [nnabla][INFO]: iter=7629 {Training error}=0.00234375\n",
            "2023-02-03 23:51:16,592 [nnabla][INFO]: iter=7639 {Training loss}=0.009875740855932236\n",
            "2023-02-03 23:51:16,592 [nnabla][INFO]: iter=7639 {Training error}=0.0015625\n",
            "2023-02-03 23:51:16,628 [nnabla][INFO]: iter=7649 {Training loss}=0.009541868232190609\n",
            "2023-02-03 23:51:16,628 [nnabla][INFO]: iter=7649 {Training error}=0.003125\n",
            "2023-02-03 23:51:16,664 [nnabla][INFO]: iter=7659 {Training loss}=0.005078384652733803\n",
            "2023-02-03 23:51:16,665 [nnabla][INFO]: iter=7659 {Training error}=0.00078125\n",
            "2023-02-03 23:51:16,703 [nnabla][INFO]: iter=7669 {Training loss}=0.008366502821445465\n",
            "2023-02-03 23:51:16,704 [nnabla][INFO]: iter=7669 {Training error}=0.00390625\n",
            "2023-02-03 23:51:16,739 [nnabla][INFO]: iter=7679 {Training loss}=0.009757677093148232\n",
            "2023-02-03 23:51:16,739 [nnabla][INFO]: iter=7679 {Training error}=0.00390625\n",
            "2023-02-03 23:51:16,773 [nnabla][INFO]: iter=7689 {Training loss}=0.010697592981159687\n",
            "2023-02-03 23:51:16,773 [nnabla][INFO]: iter=7689 {Training error}=0.00390625\n",
            "2023-02-03 23:51:16,807 [nnabla][INFO]: iter=7699 {Training loss}=0.01128346286714077\n",
            "2023-02-03 23:51:16,807 [nnabla][INFO]: iter=7699 {Training error}=0.00390625\n",
            "2023-02-03 23:51:16,807 [nnabla][INFO]: iter=7699 {Training time}=0.3696160316467285[sec/100iter] 27.179502487182617[sec]\n",
            "2023-02-03 23:51:16,824 [nnabla][INFO]: iter=7700 {Test error}=0.01171875\n",
            "2023-02-03 23:51:16,857 [nnabla][INFO]: iter=7709 {Training loss}=0.01201256550848484\n",
            "2023-02-03 23:51:16,858 [nnabla][INFO]: iter=7709 {Training error}=0.003125\n",
            "2023-02-03 23:51:16,891 [nnabla][INFO]: iter=7719 {Training loss}=0.020398421213030815\n",
            "2023-02-03 23:51:16,891 [nnabla][INFO]: iter=7719 {Training error}=0.00703125\n",
            "2023-02-03 23:51:16,925 [nnabla][INFO]: iter=7729 {Training loss}=0.00963208731263876\n",
            "2023-02-03 23:51:16,925 [nnabla][INFO]: iter=7729 {Training error}=0.00390625\n",
            "2023-02-03 23:51:16,958 [nnabla][INFO]: iter=7739 {Training loss}=0.01669120602309704\n",
            "2023-02-03 23:51:16,959 [nnabla][INFO]: iter=7739 {Training error}=0.00546875\n",
            "2023-02-03 23:51:16,994 [nnabla][INFO]: iter=7749 {Training loss}=0.02516370452940464\n",
            "2023-02-03 23:51:16,994 [nnabla][INFO]: iter=7749 {Training error}=0.009375\n",
            "2023-02-03 23:51:17,028 [nnabla][INFO]: iter=7759 {Training loss}=0.017596695572137833\n",
            "2023-02-03 23:51:17,028 [nnabla][INFO]: iter=7759 {Training error}=0.0046875\n",
            "2023-02-03 23:51:17,061 [nnabla][INFO]: iter=7769 {Training loss}=0.0036478079855442047\n",
            "2023-02-03 23:51:17,061 [nnabla][INFO]: iter=7769 {Training error}=0.00078125\n",
            "2023-02-03 23:51:17,095 [nnabla][INFO]: iter=7779 {Training loss}=0.01296196598559618\n",
            "2023-02-03 23:51:17,095 [nnabla][INFO]: iter=7779 {Training error}=0.0078125\n",
            "2023-02-03 23:51:17,128 [nnabla][INFO]: iter=7789 {Training loss}=0.017473086714744568\n",
            "2023-02-03 23:51:17,128 [nnabla][INFO]: iter=7789 {Training error}=0.00546875\n",
            "2023-02-03 23:51:17,161 [nnabla][INFO]: iter=7799 {Training loss}=0.008341452106833458\n",
            "2023-02-03 23:51:17,161 [nnabla][INFO]: iter=7799 {Training error}=0.003125\n",
            "2023-02-03 23:51:17,162 [nnabla][INFO]: iter=7799 {Training time}=0.3545722961425781[sec/100iter] 27.534074783325195[sec]\n",
            "2023-02-03 23:51:17,178 [nnabla][INFO]: iter=7800 {Test error}=0.01015625\n",
            "2023-02-03 23:51:17,212 [nnabla][INFO]: iter=7809 {Training loss}=0.018554210662841797\n",
            "2023-02-03 23:51:17,212 [nnabla][INFO]: iter=7809 {Training error}=0.00703125\n",
            "2023-02-03 23:51:17,245 [nnabla][INFO]: iter=7819 {Training loss}=0.019036592915654182\n",
            "2023-02-03 23:51:17,245 [nnabla][INFO]: iter=7819 {Training error}=0.0046875\n",
            "2023-02-03 23:51:17,278 [nnabla][INFO]: iter=7829 {Training loss}=0.01148967258632183\n",
            "2023-02-03 23:51:17,278 [nnabla][INFO]: iter=7829 {Training error}=0.003125\n",
            "2023-02-03 23:51:17,316 [nnabla][INFO]: iter=7839 {Training loss}=0.01037698332220316\n",
            "2023-02-03 23:51:17,316 [nnabla][INFO]: iter=7839 {Training error}=0.00546875\n",
            "2023-02-03 23:51:17,350 [nnabla][INFO]: iter=7849 {Training loss}=0.011010222136974335\n",
            "2023-02-03 23:51:17,350 [nnabla][INFO]: iter=7849 {Training error}=0.0046875\n",
            "2023-02-03 23:51:17,383 [nnabla][INFO]: iter=7859 {Training loss}=0.016860995441675186\n",
            "2023-02-03 23:51:17,384 [nnabla][INFO]: iter=7859 {Training error}=0.0046875\n",
            "2023-02-03 23:51:17,418 [nnabla][INFO]: iter=7869 {Training loss}=0.029504239559173584\n",
            "2023-02-03 23:51:17,418 [nnabla][INFO]: iter=7869 {Training error}=0.00625\n",
            "2023-02-03 23:51:17,451 [nnabla][INFO]: iter=7879 {Training loss}=0.013906130567193031\n",
            "2023-02-03 23:51:17,451 [nnabla][INFO]: iter=7879 {Training error}=0.00546875\n",
            "2023-02-03 23:51:17,484 [nnabla][INFO]: iter=7889 {Training loss}=0.011836111545562744\n",
            "2023-02-03 23:51:17,485 [nnabla][INFO]: iter=7889 {Training error}=0.003125\n",
            "2023-02-03 23:51:17,519 [nnabla][INFO]: iter=7899 {Training loss}=0.017274918034672737\n",
            "2023-02-03 23:51:17,519 [nnabla][INFO]: iter=7899 {Training error}=0.00703125\n",
            "2023-02-03 23:51:17,519 [nnabla][INFO]: iter=7899 {Training time}=0.35729479789733887[sec/100iter] 27.891369581222534[sec]\n",
            "2023-02-03 23:51:17,535 [nnabla][INFO]: iter=7900 {Test error}=0.0078125\n",
            "2023-02-03 23:51:17,569 [nnabla][INFO]: iter=7909 {Training loss}=0.013577188365161419\n",
            "2023-02-03 23:51:17,569 [nnabla][INFO]: iter=7909 {Training error}=0.003125\n",
            "2023-02-03 23:51:17,605 [nnabla][INFO]: iter=7919 {Training loss}=0.011836977675557137\n",
            "2023-02-03 23:51:17,605 [nnabla][INFO]: iter=7919 {Training error}=0.003125\n",
            "2023-02-03 23:51:17,639 [nnabla][INFO]: iter=7929 {Training loss}=0.011235550045967102\n",
            "2023-02-03 23:51:17,639 [nnabla][INFO]: iter=7929 {Training error}=0.00625\n",
            "2023-02-03 23:51:17,673 [nnabla][INFO]: iter=7939 {Training loss}=0.021796520799398422\n",
            "2023-02-03 23:51:17,674 [nnabla][INFO]: iter=7939 {Training error}=0.00390625\n",
            "2023-02-03 23:51:17,710 [nnabla][INFO]: iter=7949 {Training loss}=0.013577121309936047\n",
            "2023-02-03 23:51:17,710 [nnabla][INFO]: iter=7949 {Training error}=0.0046875\n",
            "2023-02-03 23:51:17,749 [nnabla][INFO]: iter=7959 {Training loss}=0.015282981097698212\n",
            "2023-02-03 23:51:17,750 [nnabla][INFO]: iter=7959 {Training error}=0.00625\n",
            "2023-02-03 23:51:17,785 [nnabla][INFO]: iter=7969 {Training loss}=0.011895482428371906\n",
            "2023-02-03 23:51:17,785 [nnabla][INFO]: iter=7969 {Training error}=0.0046875\n",
            "2023-02-03 23:51:17,822 [nnabla][INFO]: iter=7979 {Training loss}=0.007404258940368891\n",
            "2023-02-03 23:51:17,822 [nnabla][INFO]: iter=7979 {Training error}=0.00078125\n",
            "2023-02-03 23:51:17,859 [nnabla][INFO]: iter=7989 {Training loss}=0.0019109547138214111\n",
            "2023-02-03 23:51:17,860 [nnabla][INFO]: iter=7989 {Training error}=0.0\n",
            "2023-02-03 23:51:17,897 [nnabla][INFO]: iter=7999 {Training loss}=0.004145983140915632\n",
            "2023-02-03 23:51:17,898 [nnabla][INFO]: iter=7999 {Training error}=0.00078125\n",
            "2023-02-03 23:51:17,898 [nnabla][INFO]: iter=7999 {Training time}=0.379331111907959[sec/100iter] 28.270700693130493[sec]\n",
            "2023-02-03 23:51:17,919 [nnabla][INFO]: iter=8000 {Test error}=0.01171875\n",
            "2023-02-03 23:51:17,946 [nnabla][INFO]: Solver state save (.h5): output/states_8000.h5\n",
            "2023-02-03 23:51:17,954 [nnabla][INFO]: Parameter save (.h5): output/params_8000.h5\n",
            "2023-02-03 23:51:17,954 [nnabla][INFO]: Checkpoint save (.json): output/checkpoint_8000.json\n",
            "2023-02-03 23:51:17,981 [nnabla][INFO]: iter=8009 {Training loss}=0.0032451110891997814\n",
            "2023-02-03 23:51:17,981 [nnabla][INFO]: iter=8009 {Training error}=0.00078125\n",
            "2023-02-03 23:51:18,008 [nnabla][INFO]: iter=8019 {Training loss}=0.008753439411520958\n",
            "2023-02-03 23:51:18,008 [nnabla][INFO]: iter=8019 {Training error}=0.003125\n",
            "2023-02-03 23:51:18,035 [nnabla][INFO]: iter=8029 {Training loss}=0.017592718824744225\n",
            "2023-02-03 23:51:18,035 [nnabla][INFO]: iter=8029 {Training error}=0.003125\n",
            "2023-02-03 23:51:18,063 [nnabla][INFO]: iter=8039 {Training loss}=0.007656601257622242\n",
            "2023-02-03 23:51:18,063 [nnabla][INFO]: iter=8039 {Training error}=0.00234375\n",
            "2023-02-03 23:51:18,091 [nnabla][INFO]: iter=8049 {Training loss}=0.012187065556645393\n",
            "2023-02-03 23:51:18,091 [nnabla][INFO]: iter=8049 {Training error}=0.0046875\n",
            "2023-02-03 23:51:18,118 [nnabla][INFO]: iter=8059 {Training loss}=0.004605940077453852\n",
            "2023-02-03 23:51:18,118 [nnabla][INFO]: iter=8059 {Training error}=0.00078125\n",
            "2023-02-03 23:51:18,144 [nnabla][INFO]: iter=8069 {Training loss}=0.011194193735718727\n",
            "2023-02-03 23:51:18,145 [nnabla][INFO]: iter=8069 {Training error}=0.00234375\n",
            "2023-02-03 23:51:18,171 [nnabla][INFO]: iter=8079 {Training loss}=0.007060054689645767\n",
            "2023-02-03 23:51:18,171 [nnabla][INFO]: iter=8079 {Training error}=0.00078125\n",
            "2023-02-03 23:51:18,199 [nnabla][INFO]: iter=8089 {Training loss}=0.0077089592814445496\n",
            "2023-02-03 23:51:18,200 [nnabla][INFO]: iter=8089 {Training error}=0.00234375\n",
            "2023-02-03 23:51:18,227 [nnabla][INFO]: iter=8099 {Training loss}=0.010229149833321571\n",
            "2023-02-03 23:51:18,228 [nnabla][INFO]: iter=8099 {Training error}=0.00390625\n",
            "2023-02-03 23:51:18,228 [nnabla][INFO]: iter=8099 {Training time}=0.3294510841369629[sec/100iter] 28.600151777267456[sec]\n",
            "2023-02-03 23:51:18,238 [nnabla][INFO]: iter=8100 {Test error}=0.00625\n",
            "2023-02-03 23:51:18,266 [nnabla][INFO]: iter=8109 {Training loss}=0.008307641372084618\n",
            "2023-02-03 23:51:18,266 [nnabla][INFO]: iter=8109 {Training error}=0.00234375\n",
            "2023-02-03 23:51:18,293 [nnabla][INFO]: iter=8119 {Training loss}=0.015553457662463188\n",
            "2023-02-03 23:51:18,294 [nnabla][INFO]: iter=8119 {Training error}=0.00703125\n",
            "2023-02-03 23:51:18,320 [nnabla][INFO]: iter=8129 {Training loss}=0.01422387920320034\n",
            "2023-02-03 23:51:18,320 [nnabla][INFO]: iter=8129 {Training error}=0.0046875\n",
            "2023-02-03 23:51:18,348 [nnabla][INFO]: iter=8139 {Training loss}=0.015557071194052696\n",
            "2023-02-03 23:51:18,348 [nnabla][INFO]: iter=8139 {Training error}=0.00625\n",
            "2023-02-03 23:51:18,374 [nnabla][INFO]: iter=8149 {Training loss}=0.004137490876019001\n",
            "2023-02-03 23:51:18,374 [nnabla][INFO]: iter=8149 {Training error}=0.00234375\n",
            "2023-02-03 23:51:18,401 [nnabla][INFO]: iter=8159 {Training loss}=0.014871461316943169\n",
            "2023-02-03 23:51:18,402 [nnabla][INFO]: iter=8159 {Training error}=0.00625\n",
            "2023-02-03 23:51:18,428 [nnabla][INFO]: iter=8169 {Training loss}=0.023238103836774826\n",
            "2023-02-03 23:51:18,428 [nnabla][INFO]: iter=8169 {Training error}=0.0046875\n",
            "2023-02-03 23:51:18,455 [nnabla][INFO]: iter=8179 {Training loss}=0.011950443498790264\n",
            "2023-02-03 23:51:18,455 [nnabla][INFO]: iter=8179 {Training error}=0.0046875\n",
            "2023-02-03 23:51:18,482 [nnabla][INFO]: iter=8189 {Training loss}=0.006148912012577057\n",
            "2023-02-03 23:51:18,482 [nnabla][INFO]: iter=8189 {Training error}=0.00234375\n",
            "2023-02-03 23:51:18,512 [nnabla][INFO]: iter=8199 {Training loss}=0.014451691880822182\n",
            "2023-02-03 23:51:18,512 [nnabla][INFO]: iter=8199 {Training error}=0.00703125\n",
            "2023-02-03 23:51:18,512 [nnabla][INFO]: iter=8199 {Training time}=0.28428101539611816[sec/100iter] 28.884432792663574[sec]\n",
            "2023-02-03 23:51:18,522 [nnabla][INFO]: iter=8200 {Test error}=0.00625\n",
            "2023-02-03 23:51:18,549 [nnabla][INFO]: iter=8209 {Training loss}=0.02090093493461609\n",
            "2023-02-03 23:51:18,549 [nnabla][INFO]: iter=8209 {Training error}=0.00546875\n",
            "2023-02-03 23:51:18,576 [nnabla][INFO]: iter=8219 {Training loss}=0.015075162053108215\n",
            "2023-02-03 23:51:18,576 [nnabla][INFO]: iter=8219 {Training error}=0.00546875\n",
            "2023-02-03 23:51:18,603 [nnabla][INFO]: iter=8229 {Training loss}=0.012003155425190926\n",
            "2023-02-03 23:51:18,603 [nnabla][INFO]: iter=8229 {Training error}=0.00546875\n",
            "2023-02-03 23:51:18,630 [nnabla][INFO]: iter=8239 {Training loss}=0.011923031881451607\n",
            "2023-02-03 23:51:18,630 [nnabla][INFO]: iter=8239 {Training error}=0.003125\n",
            "2023-02-03 23:51:18,657 [nnabla][INFO]: iter=8249 {Training loss}=0.013217106461524963\n",
            "2023-02-03 23:51:18,657 [nnabla][INFO]: iter=8249 {Training error}=0.0046875\n",
            "2023-02-03 23:51:18,683 [nnabla][INFO]: iter=8259 {Training loss}=0.008696911856532097\n",
            "2023-02-03 23:51:18,684 [nnabla][INFO]: iter=8259 {Training error}=0.003125\n",
            "2023-02-03 23:51:18,710 [nnabla][INFO]: iter=8269 {Training loss}=0.010992033407092094\n",
            "2023-02-03 23:51:18,710 [nnabla][INFO]: iter=8269 {Training error}=0.0046875\n",
            "2023-02-03 23:51:18,741 [nnabla][INFO]: iter=8279 {Training loss}=0.010586290620267391\n",
            "2023-02-03 23:51:18,741 [nnabla][INFO]: iter=8279 {Training error}=0.0046875\n",
            "2023-02-03 23:51:18,770 [nnabla][INFO]: iter=8289 {Training loss}=0.009626876562833786\n",
            "2023-02-03 23:51:18,771 [nnabla][INFO]: iter=8289 {Training error}=0.003125\n",
            "2023-02-03 23:51:18,798 [nnabla][INFO]: iter=8299 {Training loss}=0.008252007886767387\n",
            "2023-02-03 23:51:18,798 [nnabla][INFO]: iter=8299 {Training error}=0.00234375\n",
            "2023-02-03 23:51:18,799 [nnabla][INFO]: iter=8299 {Training time}=0.2871711254119873[sec/100iter] 29.17160391807556[sec]\n",
            "2023-02-03 23:51:18,813 [nnabla][INFO]: iter=8300 {Test error}=0.0109375\n",
            "2023-02-03 23:51:18,843 [nnabla][INFO]: iter=8309 {Training loss}=0.015110594220459461\n",
            "2023-02-03 23:51:18,843 [nnabla][INFO]: iter=8309 {Training error}=0.00546875\n",
            "2023-02-03 23:51:18,870 [nnabla][INFO]: iter=8319 {Training loss}=0.009886701591312885\n",
            "2023-02-03 23:51:18,871 [nnabla][INFO]: iter=8319 {Training error}=0.0046875\n",
            "2023-02-03 23:51:18,898 [nnabla][INFO]: iter=8329 {Training loss}=0.00870200339704752\n",
            "2023-02-03 23:51:18,898 [nnabla][INFO]: iter=8329 {Training error}=0.00234375\n",
            "2023-02-03 23:51:18,925 [nnabla][INFO]: iter=8339 {Training loss}=0.008116871118545532\n",
            "2023-02-03 23:51:18,926 [nnabla][INFO]: iter=8339 {Training error}=0.00234375\n",
            "2023-02-03 23:51:18,953 [nnabla][INFO]: iter=8349 {Training loss}=0.013354817405343056\n",
            "2023-02-03 23:51:18,953 [nnabla][INFO]: iter=8349 {Training error}=0.00546875\n",
            "2023-02-03 23:51:18,981 [nnabla][INFO]: iter=8359 {Training loss}=0.008145269006490707\n",
            "2023-02-03 23:51:18,981 [nnabla][INFO]: iter=8359 {Training error}=0.00390625\n",
            "2023-02-03 23:51:19,009 [nnabla][INFO]: iter=8369 {Training loss}=0.006664294749498367\n",
            "2023-02-03 23:51:19,010 [nnabla][INFO]: iter=8369 {Training error}=0.00234375\n",
            "2023-02-03 23:51:19,038 [nnabla][INFO]: iter=8379 {Training loss}=0.017341678962111473\n",
            "2023-02-03 23:51:19,038 [nnabla][INFO]: iter=8379 {Training error}=0.0078125\n",
            "2023-02-03 23:51:19,067 [nnabla][INFO]: iter=8389 {Training loss}=0.01038593240082264\n",
            "2023-02-03 23:51:19,068 [nnabla][INFO]: iter=8389 {Training error}=0.00390625\n",
            "2023-02-03 23:51:19,096 [nnabla][INFO]: iter=8399 {Training loss}=0.011742165312170982\n",
            "2023-02-03 23:51:19,097 [nnabla][INFO]: iter=8399 {Training error}=0.00390625\n",
            "2023-02-03 23:51:19,097 [nnabla][INFO]: iter=8399 {Training time}=0.2975728511810303[sec/100iter] 29.469176769256592[sec]\n",
            "2023-02-03 23:51:19,114 [nnabla][INFO]: iter=8400 {Test error}=0.00859375\n",
            "2023-02-03 23:51:19,143 [nnabla][INFO]: iter=8409 {Training loss}=0.009481528773903847\n",
            "2023-02-03 23:51:19,143 [nnabla][INFO]: iter=8409 {Training error}=0.0046875\n",
            "2023-02-03 23:51:19,170 [nnabla][INFO]: iter=8419 {Training loss}=0.009529012255370617\n",
            "2023-02-03 23:51:19,171 [nnabla][INFO]: iter=8419 {Training error}=0.00390625\n",
            "2023-02-03 23:51:19,201 [nnabla][INFO]: iter=8429 {Training loss}=0.013848967850208282\n",
            "2023-02-03 23:51:19,201 [nnabla][INFO]: iter=8429 {Training error}=0.00625\n",
            "2023-02-03 23:51:19,230 [nnabla][INFO]: iter=8439 {Training loss}=0.027661491185426712\n",
            "2023-02-03 23:51:19,230 [nnabla][INFO]: iter=8439 {Training error}=0.0046875\n",
            "2023-02-03 23:51:19,258 [nnabla][INFO]: iter=8449 {Training loss}=0.006174130365252495\n",
            "2023-02-03 23:51:19,258 [nnabla][INFO]: iter=8449 {Training error}=0.00390625\n",
            "2023-02-03 23:51:19,285 [nnabla][INFO]: iter=8459 {Training loss}=0.01192126888781786\n",
            "2023-02-03 23:51:19,286 [nnabla][INFO]: iter=8459 {Training error}=0.0015625\n",
            "2023-02-03 23:51:19,314 [nnabla][INFO]: iter=8469 {Training loss}=0.01172521710395813\n",
            "2023-02-03 23:51:19,314 [nnabla][INFO]: iter=8469 {Training error}=0.00390625\n",
            "2023-02-03 23:51:19,342 [nnabla][INFO]: iter=8479 {Training loss}=0.011218932457268238\n",
            "2023-02-03 23:51:19,342 [nnabla][INFO]: iter=8479 {Training error}=0.00390625\n",
            "2023-02-03 23:51:19,371 [nnabla][INFO]: iter=8489 {Training loss}=0.007701592054218054\n",
            "2023-02-03 23:51:19,371 [nnabla][INFO]: iter=8489 {Training error}=0.003125\n",
            "2023-02-03 23:51:19,398 [nnabla][INFO]: iter=8499 {Training loss}=0.009855407290160656\n",
            "2023-02-03 23:51:19,399 [nnabla][INFO]: iter=8499 {Training error}=0.00234375\n",
            "2023-02-03 23:51:19,399 [nnabla][INFO]: iter=8499 {Training time}=0.302048921585083[sec/100iter] 29.771225690841675[sec]\n",
            "2023-02-03 23:51:19,414 [nnabla][INFO]: iter=8500 {Test error}=0.015625\n",
            "2023-02-03 23:51:19,442 [nnabla][INFO]: iter=8509 {Training loss}=0.008820204064249992\n",
            "2023-02-03 23:51:19,442 [nnabla][INFO]: iter=8509 {Training error}=0.00390625\n",
            "2023-02-03 23:51:19,469 [nnabla][INFO]: iter=8519 {Training loss}=0.003410967066884041\n",
            "2023-02-03 23:51:19,469 [nnabla][INFO]: iter=8519 {Training error}=0.0\n",
            "2023-02-03 23:51:19,496 [nnabla][INFO]: iter=8529 {Training loss}=0.006503920070827007\n",
            "2023-02-03 23:51:19,497 [nnabla][INFO]: iter=8529 {Training error}=0.003125\n",
            "2023-02-03 23:51:19,523 [nnabla][INFO]: iter=8539 {Training loss}=0.007987214252352715\n",
            "2023-02-03 23:51:19,524 [nnabla][INFO]: iter=8539 {Training error}=0.00234375\n",
            "2023-02-03 23:51:19,555 [nnabla][INFO]: iter=8549 {Training loss}=0.011041460558772087\n",
            "2023-02-03 23:51:19,555 [nnabla][INFO]: iter=8549 {Training error}=0.00234375\n",
            "2023-02-03 23:51:19,581 [nnabla][INFO]: iter=8559 {Training loss}=0.007314515765756369\n",
            "2023-02-03 23:51:19,582 [nnabla][INFO]: iter=8559 {Training error}=0.0015625\n",
            "2023-02-03 23:51:19,610 [nnabla][INFO]: iter=8569 {Training loss}=0.004328405950218439\n",
            "2023-02-03 23:51:19,610 [nnabla][INFO]: iter=8569 {Training error}=0.0015625\n",
            "2023-02-03 23:51:19,638 [nnabla][INFO]: iter=8579 {Training loss}=0.004599482286721468\n",
            "2023-02-03 23:51:19,638 [nnabla][INFO]: iter=8579 {Training error}=0.0015625\n",
            "2023-02-03 23:51:19,666 [nnabla][INFO]: iter=8589 {Training loss}=0.00784655474126339\n",
            "2023-02-03 23:51:19,666 [nnabla][INFO]: iter=8589 {Training error}=0.003125\n",
            "2023-02-03 23:51:19,694 [nnabla][INFO]: iter=8599 {Training loss}=0.013618391938507557\n",
            "2023-02-03 23:51:19,694 [nnabla][INFO]: iter=8599 {Training error}=0.00390625\n",
            "2023-02-03 23:51:19,694 [nnabla][INFO]: iter=8599 {Training time}=0.29529404640197754[sec/100iter] 30.066519737243652[sec]\n",
            "2023-02-03 23:51:19,706 [nnabla][INFO]: iter=8600 {Test error}=0.00859375\n",
            "2023-02-03 23:51:19,733 [nnabla][INFO]: iter=8609 {Training loss}=0.006749191787093878\n",
            "2023-02-03 23:51:19,733 [nnabla][INFO]: iter=8609 {Training error}=0.00234375\n",
            "2023-02-03 23:51:19,766 [nnabla][INFO]: iter=8619 {Training loss}=0.007790146861225367\n",
            "2023-02-03 23:51:19,766 [nnabla][INFO]: iter=8619 {Training error}=0.0015625\n",
            "2023-02-03 23:51:19,794 [nnabla][INFO]: iter=8629 {Training loss}=0.01783987507224083\n",
            "2023-02-03 23:51:19,794 [nnabla][INFO]: iter=8629 {Training error}=0.00625\n",
            "2023-02-03 23:51:19,821 [nnabla][INFO]: iter=8639 {Training loss}=0.00843420997262001\n",
            "2023-02-03 23:51:19,822 [nnabla][INFO]: iter=8639 {Training error}=0.0015625\n",
            "2023-02-03 23:51:19,848 [nnabla][INFO]: iter=8649 {Training loss}=0.008346671238541603\n",
            "2023-02-03 23:51:19,849 [nnabla][INFO]: iter=8649 {Training error}=0.00390625\n",
            "2023-02-03 23:51:19,875 [nnabla][INFO]: iter=8659 {Training loss}=0.01077869813889265\n",
            "2023-02-03 23:51:19,875 [nnabla][INFO]: iter=8659 {Training error}=0.00234375\n",
            "2023-02-03 23:51:19,906 [nnabla][INFO]: iter=8669 {Training loss}=0.005281432997435331\n",
            "2023-02-03 23:51:19,906 [nnabla][INFO]: iter=8669 {Training error}=0.0015625\n",
            "2023-02-03 23:51:19,933 [nnabla][INFO]: iter=8679 {Training loss}=0.00853011105209589\n",
            "2023-02-03 23:51:19,933 [nnabla][INFO]: iter=8679 {Training error}=0.00234375\n",
            "2023-02-03 23:51:19,961 [nnabla][INFO]: iter=8689 {Training loss}=0.004903244320303202\n",
            "2023-02-03 23:51:19,961 [nnabla][INFO]: iter=8689 {Training error}=0.0015625\n",
            "2023-02-03 23:51:19,987 [nnabla][INFO]: iter=8699 {Training loss}=0.0070778727531433105\n",
            "2023-02-03 23:51:19,988 [nnabla][INFO]: iter=8699 {Training error}=0.00234375\n",
            "2023-02-03 23:51:19,988 [nnabla][INFO]: iter=8699 {Training time}=0.2936217784881592[sec/100iter] 30.36014151573181[sec]\n",
            "2023-02-03 23:51:19,998 [nnabla][INFO]: iter=8700 {Test error}=0.0078125\n",
            "2023-02-03 23:51:20,026 [nnabla][INFO]: iter=8709 {Training loss}=0.0063034286722540855\n",
            "2023-02-03 23:51:20,026 [nnabla][INFO]: iter=8709 {Training error}=0.00234375\n",
            "2023-02-03 23:51:20,053 [nnabla][INFO]: iter=8719 {Training loss}=0.004987807013094425\n",
            "2023-02-03 23:51:20,053 [nnabla][INFO]: iter=8719 {Training error}=0.0015625\n",
            "2023-02-03 23:51:20,080 [nnabla][INFO]: iter=8729 {Training loss}=0.007741904817521572\n",
            "2023-02-03 23:51:20,080 [nnabla][INFO]: iter=8729 {Training error}=0.0046875\n",
            "2023-02-03 23:51:20,107 [nnabla][INFO]: iter=8739 {Training loss}=0.006607032380998135\n",
            "2023-02-03 23:51:20,107 [nnabla][INFO]: iter=8739 {Training error}=0.00234375\n",
            "2023-02-03 23:51:20,134 [nnabla][INFO]: iter=8749 {Training loss}=0.007493139710277319\n",
            "2023-02-03 23:51:20,134 [nnabla][INFO]: iter=8749 {Training error}=0.00234375\n",
            "2023-02-03 23:51:20,163 [nnabla][INFO]: iter=8759 {Training loss}=0.010830102488398552\n",
            "2023-02-03 23:51:20,163 [nnabla][INFO]: iter=8759 {Training error}=0.0046875\n",
            "2023-02-03 23:51:20,189 [nnabla][INFO]: iter=8769 {Training loss}=0.006754292640835047\n",
            "2023-02-03 23:51:20,190 [nnabla][INFO]: iter=8769 {Training error}=0.00234375\n",
            "2023-02-03 23:51:20,218 [nnabla][INFO]: iter=8779 {Training loss}=0.008124561049044132\n",
            "2023-02-03 23:51:20,218 [nnabla][INFO]: iter=8779 {Training error}=0.00234375\n",
            "2023-02-03 23:51:20,245 [nnabla][INFO]: iter=8789 {Training loss}=0.013177623972296715\n",
            "2023-02-03 23:51:20,245 [nnabla][INFO]: iter=8789 {Training error}=0.00546875\n",
            "2023-02-03 23:51:20,272 [nnabla][INFO]: iter=8799 {Training loss}=0.012786817736923695\n",
            "2023-02-03 23:51:20,272 [nnabla][INFO]: iter=8799 {Training error}=0.0046875\n",
            "2023-02-03 23:51:20,272 [nnabla][INFO]: iter=8799 {Training time}=0.28462815284729004[sec/100iter] 30.6447696685791[sec]\n",
            "2023-02-03 23:51:20,288 [nnabla][INFO]: iter=8800 {Test error}=0.009375\n",
            "2023-02-03 23:51:20,316 [nnabla][INFO]: iter=8809 {Training loss}=0.012414010241627693\n",
            "2023-02-03 23:51:20,316 [nnabla][INFO]: iter=8809 {Training error}=0.00625\n",
            "2023-02-03 23:51:20,343 [nnabla][INFO]: iter=8819 {Training loss}=0.01047308836132288\n",
            "2023-02-03 23:51:20,343 [nnabla][INFO]: iter=8819 {Training error}=0.003125\n",
            "2023-02-03 23:51:20,370 [nnabla][INFO]: iter=8829 {Training loss}=0.010135394521057606\n",
            "2023-02-03 23:51:20,370 [nnabla][INFO]: iter=8829 {Training error}=0.0046875\n",
            "2023-02-03 23:51:20,396 [nnabla][INFO]: iter=8839 {Training loss}=0.0065883733332157135\n",
            "2023-02-03 23:51:20,397 [nnabla][INFO]: iter=8839 {Training error}=0.00234375\n",
            "2023-02-03 23:51:20,423 [nnabla][INFO]: iter=8849 {Training loss}=0.019220083951950073\n",
            "2023-02-03 23:51:20,424 [nnabla][INFO]: iter=8849 {Training error}=0.00703125\n",
            "2023-02-03 23:51:20,450 [nnabla][INFO]: iter=8859 {Training loss}=0.029391998425126076\n",
            "2023-02-03 23:51:20,450 [nnabla][INFO]: iter=8859 {Training error}=0.00546875\n",
            "2023-02-03 23:51:20,480 [nnabla][INFO]: iter=8869 {Training loss}=0.00831963587552309\n",
            "2023-02-03 23:51:20,480 [nnabla][INFO]: iter=8869 {Training error}=0.00234375\n",
            "2023-02-03 23:51:20,508 [nnabla][INFO]: iter=8879 {Training loss}=0.010692751966416836\n",
            "2023-02-03 23:51:20,509 [nnabla][INFO]: iter=8879 {Training error}=0.0046875\n",
            "2023-02-03 23:51:20,537 [nnabla][INFO]: iter=8889 {Training loss}=0.015397639945149422\n",
            "2023-02-03 23:51:20,537 [nnabla][INFO]: iter=8889 {Training error}=0.00625\n",
            "2023-02-03 23:51:20,564 [nnabla][INFO]: iter=8899 {Training loss}=0.019624512642621994\n",
            "2023-02-03 23:51:20,564 [nnabla][INFO]: iter=8899 {Training error}=0.003125\n",
            "2023-02-03 23:51:20,564 [nnabla][INFO]: iter=8899 {Training time}=0.2918870449066162[sec/100iter] 30.936656713485718[sec]\n",
            "2023-02-03 23:51:20,575 [nnabla][INFO]: iter=8900 {Test error}=0.0109375\n",
            "2023-02-03 23:51:20,602 [nnabla][INFO]: iter=8909 {Training loss}=0.008026884868741035\n",
            "2023-02-03 23:51:20,603 [nnabla][INFO]: iter=8909 {Training error}=0.00234375\n",
            "2023-02-03 23:51:20,629 [nnabla][INFO]: iter=8919 {Training loss}=0.017100263386964798\n",
            "2023-02-03 23:51:20,629 [nnabla][INFO]: iter=8919 {Training error}=0.0046875\n",
            "2023-02-03 23:51:20,656 [nnabla][INFO]: iter=8929 {Training loss}=0.005836685188114643\n",
            "2023-02-03 23:51:20,656 [nnabla][INFO]: iter=8929 {Training error}=0.00234375\n",
            "2023-02-03 23:51:20,683 [nnabla][INFO]: iter=8939 {Training loss}=0.005667545832693577\n",
            "2023-02-03 23:51:20,683 [nnabla][INFO]: iter=8939 {Training error}=0.0015625\n",
            "2023-02-03 23:51:20,710 [nnabla][INFO]: iter=8949 {Training loss}=0.012464835308492184\n",
            "2023-02-03 23:51:20,710 [nnabla][INFO]: iter=8949 {Training error}=0.00625\n",
            "2023-02-03 23:51:20,737 [nnabla][INFO]: iter=8959 {Training loss}=0.019218409433960915\n",
            "2023-02-03 23:51:20,737 [nnabla][INFO]: iter=8959 {Training error}=0.00859375\n",
            "2023-02-03 23:51:20,765 [nnabla][INFO]: iter=8969 {Training loss}=0.017752423882484436\n",
            "2023-02-03 23:51:20,765 [nnabla][INFO]: iter=8969 {Training error}=0.00390625\n",
            "2023-02-03 23:51:20,798 [nnabla][INFO]: iter=8979 {Training loss}=0.014511769637465477\n",
            "2023-02-03 23:51:20,798 [nnabla][INFO]: iter=8979 {Training error}=0.00546875\n",
            "2023-02-03 23:51:20,827 [nnabla][INFO]: iter=8989 {Training loss}=0.010042245499789715\n",
            "2023-02-03 23:51:20,827 [nnabla][INFO]: iter=8989 {Training error}=0.00390625\n",
            "2023-02-03 23:51:20,854 [nnabla][INFO]: iter=8999 {Training loss}=0.004568675998598337\n",
            "2023-02-03 23:51:20,854 [nnabla][INFO]: iter=8999 {Training error}=0.0015625\n",
            "2023-02-03 23:51:20,854 [nnabla][INFO]: iter=8999 {Training time}=0.29006075859069824[sec/100iter] 31.226717472076416[sec]\n",
            "2023-02-03 23:51:20,866 [nnabla][INFO]: iter=9000 {Test error}=0.00703125\n",
            "2023-02-03 23:51:20,881 [nnabla][INFO]: Solver state save (.h5): output/states_9000.h5\n",
            "2023-02-03 23:51:20,888 [nnabla][INFO]: Parameter save (.h5): output/params_9000.h5\n",
            "2023-02-03 23:51:20,888 [nnabla][INFO]: Checkpoint save (.json): output/checkpoint_9000.json\n",
            "2023-02-03 23:51:20,915 [nnabla][INFO]: iter=9009 {Training loss}=0.004136070609092712\n",
            "2023-02-03 23:51:20,915 [nnabla][INFO]: iter=9009 {Training error}=0.0\n",
            "2023-02-03 23:51:20,943 [nnabla][INFO]: iter=9019 {Training loss}=0.011443917639553547\n",
            "2023-02-03 23:51:20,943 [nnabla][INFO]: iter=9019 {Training error}=0.003125\n",
            "2023-02-03 23:51:20,970 [nnabla][INFO]: iter=9029 {Training loss}=0.01375250518321991\n",
            "2023-02-03 23:51:20,971 [nnabla][INFO]: iter=9029 {Training error}=0.00546875\n",
            "2023-02-03 23:51:20,998 [nnabla][INFO]: iter=9039 {Training loss}=0.009295173920691013\n",
            "2023-02-03 23:51:20,998 [nnabla][INFO]: iter=9039 {Training error}=0.003125\n",
            "2023-02-03 23:51:21,025 [nnabla][INFO]: iter=9049 {Training loss}=0.006376571953296661\n",
            "2023-02-03 23:51:21,025 [nnabla][INFO]: iter=9049 {Training error}=0.00078125\n",
            "2023-02-03 23:51:21,053 [nnabla][INFO]: iter=9059 {Training loss}=0.0057484968565404415\n",
            "2023-02-03 23:51:21,053 [nnabla][INFO]: iter=9059 {Training error}=0.00078125\n",
            "2023-02-03 23:51:21,080 [nnabla][INFO]: iter=9069 {Training loss}=0.005888818297535181\n",
            "2023-02-03 23:51:21,080 [nnabla][INFO]: iter=9069 {Training error}=0.0015625\n",
            "2023-02-03 23:51:21,108 [nnabla][INFO]: iter=9079 {Training loss}=0.007002684287726879\n",
            "2023-02-03 23:51:21,108 [nnabla][INFO]: iter=9079 {Training error}=0.0015625\n",
            "2023-02-03 23:51:21,139 [nnabla][INFO]: iter=9089 {Training loss}=0.005503615364432335\n",
            "2023-02-03 23:51:21,139 [nnabla][INFO]: iter=9089 {Training error}=0.0015625\n",
            "2023-02-03 23:51:21,166 [nnabla][INFO]: iter=9099 {Training loss}=0.0118859326466918\n",
            "2023-02-03 23:51:21,166 [nnabla][INFO]: iter=9099 {Training error}=0.00390625\n",
            "2023-02-03 23:51:21,166 [nnabla][INFO]: iter=9099 {Training time}=0.31168627738952637[sec/100iter] 31.538403749465942[sec]\n",
            "2023-02-03 23:51:21,177 [nnabla][INFO]: iter=9100 {Test error}=0.0078125\n",
            "2023-02-03 23:51:21,203 [nnabla][INFO]: iter=9109 {Training loss}=0.008561124093830585\n",
            "2023-02-03 23:51:21,203 [nnabla][INFO]: iter=9109 {Training error}=0.00390625\n",
            "2023-02-03 23:51:21,230 [nnabla][INFO]: iter=9119 {Training loss}=0.009142259135842323\n",
            "2023-02-03 23:51:21,230 [nnabla][INFO]: iter=9119 {Training error}=0.00390625\n",
            "2023-02-03 23:51:21,257 [nnabla][INFO]: iter=9129 {Training loss}=0.006773221306502819\n",
            "2023-02-03 23:51:21,257 [nnabla][INFO]: iter=9129 {Training error}=0.003125\n",
            "2023-02-03 23:51:21,284 [nnabla][INFO]: iter=9139 {Training loss}=0.002348764333873987\n",
            "2023-02-03 23:51:21,284 [nnabla][INFO]: iter=9139 {Training error}=0.00078125\n",
            "2023-02-03 23:51:21,311 [nnabla][INFO]: iter=9149 {Training loss}=0.004589739255607128\n",
            "2023-02-03 23:51:21,311 [nnabla][INFO]: iter=9149 {Training error}=0.00078125\n",
            "2023-02-03 23:51:21,337 [nnabla][INFO]: iter=9159 {Training loss}=0.0032056078780442476\n",
            "2023-02-03 23:51:21,338 [nnabla][INFO]: iter=9159 {Training error}=0.0\n",
            "2023-02-03 23:51:21,364 [nnabla][INFO]: iter=9169 {Training loss}=0.009772907942533493\n",
            "2023-02-03 23:51:21,365 [nnabla][INFO]: iter=9169 {Training error}=0.003125\n",
            "2023-02-03 23:51:21,391 [nnabla][INFO]: iter=9179 {Training loss}=0.00662247184664011\n",
            "2023-02-03 23:51:21,391 [nnabla][INFO]: iter=9179 {Training error}=0.00234375\n",
            "2023-02-03 23:51:21,419 [nnabla][INFO]: iter=9189 {Training loss}=0.006054302677512169\n",
            "2023-02-03 23:51:21,420 [nnabla][INFO]: iter=9189 {Training error}=0.00078125\n",
            "2023-02-03 23:51:21,446 [nnabla][INFO]: iter=9199 {Training loss}=0.012028433382511139\n",
            "2023-02-03 23:51:21,446 [nnabla][INFO]: iter=9199 {Training error}=0.00546875\n",
            "2023-02-03 23:51:21,446 [nnabla][INFO]: iter=9199 {Training time}=0.2805516719818115[sec/100iter] 31.818955421447754[sec]\n",
            "2023-02-03 23:51:21,457 [nnabla][INFO]: iter=9200 {Test error}=0.0109375\n",
            "2023-02-03 23:51:21,487 [nnabla][INFO]: iter=9209 {Training loss}=0.006473885383456945\n",
            "2023-02-03 23:51:21,487 [nnabla][INFO]: iter=9209 {Training error}=0.003125\n",
            "2023-02-03 23:51:21,515 [nnabla][INFO]: iter=9219 {Training loss}=0.007187271025031805\n",
            "2023-02-03 23:51:21,516 [nnabla][INFO]: iter=9219 {Training error}=0.00234375\n",
            "2023-02-03 23:51:21,543 [nnabla][INFO]: iter=9229 {Training loss}=0.010347532108426094\n",
            "2023-02-03 23:51:21,543 [nnabla][INFO]: iter=9229 {Training error}=0.003125\n",
            "2023-02-03 23:51:21,570 [nnabla][INFO]: iter=9239 {Training loss}=0.0035790298134088516\n",
            "2023-02-03 23:51:21,570 [nnabla][INFO]: iter=9239 {Training error}=0.0\n",
            "2023-02-03 23:51:21,597 [nnabla][INFO]: iter=9249 {Training loss}=0.010313136503100395\n",
            "2023-02-03 23:51:21,597 [nnabla][INFO]: iter=9249 {Training error}=0.00234375\n",
            "2023-02-03 23:51:21,624 [nnabla][INFO]: iter=9259 {Training loss}=0.011773035861551762\n",
            "2023-02-03 23:51:21,624 [nnabla][INFO]: iter=9259 {Training error}=0.00625\n",
            "2023-02-03 23:51:21,651 [nnabla][INFO]: iter=9269 {Training loss}=0.025880644097924232\n",
            "2023-02-03 23:51:21,651 [nnabla][INFO]: iter=9269 {Training error}=0.00703125\n",
            "2023-02-03 23:51:21,677 [nnabla][INFO]: iter=9279 {Training loss}=0.017706111073493958\n",
            "2023-02-03 23:51:21,677 [nnabla][INFO]: iter=9279 {Training error}=0.00546875\n",
            "2023-02-03 23:51:21,704 [nnabla][INFO]: iter=9289 {Training loss}=0.009173894301056862\n",
            "2023-02-03 23:51:21,704 [nnabla][INFO]: iter=9289 {Training error}=0.00234375\n",
            "2023-02-03 23:51:21,734 [nnabla][INFO]: iter=9299 {Training loss}=0.008142175152897835\n",
            "2023-02-03 23:51:21,734 [nnabla][INFO]: iter=9299 {Training error}=0.0015625\n",
            "2023-02-03 23:51:21,734 [nnabla][INFO]: iter=9299 {Training time}=0.2876708507537842[sec/100iter] 32.10662627220154[sec]\n",
            "2023-02-03 23:51:21,745 [nnabla][INFO]: iter=9300 {Test error}=0.00703125\n",
            "2023-02-03 23:51:21,772 [nnabla][INFO]: iter=9309 {Training loss}=0.008223607204854488\n",
            "2023-02-03 23:51:21,772 [nnabla][INFO]: iter=9309 {Training error}=0.003125\n",
            "2023-02-03 23:51:21,802 [nnabla][INFO]: iter=9319 {Training loss}=0.013483382761478424\n",
            "2023-02-03 23:51:21,804 [nnabla][INFO]: iter=9319 {Training error}=0.0046875\n",
            "2023-02-03 23:51:21,834 [nnabla][INFO]: iter=9329 {Training loss}=0.008353064768016338\n",
            "2023-02-03 23:51:21,835 [nnabla][INFO]: iter=9329 {Training error}=0.003125\n",
            "2023-02-03 23:51:21,862 [nnabla][INFO]: iter=9339 {Training loss}=0.008169824257493019\n",
            "2023-02-03 23:51:21,862 [nnabla][INFO]: iter=9339 {Training error}=0.00234375\n",
            "2023-02-03 23:51:21,890 [nnabla][INFO]: iter=9349 {Training loss}=0.005773686803877354\n",
            "2023-02-03 23:51:21,890 [nnabla][INFO]: iter=9349 {Training error}=0.003125\n",
            "2023-02-03 23:51:21,917 [nnabla][INFO]: iter=9359 {Training loss}=0.006763538811355829\n",
            "2023-02-03 23:51:21,918 [nnabla][INFO]: iter=9359 {Training error}=0.0015625\n",
            "2023-02-03 23:51:21,945 [nnabla][INFO]: iter=9369 {Training loss}=0.011597493663430214\n",
            "2023-02-03 23:51:21,945 [nnabla][INFO]: iter=9369 {Training error}=0.003125\n",
            "2023-02-03 23:51:21,973 [nnabla][INFO]: iter=9379 {Training loss}=0.011581292375922203\n",
            "2023-02-03 23:51:21,973 [nnabla][INFO]: iter=9379 {Training error}=0.0015625\n",
            "2023-02-03 23:51:21,999 [nnabla][INFO]: iter=9389 {Training loss}=0.011863252148032188\n",
            "2023-02-03 23:51:22,000 [nnabla][INFO]: iter=9389 {Training error}=0.0046875\n",
            "2023-02-03 23:51:22,028 [nnabla][INFO]: iter=9399 {Training loss}=0.006179722957313061\n",
            "2023-02-03 23:51:22,028 [nnabla][INFO]: iter=9399 {Training error}=0.00234375\n",
            "2023-02-03 23:51:22,028 [nnabla][INFO]: iter=9399 {Training time}=0.29413342475891113[sec/100iter] 32.40075969696045[sec]\n",
            "2023-02-03 23:51:22,041 [nnabla][INFO]: iter=9400 {Test error}=0.009375\n",
            "2023-02-03 23:51:22,069 [nnabla][INFO]: iter=9409 {Training loss}=0.007170589175075293\n",
            "2023-02-03 23:51:22,070 [nnabla][INFO]: iter=9409 {Training error}=0.00234375\n",
            "2023-02-03 23:51:22,097 [nnabla][INFO]: iter=9419 {Training loss}=0.007454217877238989\n",
            "2023-02-03 23:51:22,097 [nnabla][INFO]: iter=9419 {Training error}=0.00390625\n",
            "2023-02-03 23:51:22,124 [nnabla][INFO]: iter=9429 {Training loss}=0.007294021546840668\n",
            "2023-02-03 23:51:22,124 [nnabla][INFO]: iter=9429 {Training error}=0.00234375\n",
            "2023-02-03 23:51:22,151 [nnabla][INFO]: iter=9439 {Training loss}=0.01860002614557743\n",
            "2023-02-03 23:51:22,151 [nnabla][INFO]: iter=9439 {Training error}=0.003125\n",
            "2023-02-03 23:51:22,178 [nnabla][INFO]: iter=9449 {Training loss}=0.010068227536976337\n",
            "2023-02-03 23:51:22,178 [nnabla][INFO]: iter=9449 {Training error}=0.00390625\n",
            "2023-02-03 23:51:22,205 [nnabla][INFO]: iter=9459 {Training loss}=0.018218735232949257\n",
            "2023-02-03 23:51:22,205 [nnabla][INFO]: iter=9459 {Training error}=0.00390625\n",
            "2023-02-03 23:51:22,233 [nnabla][INFO]: iter=9469 {Training loss}=0.012369267642498016\n",
            "2023-02-03 23:51:22,233 [nnabla][INFO]: iter=9469 {Training error}=0.00625\n",
            "2023-02-03 23:51:22,259 [nnabla][INFO]: iter=9479 {Training loss}=0.010441959835588932\n",
            "2023-02-03 23:51:22,259 [nnabla][INFO]: iter=9479 {Training error}=0.00390625\n",
            "2023-02-03 23:51:22,292 [nnabla][INFO]: iter=9489 {Training loss}=0.013151830062270164\n",
            "2023-02-03 23:51:22,292 [nnabla][INFO]: iter=9489 {Training error}=0.00390625\n",
            "2023-02-03 23:51:22,324 [nnabla][INFO]: iter=9499 {Training loss}=0.003446193877607584\n",
            "2023-02-03 23:51:22,324 [nnabla][INFO]: iter=9499 {Training error}=0.0\n",
            "2023-02-03 23:51:22,325 [nnabla][INFO]: iter=9499 {Training time}=0.29624509811401367[sec/100iter] 32.69700479507446[sec]\n",
            "2023-02-03 23:51:22,338 [nnabla][INFO]: iter=9500 {Test error}=0.0078125\n",
            "2023-02-03 23:51:22,365 [nnabla][INFO]: iter=9509 {Training loss}=0.004767061676830053\n",
            "2023-02-03 23:51:22,365 [nnabla][INFO]: iter=9509 {Training error}=0.0015625\n",
            "2023-02-03 23:51:22,393 [nnabla][INFO]: iter=9519 {Training loss}=0.009204831905663013\n",
            "2023-02-03 23:51:22,393 [nnabla][INFO]: iter=9519 {Training error}=0.00390625\n",
            "2023-02-03 23:51:22,421 [nnabla][INFO]: iter=9529 {Training loss}=0.007408416364341974\n",
            "2023-02-03 23:51:22,422 [nnabla][INFO]: iter=9529 {Training error}=0.003125\n",
            "2023-02-03 23:51:22,449 [nnabla][INFO]: iter=9539 {Training loss}=0.009455773048102856\n",
            "2023-02-03 23:51:22,449 [nnabla][INFO]: iter=9539 {Training error}=0.00390625\n",
            "2023-02-03 23:51:22,476 [nnabla][INFO]: iter=9549 {Training loss}=0.005783009808510542\n",
            "2023-02-03 23:51:22,476 [nnabla][INFO]: iter=9549 {Training error}=0.0015625\n",
            "2023-02-03 23:51:22,503 [nnabla][INFO]: iter=9559 {Training loss}=0.006521048489958048\n",
            "2023-02-03 23:51:22,503 [nnabla][INFO]: iter=9559 {Training error}=0.003125\n",
            "2023-02-03 23:51:22,529 [nnabla][INFO]: iter=9569 {Training loss}=0.013934311456978321\n",
            "2023-02-03 23:51:22,530 [nnabla][INFO]: iter=9569 {Training error}=0.00390625\n",
            "2023-02-03 23:51:22,557 [nnabla][INFO]: iter=9579 {Training loss}=0.010593943297863007\n",
            "2023-02-03 23:51:22,557 [nnabla][INFO]: iter=9579 {Training error}=0.0046875\n",
            "2023-02-03 23:51:22,584 [nnabla][INFO]: iter=9589 {Training loss}=0.013322326354682446\n",
            "2023-02-03 23:51:22,584 [nnabla][INFO]: iter=9589 {Training error}=0.0046875\n",
            "2023-02-03 23:51:22,611 [nnabla][INFO]: iter=9599 {Training loss}=0.009010819718241692\n",
            "2023-02-03 23:51:22,611 [nnabla][INFO]: iter=9599 {Training error}=0.0046875\n",
            "2023-02-03 23:51:22,611 [nnabla][INFO]: iter=9599 {Training time}=0.2864696979522705[sec/100iter] 32.98347449302673[sec]\n",
            "2023-02-03 23:51:22,626 [nnabla][INFO]: iter=9600 {Test error}=0.01640625\n",
            "2023-02-03 23:51:22,652 [nnabla][INFO]: iter=9609 {Training loss}=0.005290570668876171\n",
            "2023-02-03 23:51:22,652 [nnabla][INFO]: iter=9609 {Training error}=0.00078125\n",
            "2023-02-03 23:51:22,679 [nnabla][INFO]: iter=9619 {Training loss}=0.0033702056389302015\n",
            "2023-02-03 23:51:22,679 [nnabla][INFO]: iter=9619 {Training error}=0.00078125\n",
            "2023-02-03 23:51:22,706 [nnabla][INFO]: iter=9629 {Training loss}=0.007326131220906973\n",
            "2023-02-03 23:51:22,706 [nnabla][INFO]: iter=9629 {Training error}=0.00390625\n",
            "2023-02-03 23:51:22,733 [nnabla][INFO]: iter=9639 {Training loss}=0.005038734991103411\n",
            "2023-02-03 23:51:22,733 [nnabla][INFO]: iter=9639 {Training error}=0.00078125\n",
            "2023-02-03 23:51:22,760 [nnabla][INFO]: iter=9649 {Training loss}=0.006927539594471455\n",
            "2023-02-03 23:51:22,760 [nnabla][INFO]: iter=9649 {Training error}=0.00234375\n",
            "2023-02-03 23:51:22,786 [nnabla][INFO]: iter=9659 {Training loss}=0.007151884492486715\n",
            "2023-02-03 23:51:22,787 [nnabla][INFO]: iter=9659 {Training error}=0.00078125\n",
            "2023-02-03 23:51:22,813 [nnabla][INFO]: iter=9669 {Training loss}=0.006936711724847555\n",
            "2023-02-03 23:51:22,813 [nnabla][INFO]: iter=9669 {Training error}=0.003125\n",
            "2023-02-03 23:51:22,845 [nnabla][INFO]: iter=9679 {Training loss}=0.011003674939274788\n",
            "2023-02-03 23:51:22,846 [nnabla][INFO]: iter=9679 {Training error}=0.0015625\n",
            "2023-02-03 23:51:22,873 [nnabla][INFO]: iter=9689 {Training loss}=0.005909745581448078\n",
            "2023-02-03 23:51:22,873 [nnabla][INFO]: iter=9689 {Training error}=0.0015625\n",
            "2023-02-03 23:51:22,900 [nnabla][INFO]: iter=9699 {Training loss}=0.004553996957838535\n",
            "2023-02-03 23:51:22,900 [nnabla][INFO]: iter=9699 {Training error}=0.0015625\n",
            "2023-02-03 23:51:22,900 [nnabla][INFO]: iter=9699 {Training time}=0.28898119926452637[sec/100iter] 33.27245569229126[sec]\n",
            "2023-02-03 23:51:22,911 [nnabla][INFO]: iter=9700 {Test error}=0.00859375\n",
            "2023-02-03 23:51:22,941 [nnabla][INFO]: iter=9709 {Training loss}=0.010950133204460144\n",
            "2023-02-03 23:51:22,941 [nnabla][INFO]: iter=9709 {Training error}=0.00078125\n",
            "2023-02-03 23:51:22,968 [nnabla][INFO]: iter=9719 {Training loss}=0.003450285643339157\n",
            "2023-02-03 23:51:22,968 [nnabla][INFO]: iter=9719 {Training error}=0.0\n",
            "2023-02-03 23:51:22,995 [nnabla][INFO]: iter=9729 {Training loss}=0.006384889129549265\n",
            "2023-02-03 23:51:22,995 [nnabla][INFO]: iter=9729 {Training error}=0.0015625\n",
            "2023-02-03 23:51:23,022 [nnabla][INFO]: iter=9739 {Training loss}=0.007450351025909185\n",
            "2023-02-03 23:51:23,022 [nnabla][INFO]: iter=9739 {Training error}=0.00234375\n",
            "2023-02-03 23:51:23,049 [nnabla][INFO]: iter=9749 {Training loss}=0.006175904534757137\n",
            "2023-02-03 23:51:23,050 [nnabla][INFO]: iter=9749 {Training error}=0.003125\n",
            "2023-02-03 23:51:23,077 [nnabla][INFO]: iter=9759 {Training loss}=0.006175525952130556\n",
            "2023-02-03 23:51:23,077 [nnabla][INFO]: iter=9759 {Training error}=0.003125\n",
            "2023-02-03 23:51:23,104 [nnabla][INFO]: iter=9769 {Training loss}=0.007257835473865271\n",
            "2023-02-03 23:51:23,104 [nnabla][INFO]: iter=9769 {Training error}=0.00234375\n",
            "2023-02-03 23:51:23,131 [nnabla][INFO]: iter=9779 {Training loss}=0.004740810487419367\n",
            "2023-02-03 23:51:23,131 [nnabla][INFO]: iter=9779 {Training error}=0.00234375\n",
            "2023-02-03 23:51:23,158 [nnabla][INFO]: iter=9789 {Training loss}=0.017105335369706154\n",
            "2023-02-03 23:51:23,158 [nnabla][INFO]: iter=9789 {Training error}=0.00546875\n",
            "2023-02-03 23:51:23,185 [nnabla][INFO]: iter=9799 {Training loss}=0.00704670837149024\n",
            "2023-02-03 23:51:23,185 [nnabla][INFO]: iter=9799 {Training error}=0.003125\n",
            "2023-02-03 23:51:23,185 [nnabla][INFO]: iter=9799 {Training time}=0.2849769592285156[sec/100iter] 33.557432651519775[sec]\n",
            "2023-02-03 23:51:23,195 [nnabla][INFO]: iter=9800 {Test error}=0.00546875\n",
            "2023-02-03 23:51:23,223 [nnabla][INFO]: iter=9809 {Training loss}=0.014015063643455505\n",
            "2023-02-03 23:51:23,223 [nnabla][INFO]: iter=9809 {Training error}=0.00390625\n",
            "2023-02-03 23:51:23,252 [nnabla][INFO]: iter=9819 {Training loss}=0.0109474528580904\n",
            "2023-02-03 23:51:23,252 [nnabla][INFO]: iter=9819 {Training error}=0.0046875\n",
            "2023-02-03 23:51:23,281 [nnabla][INFO]: iter=9829 {Training loss}=0.006611566990613937\n",
            "2023-02-03 23:51:23,281 [nnabla][INFO]: iter=9829 {Training error}=0.00234375\n",
            "2023-02-03 23:51:23,308 [nnabla][INFO]: iter=9839 {Training loss}=0.012003566138446331\n",
            "2023-02-03 23:51:23,309 [nnabla][INFO]: iter=9839 {Training error}=0.00390625\n",
            "2023-02-03 23:51:23,337 [nnabla][INFO]: iter=9849 {Training loss}=0.010263824835419655\n",
            "2023-02-03 23:51:23,337 [nnabla][INFO]: iter=9849 {Training error}=0.00390625\n",
            "2023-02-03 23:51:23,365 [nnabla][INFO]: iter=9859 {Training loss}=0.005870625376701355\n",
            "2023-02-03 23:51:23,365 [nnabla][INFO]: iter=9859 {Training error}=0.003125\n",
            "2023-02-03 23:51:23,392 [nnabla][INFO]: iter=9869 {Training loss}=0.005003263242542744\n",
            "2023-02-03 23:51:23,393 [nnabla][INFO]: iter=9869 {Training error}=0.00234375\n",
            "2023-02-03 23:51:23,420 [nnabla][INFO]: iter=9879 {Training loss}=0.00921810232102871\n",
            "2023-02-03 23:51:23,420 [nnabla][INFO]: iter=9879 {Training error}=0.003125\n",
            "2023-02-03 23:51:23,447 [nnabla][INFO]: iter=9889 {Training loss}=0.0029668211936950684\n",
            "2023-02-03 23:51:23,447 [nnabla][INFO]: iter=9889 {Training error}=0.0015625\n",
            "2023-02-03 23:51:23,474 [nnabla][INFO]: iter=9899 {Training loss}=0.027692008763551712\n",
            "2023-02-03 23:51:23,474 [nnabla][INFO]: iter=9899 {Training error}=0.0078125\n",
            "2023-02-03 23:51:23,474 [nnabla][INFO]: iter=9899 {Training time}=0.2892262935638428[sec/100iter] 33.84665894508362[sec]\n",
            "2023-02-03 23:51:23,485 [nnabla][INFO]: iter=9900 {Test error}=0.01484375\n",
            "2023-02-03 23:51:23,515 [nnabla][INFO]: iter=9909 {Training loss}=0.0074809761717915535\n",
            "2023-02-03 23:51:23,515 [nnabla][INFO]: iter=9909 {Training error}=0.00390625\n",
            "2023-02-03 23:51:23,544 [nnabla][INFO]: iter=9919 {Training loss}=0.009894689545035362\n",
            "2023-02-03 23:51:23,544 [nnabla][INFO]: iter=9919 {Training error}=0.00390625\n",
            "2023-02-03 23:51:23,571 [nnabla][INFO]: iter=9929 {Training loss}=0.0034778970293700695\n",
            "2023-02-03 23:51:23,571 [nnabla][INFO]: iter=9929 {Training error}=0.0015625\n",
            "2023-02-03 23:51:23,598 [nnabla][INFO]: iter=9939 {Training loss}=0.004656600765883923\n",
            "2023-02-03 23:51:23,598 [nnabla][INFO]: iter=9939 {Training error}=0.00078125\n",
            "2023-02-03 23:51:23,625 [nnabla][INFO]: iter=9949 {Training loss}=0.004536962136626244\n",
            "2023-02-03 23:51:23,626 [nnabla][INFO]: iter=9949 {Training error}=0.0015625\n",
            "2023-02-03 23:51:23,653 [nnabla][INFO]: iter=9959 {Training loss}=0.0033041834831237793\n",
            "2023-02-03 23:51:23,653 [nnabla][INFO]: iter=9959 {Training error}=0.00078125\n",
            "2023-02-03 23:51:23,679 [nnabla][INFO]: iter=9969 {Training loss}=0.006329650990664959\n",
            "2023-02-03 23:51:23,680 [nnabla][INFO]: iter=9969 {Training error}=0.00234375\n",
            "2023-02-03 23:51:23,709 [nnabla][INFO]: iter=9979 {Training loss}=0.004685658495873213\n",
            "2023-02-03 23:51:23,709 [nnabla][INFO]: iter=9979 {Training error}=0.0015625\n",
            "2023-02-03 23:51:23,736 [nnabla][INFO]: iter=9989 {Training loss}=0.005711860489100218\n",
            "2023-02-03 23:51:23,736 [nnabla][INFO]: iter=9989 {Training error}=0.0015625\n",
            "2023-02-03 23:51:23,763 [nnabla][INFO]: iter=9999 {Training loss}=0.007570034358650446\n",
            "2023-02-03 23:51:23,763 [nnabla][INFO]: iter=9999 {Training error}=0.003125\n",
            "2023-02-03 23:51:23,763 [nnabla][INFO]: iter=9999 {Training time}=0.2891845703125[sec/100iter] 34.13584351539612[sec]\n",
            "2023-02-03 23:51:23,774 [nnabla][INFO]: iter=9999 {Test error}=0.00703125\n",
            "2023-02-03 23:51:23,782 [nnabla][INFO]: Parameter save (.h5): output/lenet_params_010000.h5\n",
            "2023-02-03 23:51:23,784 [nnabla][INFO]: Saving output/lenet_result.nnp as nnp\n",
            "2023-02-03 23:51:23,784 [nnabla][INFO]: Saving <_io.StringIO object at 0x7fbfe16baee0> as prototxt\n",
            "2023-02-03 23:51:23,792 [nnabla][INFO]: Parameter save (.h5): <_io.BytesIO object at 0x7fbfe0e25630>\n",
            "2023-02-03 23:51:23,792 [nnabla][INFO]: Model file is saved as (.nnp): output/lenet_result.nnp\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "outputフォルダが作られていることを確認"
      ],
      "metadata": {
        "id": "0RUxiRYy9fgE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!ls"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rmQG5smVN1qd",
        "outputId": "19a4a49c-7eb1-49af-cd36-519f9107c06d"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "args.py\t\t\t dcgan.py\tREADME.md\t  vae.py\n",
            "_checkpoint_nnp_util.py  mnist_data.py\trequirements.txt  vat.py\n",
            "classification_bnn.py\t output\t\tsiamese.py\n",
            "classification.py\t __pycache__\ttmp.monitor\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Cソースコードの出力先フォルダを作成。"
      ],
      "metadata": {
        "id": "Ur3Ks-Ax9li5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!mkdir ./output_csrc"
      ],
      "metadata": {
        "id": "iPaB93ZzOrKW"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "学習済みモデルファイル(.nnp)をCソースコードに変換。"
      ],
      "metadata": {
        "id": "QAw63svH9w9e"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!nnabla_cli convert -O CSRC -b 1 ./output/lenet_result.nnp ./output_csrc"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kTEVkmfpOvrc",
        "outputId": "2b19a7a0-58e6-440c-95cf-b8bedf72f85b"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2023-02-03 23:59:26,785 [nnabla][INFO]: Initializing CPU extension...\n",
            "NNabla command line interface (Version:1.33.0, Build:230118094635)\n",
            "2023-02-03 23:59:27,836 [nnabla][WARNING]: The export file format is 'CSRC' or 'SAVED_MODEL' that argument '--export-format' will have to be set!!!\n",
            "Importing ./output/lenet_result.nnp\n",
            " Expanding Validation.\n",
            "Using network [Validation].\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "変換の結果、次の４つのファイルが出来ていることを確認（その他のファイルは使わない）。  \n",
        "Validation_inference.c  \n",
        "Validation_inference.h  \n",
        "Validation_parameters.c  \n",
        "Validation_parameters.h"
      ],
      "metadata": {
        "id": "Gjtv8LjP-DVR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!ls output_csrc"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CWcBDYgQOykZ",
        "outputId": "a3c294f3-24aa-44d6-92f3-ddd4068cf5f1"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GNUmakefile\t      Validation_inference.c  Validation_parameters.c\n",
            "Validation_example.c  Validation_inference.h  Validation_parameters.h\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "変換後のファイルをPCにダウンロードするために、Google Driveに一旦コピーする。"
      ],
      "metadata": {
        "id": "8pVy_nSW_O1I"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UI4dG3MPLiqO",
        "outputId": "88ae0839-de45-4840-ccaa-d5cff883032e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!cp -r output_csrc /content/drive/MyDrive"
      ],
      "metadata": {
        "id": "YfpyR1jhymWj"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}